# CODEIT AI 3íŒ€ - SaaS Ad Content Platform

![Python](https://img.shields.io/badge/Python-3.11-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-0.109-teal)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0-orange)
![Docker](https://img.shields.io/badge/Docker-Compose-blue)

**ìƒì„±í˜• AI ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ì†Œìƒê³µì¸ì´ ê´‘ê³  ì½˜í…ì¸ (ë°°ë„ˆ, í™ˆí˜ì´ì§€)ë¥¼ ì†ì‰½ê²Œ ì œì‘í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ìë™í™” í”Œë«í¼ì…ë‹ˆë‹¤.**
ì˜¤í”„ë¼ì¸ ì¤‘ì‹¬ì˜ ì†Œìƒê³µì¸ì´ ë³µì¡í•œ ê³¼ì • ì—†ì´ ì˜¨ë¼ì¸ ë§ˆì¼€íŒ…ì„ ì‹œì‘í•  ìˆ˜ ìˆë„ë¡, ê´‘ê³  ë¬¸êµ¬, ì´ë¯¸ì§€, ê·¸ë¦¬ê³  ëœë”© í˜ì´ì§€ê¹Œì§€ All-in-Oneìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

## ğŸ“¢ ë°œí‘œ ìë£Œ

**ìµœì¢… ê²°ê³¼ ppt**: [ìµœì¢… ë°œí‘œìë£Œ ë‹¤ìš´ë¡œë“œ (PDF)](./3íŒ€_ë‚˜ë…¸ì½”ì½”ì•„_ìµœì¢…ë°œí‘œìë£Œ.pdf)

**ì‹œì—° ì˜ìƒ**: [ë°œí‘œìë£Œ/ì½”ë“œì‡AIì—”ì§€ë‹ˆì–´4ê¸°_3íŒ€_í…ŒìŠ¤íŠ¸ë™ì˜ìƒ_260123_141208-c.mp4](https://krapnuyij.github.io/codeit-ai-3team-ad-content/ë°œí‘œìë£Œ/ì½”ë“œì‡AIì—”ì§€ë‹ˆì–´4ê¸°_3íŒ€_í…ŒìŠ¤íŠ¸ë™ì˜ìƒ_260123_141208-c.mp4)<br/>
<video src="https://krapnuyij.github.io/codeit-ai-3team-ad-content/ë°œí‘œìë£Œ/ì½”ë“œì‡AIì—”ì§€ë‹ˆì–´4ê¸°_3íŒ€_í…ŒìŠ¤íŠ¸ë™ì˜ìƒ_260123_141208-c.mp4" controls width="320"></video>

---

## ğŸ‘¥ íŒ€ êµ¬ì„± ë° ì—­í• 

| ì´ë¦„ | ì—­í•  | ë‹´ë‹¹ ì—…ë¬´ |
|---|---|---|
| **ê¹€ëª…í™˜** | ì•„í‚¤í…ì²˜/Data | ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ê³„, ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì„±, ëª¨ë¸ ê´€ë¦¬ ì„œë²„ ì„¤ê³„ |
| **ê¹€ë¯¼í˜** | AI Modeling | í…ìŠ¤íŠ¸ ìƒì„± ë° ì¡°í•© ëª¨ë¸ ê°œë°œ, í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ |
| **ë°•ì§€ìœ¤** | PM | í”„ë¡œì íŠ¸ ê´€ë¦¬, ì¼ì • ì¡°ìœ¨, ê¸°íš, GCP ì¸í”„ë¼ êµ¬ì¶• |
| **ì´ê±´í¬** | Full Stack | ë°±ì—”ë“œ(FastAPI), í”„ë¡ íŠ¸ì—”ë“œ(Jinja2/HTML) |
| **ì´ì†”í˜•** | AI Modeling | ì´ë¯¸ì§€ íŠ¹ì„± ì¶”ì¶œ, ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ ìµœì í™” |

## ğŸ“ í˜‘ì—…ì¼ì§€

íŒ€ì›ë³„ ê°œë°œ ê³¼ì • ë° í•™ìŠµ ë‚´ìš©ì„ ê¸°ë¡í•œ í˜‘ì—…ì¼ì§€ì…ë‹ˆë‹¤.

- [ê¹€ëª…í™˜ í˜‘ì—…ì¼ì§€ (ì•„í‚¤í…ì²˜ & íŒŒì´í”„ë¼ì¸)](https://krapnuyij.github.io/codeit-ai-3team-ad-content/í˜‘ì—…ì¼ì§€/ê¹€ëª…í™˜/)
- [ê¹€ë¯¼í˜ í˜‘ì—…ì¼ì§€ (í…ìŠ¤íŠ¸ ìƒì„± ë° ì¡°í•© ëª¨ë¸ ê°œë°œ)](https://krapnuyij.github.io/codeit-ai-3team-ad-content/í˜‘ì—…ì¼ì§€/ê¹€ë¯¼í˜/)
- [ë°•ì§€ìœ¤ í˜‘ì—…ì¼ì§€ (PM & ê¸°íš)](https://krapnuyij.github.io/codeit-ai-3team-ad-content/í˜‘ì—…ì¼ì§€/ë°•ì§€ìœ¤/)
- [ì´ê±´í¬ í˜‘ì—…ì¼ì§€ (ë°±ì—”ë“œ & í”„ë¡ íŠ¸ì—”ë“œ)](https://krapnuyij.github.io/codeit-ai-3team-ad-content/í˜‘ì—…ì¼ì§€/ì´ê±´í¬/)
- [ì´ì†”í˜• í˜‘ì—…ì¼ì§€ (ì´ë¯¸ì§€ íŠ¹ì„± ì¶”ì¶œ ë° ì´ë¯¸ì§€ ìƒì„±)](https://krapnuyij.github.io/codeit-ai-3team-ad-content/í˜‘ì—…ì¼ì§€/ì´ì†”í˜•/)

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

ì´ í”„ë¡œì íŠ¸ëŠ” **Microservices Architecture**ë¥¼ ì±„íƒí•˜ì—¬ ê° ê¸°ëŠ¥ì´ ë…ë¦½ì ì¸ ì»¨í…Œì´ë„ˆë¡œ ë™ì‘í•˜ë©°, Docker Composeë¥¼ í†µí•´ í†µí•© ê´€ë¦¬ë©ë‹ˆë‹¤.

**êµ¬ì¡°ë„ (High-Level Architecture)**

```mermaid
graph TB
    subgraph "ì‚¬ìš©ì í™˜ê²½"
        User["ì‚¬ìš©ì (ì†Œìƒê³µì¸)"]
        LLMClient["LLM / GPT
(MCP í´ë¼ì´ì–¸íŠ¸)"]
    end

    subgraph "Docker Network: nanococoa-network"
        subgraph "ë°±ì—”ë“œ ê³„ì¸µ (backend)"
            Backend["FastAPI ì„œë²„
ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
Port: 8080"]
            HPGen["Homepage Generator
LangGraph + Multi-Agent
Port: 8081"]
            DB[("PostgreSQL
ê³ ê° ë°ì´í„°")]
        end

        subgraph "MCP ì„œë²„ (nanoCocoa_mcpserver)"
            MCPServer["MCP ì„œë²„
MCP Protocol Bridge
Port: 3000"]
        end

        subgraph "ëª¨ë¸ì„œë¹™ ê³„ì¸µ (nanoCocoa_aiserver)"
            ModelServer["FastAPI ëª¨ë¸ ì„œë²„
Port: 8000"]

            subgraph "AI ëª¨ë¸ íŒŒì´í”„ë¼ì¸"
                BiRefNet["BiRefNet
(ì´ë¯¸ì§€ ëˆ„ë¼)"]
                FLUX["FLUX.1-dev
(ë°°ê²½ ìƒì„±)"]
                Qwen["Qwen2-VL
(ì´ë¯¸ì§€ ë¶„ì„)"]
            end

            LLMText["OpenAI API
(HTML/CSS ìƒì„±)"]
            GPU["NVIDIA L4 GPU
24GB VRAM"]
        end
    end

    User -->|HTTP/ì›¹| Backend
    Backend --> DB
    Backend --> HPGen
    HPGen -->|HTTP| MCPServer
    Backend -->|"REST API
Port 8000"| ModelServer
    
    LLMClient -.->|"MCP Protocol
(SSE)"| MCPServer
    MCPServer -->|"REST API
Internal Network"| ModelServer
    
    ModelServer --> BiRefNet
    ModelServer --> FLUX
    ModelServer --> Qwen
    ModelServer --> LLMText
    
    BiRefNet -.->|JIT ë¡œë”©| GPU
    FLUX -.->|JIT ë¡œë”©| GPU
    Qwen -.->|JIT ë¡œë”©| GPU
```

**ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨**

```mermaid
sequenceDiagram
    participant User as ì‚¬ìš©ì
    participant FE as í”„ë¡ íŠ¸ì—”ë“œ (FastAPI)
    participant BE as ë°±ì—”ë“œ (FastAPI)
    participant LLM as OpenAI GPT-5-mini
    participant MS as ëª¨ë¸ì„œë¹™ (FastAPI)
    participant GPU as L4 GPU

    User->>FE: 1. ì´ë¯¸ì§€ ì—…ë¡œë“œ + ê´‘ê³  ë¬¸êµ¬ ì…ë ¥
    FE->>FE: 2. ì…ë ¥ ê²€ì¦
    FE->>BE: 3. POST /api/generate {image, text, options}

    BE->>BE: 4. ìš”ì²­ ê²€ì¦
    BE->>LLM: 5. í”„ë¡¬í”„íŠ¸ ìƒì„± ìš”ì²­ "ê±´ì–´ë¬¼ ëŒ€ë°• ì„¸ì¼"
    LLM-->>BE: 6. ì˜ë¬¸ í”„ë¡¬í”„íŠ¸ ë°˜í™˜ "Dried seafood..."

    BE->>MS: 7. POST /generate {product_image, bg_prompt, text_content}
    MS->>MS: 8. Job ID ìƒì„± Worker Process ìƒì„±
    MS-->>BE: 9. {job_id, status: "started"}
    BE-->>FE: 10. {job_id}
    FE-->>User: 11. "ìƒì„± ì¤‘..." í‘œì‹œ

    loop ì§„í–‰ ìƒí™© í´ë§ (Polling)
        FE->>BE: 12. GET /api/status/{job_id}
        BE->>MS: 13. GET /status/{job_id}

        MS->>MS: Stage 1 ì‹¤í–‰
        MS->>GPU: BiRefNet ë¡œë“œ
        GPU-->>MS: ëˆ„ë¼ ì´ë¯¸ì§€
        MS->>GPU: FLUX.1-dev ë¡œë“œ
        GPU-->>MS: ë°°ê²½ ì´ë¯¸ì§€
        MS->>MS: í•©ì„± ë° ë¦¬í„°ì¹­

        MS-->>BE: 14. {status: "running", progress: 50%, step1_result}
        BE-->>FE: 15. {progress, step1_preview}
        FE-->>User: 16. ì§„í–‰ë¥  + ì¤‘ê°„ ê²°ê³¼ í‘œì‹œ

        MS->>MS: Stage 2 ì‹¤í–‰
        
        alt use_qwen_analysis=true
            MS->>GPU: Qwen2-VL ë¡œë“œ
            GPU-->>MS: ì´ë¯¸ì§€ ë¶„ì„ í…ìŠ¤íŠ¸
            MS->>GPU: Qwen2-VL ì–¸ë¡œë“œ
        end
        
        MS->>LLM: HTML/CSS ìƒì„± ìš”ì²­ (Qwen ë¶„ì„ í¬í•¨)
        LLM-->>MS: HTML/CSS ì½”ë“œ
        MS->>MS: HTML ë Œë”ë§ (Headless Browser)
        MS->>MS: í…ìŠ¤íŠ¸ ë ˆì´ì–´ í•©ì„±

        MS-->>BE: 17. {status: "completed", final_result}
        BE-->>FE: 18. {status: "done", final_image}
    end

    FE-->>User: 19. ìµœì¢… ê²°ê³¼ í‘œì‹œ + ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
```

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1. ì‚¬ì „ ì¤€ë¹„ (Prerequisites)
- [Docker](https://www.docker.com/products/docker-desktop/) ì„¤ì¹˜
- NVIDIA GPU ê¶Œì¥ (AI ì´ë¯¸ì§€ ìƒì„± ì†ë„ í–¥ìƒ ìœ„í•¨)
    - GPU ì‚¬ìš© ì‹œ `nvidia-container-toolkit` ì„¤ì • í•„ìš”.

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
`src/.env` íŒŒì¼ì„ ìƒì„±í•˜ê³  ì•„ë˜ ë‚´ìš©ì„ ì‘ì„±í•˜ì„¸ìš”. (ë³´ì•ˆìƒ ì‹¤ì œ í‚¤ëŠ” ì œì™¸ë¨)

```env
# Database
POSTGRES_USER=owner
POSTGRES_PASSWORD=owner1234
POSTGRES_DB=customer_db

# External APIs (í•„ìˆ˜)
OPENAI_API_KEY=sk-proj-...
HF_TOKEN=hf_...

# Internal Network URLs (Docker Service Names)
DATABASE_URL=postgresql://owner:owner1234@customer_db:5432/customer_db
HOMEPAGE_GENERATOR_URL=http://homepage_generator:8891
NANOCOCOA_URL=http://nanococoa_aiserver:8892
```

### 3. ì„œë¹„ìŠ¤ ì‹¤í–‰
`src` í´ë” ìœ„ì¹˜ì—ì„œ í„°ë¯¸ë„ì„ ì—´ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.

```bash
# ì‹¤í–‰ (ì´ë¯¸ì§€ ë¹Œë“œ í¬í•¨)
docker-compose up --build

# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ì‹œ
docker-compose up --build -d
```

### 4. ì ‘ì† ì •ë³´

| ì„œë¹„ìŠ¤ | URL | ì„¤ëª… |
|---|---|---|
| **ë©”ì¸ ì›¹ ì„œë¹„ìŠ¤** | [http://localhost:8890](http://localhost:8890) | ì‚¬ìš©ì ëŒ€ì‹œë³´ë“œ ë° ì‘ì—… ìš”ì²­ |
| **ìƒì„±ëœ í™ˆí˜ì´ì§€** | [http://localhost:8893/sites/...](http://localhost:8893) | ê²°ê³¼ë¬¼ í™•ì¸ (ê²½ë¡œëŠ” ìƒì„± í›„ ì œê³µë¨) |
| **API Docs (Backend)** | [http://localhost:8890/docs](http://localhost:8890/docs) | ë°±ì—”ë“œ API ë¬¸ì„œ |
| **API Docs (AI)** | [http://localhost:8892/docs](http://localhost:8892/docs) | AI ì„œë²„ API ë¬¸ì„œ |

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰

### ê°„í¸ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© (ê¶Œì¥)

```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸ (dummy ëª¨ë“œ - GPU ë¯¸ì‚¬ìš©)
./tests/run_tests.sh

# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë§Œ
./tests/run_tests.sh --fast

# ì‹¤ì œ AI ì—”ì§„ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ (GPU í•„ìš”)
./tests/run_tests.sh --real

# ë„ì›€ë§
./tests/run_tests.sh --help
```

### pytest ì§ì ‘ ì‹¤í–‰

**ê¸°ë³¸ í…ŒìŠ¤íŠ¸ (Dummy ëª¨ë“œ)**

```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (GPU ë¯¸ì‚¬ìš©, ë¹ ë¥¸ ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸)
pytest tests -v

# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë§Œ (slow, docker ì œì™¸)
pytest tests -v -m "not slow and not docker"

# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ë§Œ
pytest tests/units -v
```

**ì‹¤ì œ AI ì—”ì§„ í…ŒìŠ¤íŠ¸ (GPU í•„ìš”)**

```bash
# ì‹¤ì œ AI ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ (GPU í•„ìš”)
pytest tests -v --no-dummy

# íŠ¹ì • íŒŒì¼ë§Œ ì‹¤ì œ ì—”ì§„ìœ¼ë¡œ
pytest tests/units/test_api_scenarios.py -v --no-dummy
```

**ë§ˆì»¤ë³„ ì‹¤í–‰**

```bash
# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ë§Œ
pytest tests -v -m "unit"

# í†µí•© í…ŒìŠ¤íŠ¸ë§Œ (AI ì„œë²„ ì‹¤í–‰ í•„ìš”)
pytest tests -v -m "integration"

# slow í…ŒìŠ¤íŠ¸ ì œì™¸
pytest tests -v -m "not slow"
```

ìì„¸í•œ í…ŒìŠ¤íŠ¸ ê°€ì´ë“œëŠ” [TEST_GUIDE.md](docs/doc/TEST_GUIDE.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

---

## ğŸ“‚ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„¸

```
src/
â”œâ”€â”€ backend/                # ë©”ì¸ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜
â”‚   â”œâ”€â”€ templates/          # Jinja2 HTML í…œí”Œë¦¿
â”‚   â”œâ”€â”€ static/             # CSS, JS, Images
â”‚   â””â”€â”€ app.py              # ë©”ì¸ ì‹¤í–‰ íŒŒì¼
â”œâ”€â”€ homepage_generator/     # í™ˆí˜ì´ì§€ ìƒì„± ì—ì´ì „íŠ¸
â”‚   â”œâ”€â”€ nodes/              # LangGraph ë…¸ë“œ (ê¸°íš, ë””ìì¸ ë“±)
â”‚   â””â”€â”€ api.py              # API ì—”ë“œí¬ì¸íŠ¸
â”œâ”€â”€ nanoCocoa_aiserver/     # ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ ì„œë²„
â”‚   â”œâ”€â”€ models/             # AI ëª¨ë¸ ê´€ë ¨ ì½”ë“œ
â”‚   â””â”€â”€ main.py             # ì‹¤í–‰ íŒŒì¼
â”œâ”€â”€ docker-compose.yaml     # í†µí•© ì‹¤í–‰ ì„¤ì •
â””â”€â”€ README.md               # í”„ë¡œì íŠ¸ ì„¤ëª… (í˜„ì¬ íŒŒì¼)
```
