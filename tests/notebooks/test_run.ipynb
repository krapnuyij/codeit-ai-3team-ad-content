{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "877d997e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mí˜„ì¬ ì‚¬ìš© ì¤‘ì¸ íŒŒì´ì¬ ê²½ë¡œ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msys.executable\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import matplotlib\n",
    "import torch\n",
    "\n",
    "print(f\"í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ íŒŒì´ì¬ ê²½ë¡œ: {sys.executable}\")\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"Matplotlib ë²„ì „: {matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae66f49",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForImageSegmentation, AutoImageProcessor\n\u001b[32m     15\u001b[39m INPUT_DIR = Path(\u001b[33m\"\u001b[39m\u001b[33mdata/inputs\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# ğŸ§ª STEP 1 â€” BiRefNet ë°°ê²½ ì œê±° í…ŒìŠ¤íŠ¸\n",
    "# ===============================\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoModelForImageSegmentation, AutoImageProcessor\n",
    "\n",
    "\n",
    "INPUT_DIR = Path(\"data/inputs\")\n",
    "OUTPUT_FG = Path(\"outputs/fg_cut\")\n",
    "OUTPUT_MASK = Path(\"outputs/fg_mask\")\n",
    "\n",
    "OUTPUT_FG.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_MASK.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"â–¶ Loading BiRefNet segmentation model...\")\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"ZhengPeng7/BiRefNet\")\n",
    "birefnet = AutoModelForImageSegmentation.from_pretrained(\n",
    "    \"ZhengPeng7/BiRefNet\"\n",
    ").to(\"cuda\").eval()\n",
    "\n",
    "# ì²« ë²ˆì§¸ ì…ë ¥ ì´ë¯¸ì§€ ìë™ ì„ íƒ\n",
    "image_files = sorted(list(INPUT_DIR.glob(\"*.*\")))\n",
    "assert len(image_files) > 0, \"âŒ data/inputs í´ë”ì— ì´ë¯¸ì§€ ë„£ì–´ì¤˜ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "\n",
    "img_path = image_files[0]\n",
    "print(\"âœ” Input image:\", img_path)\n",
    "\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "inputs = processor(images=img, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = birefnet(**inputs)\n",
    "    mask = torch.sigmoid(outputs.logits)[0,0].cpu().numpy()\n",
    "\n",
    "# ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬\n",
    "mask = (mask * 255).astype(np.uint8)\n",
    "mask_3c = cv2.merge([mask, mask, mask])\n",
    "\n",
    "# ì›ë³¸ + ì•ŒíŒŒ ê²°í•©\n",
    "img_np = np.array(img)\n",
    "rgba = np.concatenate([img_np, mask[...,None]], axis=2)\n",
    "\n",
    "fg_path = OUTPUT_FG / f\"{img_path.stem}_fg.png\"\n",
    "mask_path = OUTPUT_MASK / f\"{img_path.stem}_mask.png\"\n",
    "\n",
    "Image.fromarray(rgba).save(fg_path)\n",
    "Image.fromarray(mask).save(mask_path)\n",
    "\n",
    "print(\"âœ” Foreground saved:\", fg_path)\n",
    "print(\"âœ” Mask saved:\", mask_path)\n",
    "\n",
    "# ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,3,1); plt.title(\"Original\"); plt.imshow(img); plt.axis(\"off\")\n",
    "plt.subplot(1,3,2); plt.title(\"Mask\"); plt.imshow(mask, cmap=\"gray\"); plt.axis(\"off\")\n",
    "plt.subplot(1,3,3); plt.title(\"Cut-out\"); plt.imshow(Image.open(fg_path)); plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938a203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from diffusers import (\n",
    "    StableDiffusionPipeline,\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    ControlNetModel,\n",
    "    StableDiffusionXLPipeline\n",
    ")\n",
    "\n",
    "\n",
    "# ê¸°ë³¸ ì„¤ì •\n",
    "\n",
    "MODE = \"sd15_controlnet\"      # ì›í•˜ëŠ” ëª¨ë¸ ì„ íƒ: \"sd15_controlnet\" | \"sdxl\"\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "INPUT_DIR = Path(\"inputs\")\n",
    "\n",
    "RUN_NAME = f\"run_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "SAVE_DIR = os.path.join(OUTPUT_DIR, RUN_NAME)\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(SAVE_DIR, \"candidates\"), exist_ok=True)\n",
    "\n",
    "\n",
    "# ê³µìš© ëª¨ë¸ ID ì •ì˜\n",
    "\n",
    "MODEL_IDS = {\n",
    "    \"sd15\": \"runwayml/stable-diffusion-v1-5\",\n",
    "    \"sdxl\": \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    \"controlnet\": \"lllyasviel/control_v11p_sd15_canny\",\n",
    "}\n",
    "\n",
    "\n",
    "pipe = None\n",
    "controlnet = None\n",
    "\n",
    "\n",
    "# íŒŒì´í”„ë¼ì¸ ì–¸ë¡œë“œ (ëª¨ë¸ ë³€ê²½ ì‹œ ë©”ëª¨ë¦¬ íšŒìˆ˜)\n",
    "\n",
    "def unload_pipeline():\n",
    "    \"\"\"ëª¨ë¸ íŒŒì´í”„ë¼ì¸ì„ ì•ˆì „í•˜ê²Œ ì–¸ë¡œë“œ\"\"\"\n",
    "    global pipe, controlnet\n",
    "\n",
    "    pipe = None\n",
    "    controlnet = None\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "# íŒŒì´í”„ë¼ì¸ ë¡œë” (MODE ê¸°ë°˜)\n",
    "\n",
    "\n",
    "def load_pipeline(mode: str, use_controlnet: bool, canny_image=None):\n",
    "    \"\"\"ëª¨ë¸ ëª¨ë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ì„ ë¡œë“œ\"\"\"\n",
    "    global pipe, controlnet\n",
    "\n",
    "    unload_pipeline()\n",
    "\n",
    "    if mode == \"sd15_controlnet\" and use_controlnet:\n",
    "        print(\"â–¶ Loading SD1.5 + ControlNet pipeline\")\n",
    "\n",
    "        controlnet = ControlNetModel.from_pretrained(\n",
    "            MODEL_IDS[\"controlnet\"],\n",
    "            torch_dtype=torch.float16\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "            MODEL_IDS[\"sd15\"],\n",
    "            controlnet=controlnet,\n",
    "            torch_dtype=torch.float16\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "    elif mode == \"sdxl\":\n",
    "        print(\"â–¶ Loading SDXL pipeline\")\n",
    "\n",
    "        pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "            MODEL_IDS[\"sdxl\"],\n",
    "            torch_dtype=torch.float16\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "    else:\n",
    "        print(\"â–¶ Loading SD1.5 base pipeline\")\n",
    "\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            MODEL_IDS[\"sd15\"],\n",
    "            torch_dtype=torch.float16\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "# ControlNet ì…ë ¥ ì´ë¯¸ì§€ íƒìƒ‰ ë° Canny ë³€í™˜\n",
    "\n",
    "def get_controlnet_image():\n",
    "    \"\"\"ì…ë ¥ ë””ë ‰í„°ë¦¬ì—ì„œ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì•„ Canny ë³€í™˜\"\"\"\n",
    "    image_files = sorted([\n",
    "        *INPUT_DIR.glob(\"*.jpg\"),\n",
    "        *INPUT_DIR.glob(\"*.jpeg\"),\n",
    "        *INPUT_DIR.glob(\"*.png\"),\n",
    "        *INPUT_DIR.glob(\"*.webp\"),\n",
    "    ])\n",
    "\n",
    "    if len(image_files) == 0:\n",
    "        print(\"â„¹ ì…ë ¥ ì´ë¯¸ì§€ ì—†ìŒ â€” ControlNet ë¹„í™œì„±í™”\")\n",
    "        return None, False\n",
    "\n",
    "    path = image_files[0]\n",
    "    print(f\"âœ” ì°¸ì¡° ì´ë¯¸ì§€ ì‚¬ìš©: {path}\")\n",
    "\n",
    "    img = cv2.imread(str(path))\n",
    "\n",
    "    if img is None:\n",
    "        print(\"ì´ë¯¸ì§€ë¥¼ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ â€” ControlNet ë¹„í™œì„±í™”\")\n",
    "        return None, False\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (512, 512))\n",
    "\n",
    "    edges = cv2.Canny(img, 100, 200)\n",
    "    canny = Image.fromarray(edges)\n",
    "\n",
    "    print(\"âœ” ControlNet Canny ì ìš© ì™„ë£Œ\")\n",
    "    return path, canny\n",
    "\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "\n",
    "prompt = (\n",
    "    \"illustration style, clean poster layout, warm cozy color tone, \"\n",
    "    \"korean traditional market dried seafood poster, \"\n",
    "    \"korean hangul signage only, authentic korean market mood, \"\n",
    "    \"wooden baskets and display table, dried anchovies, dried squid, pollack strips, seaweed, \"\n",
    "    \"neatly arranged products, soft textured illustration shading, \"\n",
    "    \"natural warm lighting, product-focused composition\"\n",
    ")\n",
    "\n",
    "negative_prompt = (\n",
    "    \"japanese text, chinese text, kanji, latin letters, english text, \"\n",
    "    \"blurry, noisy, messy layout, distorted shapes, watermark, logo\"\n",
    ")\n",
    "\n",
    "\n",
    "# ìƒì„± íŒŒë¼ë¯¸í„°\n",
    "\n",
    "num_steps = 30\n",
    "guidance = 5.5\n",
    "num_images = 3\n",
    "seed = 42\n",
    "control_scale = 0.45\n",
    "\n",
    "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
    "\n",
    "\n",
    "# ì´ë¯¸ì§€ ìƒì„± + ë²¤ì¹˜ë§ˆí‚¹ ê³„ì¸¡\n",
    "\n",
    "input_image_path, canny_img = get_controlnet_image()\n",
    "use_controlnet = (MODE == \"sd15_controlnet\" and canny_img is not None)\n",
    "\n",
    "pipe = load_pipeline(MODE, use_controlnet, canny_img)\n",
    "\n",
    "print(\"â–¶ Generating candidate images...\")\n",
    "\n",
    "start_time = time.time()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "if use_controlnet:\n",
    "    images = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        image=canny_img,\n",
    "        controlnet_conditioning_scale=control_scale,\n",
    "        num_inference_steps=num_steps,\n",
    "        guidance_scale=guidance,\n",
    "        num_images_per_prompt=num_images,\n",
    "        generator=generator\n",
    "    ).images\n",
    "else:\n",
    "    images = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_inference_steps=num_steps,\n",
    "        guidance_scale=guidance,\n",
    "        num_images_per_prompt=num_images,\n",
    "        generator=generator\n",
    "    ).images\n",
    "\n",
    "latency_ms = int((time.time() - start_time) * 1000)\n",
    "vram_peak = round(torch.cuda.max_memory_allocated() / 1024 / 1024, 2)\n",
    "\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "\n",
    "paths = []\n",
    "for i, img in enumerate(images, start=1):\n",
    "    path = os.path.join(SAVE_DIR, \"candidates\", f\"candidate_v{i}.png\")\n",
    "    img.save(path)\n",
    "    paths.append(path)\n",
    "\n",
    "print(\"âœ” Saved candidates:\")\n",
    "print(\"\\n\".join(paths))\n",
    "\n",
    "\n",
    "# params.json (ë²¤ì¹˜ë§ˆí‚¹ ë¡œê·¸)\n",
    "\n",
    "params = {\n",
    "    \"mode\": MODE,\n",
    "    \"seed\": seed,\n",
    "    \"steps\": num_steps,\n",
    "    \"guidance_scale\": guidance,\n",
    "    \"controlnet_scale\": control_scale if use_controlnet else None,\n",
    "    \"latency_ms\": latency_ms,\n",
    "    \"vram_used_mb\": vram_peak,\n",
    "    \"input_image\": str(input_image_path) if use_controlnet else None,\n",
    "    \"output_path\": str(SAVE_DIR),\n",
    "}\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, \"params.json\"), \"w\") as f:\n",
    "    json.dump(params, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ” params.json saved\")\n",
    "print(\"Run folder:\", SAVE_DIR)\n",
    "print(f\"â–¶ latency: {latency_ms} ms   |   vram_peak: {vram_peak} MB\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
