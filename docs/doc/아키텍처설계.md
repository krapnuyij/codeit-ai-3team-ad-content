---
layout: default
title: "AI ê´‘ê³  ì½˜í…ì¸  ìƒì„± ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ê³„ì„œ"
description: "AI ê´‘ê³  ì½˜í…ì¸  ìƒì„± ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ê³„ì„œ"
date: 2026-01-02
author: "ê¹€ëª…í™˜"
cache-control: no-cache
expires: 0
pragma: no-cache
---

# AI ê´‘ê³  ì½˜í…ì¸  ìƒì„± ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ê³„ì„œ

**ì‘ì„±ì¼**: 2026.01.01<br/>
**ì‘ì„±ì**: ê¹€ëª…í™˜<br/>
**ë²„ì „**: v1.0<br/>
**í”„ë¡œì íŠ¸**: ìƒì„±í˜• AI ê¸°ë°˜ ì†Œìƒê³µì¸ ê´‘ê³  ì½˜í…ì¸  ì œì‘ ì§€ì› ì„œë¹„ìŠ¤<br/>

---

## 1. ê°œìš” (Overview)

### 1.1. ë¬¸ì„œ ëª©ì 

ë³¸ ë¬¸ì„œëŠ” ìƒì„±í˜• AI ê¸°ë°˜ ê´‘ê³  ì½˜í…ì¸  ìƒì„± ì‹œìŠ¤í…œì˜ ì „ì²´ ì•„í‚¤í…ì²˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì‚¬ìš©ì-í”„ë¡ íŠ¸ì—”ë“œ-ë°±ì—”ë“œ-ëª¨ë¸ì„œë¹™ì˜ 4ê³„ì¸µ êµ¬ì¡°ì™€ ê° ì»´í¬ë„ŒíŠ¸ ê°„ í†µì‹  ë°©ì‹, ë°ì´í„° íë¦„, ë°°í¬ ì „ëµì„ ê¸°ìˆ í•©ë‹ˆë‹¤.

### 1.2. ì‹œìŠ¤í…œ ëª©í‘œ

- **ì‚¬ìš©ì ì¹œí™”ì„±**: FastAPI ê¸°ë°˜ì˜ ì§ê´€ì ì¸ UIë¡œ ë””ìì¸ ì—­ëŸ‰ì´ ë¶€ì¡±í•œ ì†Œìƒê³µì¸ë„ ì‰½ê²Œ ì‚¬ìš©
- **ê³ ì„±ëŠ¥ AI ì¶”ë¡ **: NVIDIA L4 GPUë¥¼ í™œìš©í•œ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±
- **í™•ì¥ ê°€ëŠ¥ì„±**: Docker ê¸°ë°˜ ì»¨í…Œì´ë„ˆí™”ë¡œ ê° ê³„ì¸µì˜ ë…ë¦½ì ì¸ í™•ì¥ ë° ë°°í¬
- **ì•ˆì •ì„±**: ë¹„ë™ê¸° ì²˜ë¦¬ ë° ì—ëŸ¬ í•¸ë“¤ë§ì„ í†µí•œ ì„œë¹„ìŠ¤ ì•ˆì •ì„± í™•ë³´

### 1.3. ê¸°ìˆ  ìŠ¤íƒ

| ê³„ì¸µ | ê¸°ìˆ  ìŠ¤íƒ | ë¹„ê³  |
|-----|----------|------|
| **í”„ë¡ íŠ¸ì—”ë“œ** | FastAPI, Python 3.11+ | ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ |
| **ë°±ì—”ë“œ** | FastAPI, Python 3.11+ | ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§, LLM ì—°ë™ |
| **ëª¨ë¸ì„œë¹™** | FastAPI, PyTorch, Diffusers | AI ëª¨ë¸ ì¶”ë¡  ì„œë²„ |
| **AI ëª¨ë¸** | FLUX.1-dev, SDXL, BiRefNet | ì´ë¯¸ì§€ ìƒì„± ë° ì²˜ë¦¬ |
| **ë°°í¬** | Docker, Docker Compose | ì»¨í…Œì´ë„ˆ ê¸°ë°˜ ë°°í¬ |
| **ì¸í”„ë¼** | GCP VM (L4 GPU) | NVIDIA L4 24GB VRAM |

---

## 2. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ (System Architecture)

### 2.1. ì „ì²´ êµ¬ì¡°ë„ (High-Level Architecture)

```mermaid
graph TB
    subgraph "ì‚¬ìš©ì í™˜ê²½"
        User["ì‚¬ìš©ì (ì†Œìƒê³µì¸)"]
        Claude["Claude Desktop/Code
(LLM í´ë¼ì´ì–¸íŠ¸)"]
    end

    subgraph "í”„ë¡ íŠ¸ì—”ë“œ ê³„ì¸µ (Docker Container 1)"
        Frontend["FastAPI UI
FastAPI ê¸°ë°˜
Port: External"]
    end

    subgraph "ë°±ì—”ë“œ ê³„ì¸µ (Docker Container 2)"
        Backend["FastAPI ì„œë²„
ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
LLM ì—°ë™
Port: 8080"]
        LLM["OpenAI GPT-4o
í”„ë¡¬í”„íŠ¸ ìƒì„±"]
    end

    subgraph "Docker Network: nanococoa-network"
        subgraph "MCP ì„œë²„ (nanoCocoa_mcpserver)"
            MCPServer["MCP ì„œë²„
FastAPI
Port: 3000"]
        end

        subgraph "ëª¨ë¸ì„œë¹™ ê³„ì¸µ (nanoCocoa_aiserver)"
            ModelServer["FastAPI ëª¨ë¸ ì„œë²„
Port: 8000"]

            subgraph "AI ëª¨ë¸ íŒŒì´í”„ë¼ì¸"
                BiRefNet["BiRefNet
(ì´ë¯¸ì§€ ëˆ„ë¼)"]
                FLUX["FLUX.1-dev
(ë°°ê²½ ìƒì„±)"]
                SDXL["SDXL ControlNet
(3D í…ìŠ¤íŠ¸)"]
            end

            GPU["NVIDIA L4 GPU
24GB VRAM"]
        end
    end

    User -->|HTTP ìš”ì²­| Frontend
    Frontend -->|"REST API
Port 8080"| Backend
    Backend -->|LLM API| LLM
    Backend -->|"REST API
Port 8000"| ModelServer
    Claude -.->|"MCP Protocol
(SSE)"| MCPServer
    MCPServer -->|"REST API
Internal Network"| ModelServer
    ModelServer --> BiRefNet
    ModelServer --> FLUX
    ModelServer --> SDXL
    BiRefNet -.->|JIT ë¡œë”©| GPU
    FLUX -.->|JIT ë¡œë”©| GPU
    SDXL -.->|JIT ë¡œë”©| GPU
```

### 2.2. ê³„ì¸µë³„ ì—­í•  (Layer Responsibilities)

#### 2.2.1. í”„ë¡ íŠ¸ì—”ë“œ ê³„ì¸µ

**ì—­í• **: ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ì œê³µ ë° ì…ë ¥ ë°ì´í„° ìˆ˜ì§‘

- FastAPI ê¸°ë°˜ ì›¹ UI
- ì´ë¯¸ì§€ ì—…ë¡œë“œ (ìƒí’ˆ ì´ë¯¸ì§€)
- ê´‘ê³  ë¬¸êµ¬ ì…ë ¥ (í…ìŠ¤íŠ¸)
- ìƒì„± ì˜µì…˜ ì„¤ì • (ë°°ê²½ ìŠ¤íƒ€ì¼, í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ ë“±)
- ìƒì„± ê²°ê³¼ í‘œì‹œ ë° ë‹¤ìš´ë¡œë“œ

**í†µì‹  ë°©ì‹**: HTTP/REST API (â†’ ë°±ì—”ë“œ 8080 í¬íŠ¸)

#### 2.2.2. ë°±ì—”ë“œ ê³„ì¸µ

**ì—­í• **: ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì²˜ë¦¬ ë° ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—°ë™

- ì‚¬ìš©ì ìš”ì²­ ê²€ì¦ ë° ì „ì²˜ë¦¬
- LLM(GPT-4o) ì—°ë™ì„ í†µí•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
  - ì‚¬ìš©ì ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ AI ëª¨ë¸ì— ì í•©í•œ ì˜ë¬¸ í”„ë¡¬í”„íŠ¸ ìë™ ìƒì„±
  - ì˜ˆ: "ê±´ì–´ë¬¼ ëŒ€ë°• ì„¸ì¼" â†’ "Dried seafood products on a rustic wooden table, traditional Korean market atmosphere, vibrant colors, photorealistic"
- ëª¨ë¸ ì„œë²„ í˜¸ì¶œ ë° ì‘ë‹µ ê´€ë¦¬
- ì‘ì—… ìƒíƒœ ì¶”ì  ë° í´ë§ ì²˜ë¦¬
- ì—ëŸ¬ í•¸ë“¤ë§ ë° ì‚¬ìš©ì í”¼ë“œë°±

**í†µì‹  ë°©ì‹**:
- í”„ë¡ íŠ¸ì—”ë“œ â† HTTP/REST API (8080 í¬íŠ¸ ìˆ˜ì‹ )
- ëª¨ë¸ì„œë¹™ â†’ HTTP/REST API (8000 í¬íŠ¸ í˜¸ì¶œ)
- OpenAI API â†’ HTTPS

#### 2.2.3. ëª¨ë¸ì„œë¹™ ê³„ì¸µ (nanoCocoa_aiserver)

**ì—­í• **: AI ëª¨ë¸ ì¶”ë¡  ë° ì´ë¯¸ì§€ ìƒì„±

- FastAPI ê¸°ë°˜ REST API ì„œë²„
- GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ (JIT ë¡œë”©/ì–¸ë¡œë”©)
- ë¹„ë™ê¸° ì¶”ë¡  ì‘ì—… ì²˜ë¦¬ (ë©€í‹°í”„ë¡œì„¸ì‹±)
- 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
  - **Stage 1 (ë°°ê²½ ìƒì„±)**: BiRefNet ëˆ„ë¼ â†’ FLUX ë°°ê²½ ìƒì„± â†’ í•©ì„± ë° ë¦¬í„°ì¹­
  - **Stage 2 (í…ìŠ¤íŠ¸ ìƒì„±)**: SDXL ControlNet 3D í…ìŠ¤íŠ¸ ìƒì„± â†’ BiRefNet ë°°ê²½ ì œê±°
  - **Stage 3 (ìµœì¢… í•©ì„±)**: ë°°ê²½ + í…ìŠ¤íŠ¸ ë ˆì´ì–´ í•©ì„±

**Docker ë°°í¬**:
- ì»¨í…Œì´ë„ˆëª…: `nanococoa-aiserver`
- í¬íŠ¸: 8000 (ì™¸ë¶€ ë…¸ì¶œ)
- GPU ì ‘ê·¼: NVIDIA Driver, ëª¨ë“  GPU ì‚¬ìš© ê°€ëŠ¥
- ë³¼ë¥¨:
  - `/opt/huggingface` â†’ HuggingFace ëª¨ë¸ ìºì‹œ
  - `./nanoCocoa_aiserver/static/uploads` â†’ ì—…ë¡œë“œ íŒŒì¼
  - `./nanoCocoa_aiserver/static/results` â†’ ê²°ê³¼ íŒŒì¼
  - `./nanoCocoa_aiserver/logs` â†’ ë¡œê·¸

**í†µì‹  ë°©ì‹**:
- ë°±ì—”ë“œ â† HTTP/REST API (8000 í¬íŠ¸ ìˆ˜ì‹ )
- MCP ì„œë²„ â† HTTP/REST API (ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬, nanococoa-network)

**ìƒì„¸ ì•„í‚¤í…ì²˜**: [nanoCocoa_AI_Server_ì•„í‚¤í…ì²˜ì„¤ê³„.md](./nanoCocoa_AI_Server_ì•„í‚¤í…ì²˜ì„¤ê³„.md) ì°¸ì¡°

#### 2.2.4. MCP ì„œë²„ ê³„ì¸µ (nanoCocoa_mcpserver)

**ì—­í• **: MCP í”„ë¡œí† ì½œ ë¸Œë¦¿ì§€ ì„œë²„

- nanoCocoa_aiserver REST APIë¥¼ MCP í”„ë¡œí† ì½œë¡œ ë³€í™˜
- Claude Desktop/Codeì™€ ì—°ë™í•˜ì—¬ ìì—°ì–´ë¡œ ê´‘ê³  ì´ë¯¸ì§€ ìƒì„± ê°€ëŠ¥
- 8ê°œì˜ MCP ë„êµ¬ ì œê³µ (generate_ad_image, check_generation_status ë“±)
- SSE (Server-Sent Events) ì „ì†¡ ë°©ì‹

**Docker ë°°í¬**:
- ì»¨í…Œì´ë„ˆëª…: `nanococoa-mcpserver`
- í¬íŠ¸: 3000 (ì™¸ë¶€ ë…¸ì¶œ)
- ì˜ì¡´ì„±: nanoCocoa-aiserver (health check ëŒ€ê¸°)
- í™˜ê²½ë³€ìˆ˜:
  - `MCP_TRANSPORT=sse`
  - `MCP_PORT=3000`
  - `AISERVER_BASE_URL=http://nanococoa-aiserver:8000` (ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬)

**í†µì‹  ë°©ì‹**:
- Claude Desktop/Code â† MCP Protocol (SSE, 3000 í¬íŠ¸)
- nanoCocoa_aiserver â†’ HTTP/REST API (ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬)

---

## 3. ë°ì´í„° íë¦„ (Data Flow)

### 3.1. ì „ì²´ ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
sequenceDiagram
    participant User as ì‚¬ìš©ì
    participant FE as í”„ë¡ íŠ¸ì—”ë“œ (FastAPI)
    participant BE as ë°±ì—”ë“œ (FastAPI)
    participant LLM as OpenAI GPT-4o
    participant MS as ëª¨ë¸ì„œë¹™ (FastAPI)
    participant GPU as L4 GPU

    User->>FE: 1. ì´ë¯¸ì§€ ì—…ë¡œë“œ + ê´‘ê³  ë¬¸êµ¬ ì…ë ¥
    FE->>FE: 2. ì…ë ¥ ê²€ì¦
    FE->>BE: 3. POST /api/generate {image, text, options}

    BE->>BE: 4. ìš”ì²­ ê²€ì¦
    BE->>LLM: 5. í”„ë¡¬í”„íŠ¸ ìƒì„± ìš”ì²­ "ê±´ì–´ë¬¼ ëŒ€ë°• ì„¸ì¼"
    LLM-->>BE: 6. ì˜ë¬¸ í”„ë¡¬í”„íŠ¸ ë°˜í™˜ "Dried seafood..."

    BE->>MS: 7. POST /generate {input_image, bg_prompt, text_content}
    MS->>MS: 8. Job ID ìƒì„± Worker Process ìƒì„±
    MS-->>BE: 9. {job_id, status: "started"}
    BE-->>FE: 10. {job_id}
    FE-->>User: 11. "ìƒì„± ì¤‘..." í‘œì‹œ

    loop ì§„í–‰ ìƒí™© í´ë§ (Polling)
        FE->>BE: 12. GET /api/status/{job_id}
        BE->>MS: 13. GET /status/{job_id}

        MS->>MS: Stage 1 ì‹¤í–‰
        MS->>GPU: BiRefNet ë¡œë“œ
        GPU-->>MS: ëˆ„ë¼ ì´ë¯¸ì§€
        MS->>GPU: FLUX ë¡œë“œ
        GPU-->>MS: ë°°ê²½ ì´ë¯¸ì§€
        MS->>MS: í•©ì„± ë° ë¦¬í„°ì¹­

        MS-->>BE: 14. {status: "running", progress: 30%, step1_result}
        BE-->>FE: 15. {progress, step1_preview}
        FE-->>User: 16. ì§„í–‰ë¥  + ì¤‘ê°„ ê²°ê³¼ í‘œì‹œ

        MS->>MS: Stage 2 ì‹¤í–‰
        MS->>GPU: SDXL ë¡œë“œ
        GPU-->>MS: 3D í…ìŠ¤íŠ¸ ì´ë¯¸ì§€

        MS->>MS: Stage 3 ì‹¤í–‰ (ìµœì¢… í•©ì„±)
        MS-->>BE: 17. {status: "completed", final_result}
        BE-->>FE: 18. {status: "done", final_image}
    end

    FE-->>User: 19. ìµœì¢… ê²°ê³¼ í‘œì‹œ + ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
```

### 3.2. API ì—”ë“œí¬ì¸íŠ¸ ëª…ì„¸

#### 3.2.1. ë°±ì—”ë“œ API (Port 8080)

| ë©”ì„œë“œ | ê²½ë¡œ | ì„¤ëª… | ìš”ì²­ | ì‘ë‹µ |
|--------|------|------|------|------|
| POST | `/api/generate` | ê´‘ê³  ìƒì„± ì‹œì‘ | `{image, text, style}` | `{job_id}` |
| GET | `/api/status/{job_id}` | ì‘ì—… ìƒíƒœ ì¡°íšŒ | - | `{status, progress, result}` |
| POST | `/api/stop/{job_id}` | ì‘ì—… ì¤‘ë‹¨ | - | `{status: "stopped"}` |

**ìš”ì²­ ì˜ˆì‹œ (ë°±ì—”ë“œ)**:
```json
{
  "product_image": "base64_encoded_image...",
  "ad_text": "ê±´ì–´ë¬¼ ëŒ€ë°• ì„¸ì¼",
  "background_style": "ì „í†µì‹œì¥ ë¶„ìœ„ê¸°",
  "text_style": "ê³¨ë“œ í’ì„  í…ìŠ¤íŠ¸"
}
```

**ì‘ë‹µ ì˜ˆì‹œ (ë°±ì—”ë“œ)**:
```json
{
  "job_id": "uuid-v4",
  "status": "processing"
}
```

#### 3.2.2. ëª¨ë¸ì„œë¹™ API (Port 8000)

| ë©”ì„œë“œ | ê²½ë¡œ | ì„¤ëª… | ìš”ì²­ | ì‘ë‹µ |
|--------|------|------|------|------|
| POST | `/generate` | AI ìƒì„± ì‘ì—… ì‹œì‘ | `{input_image, bg_prompt, text_content, ...}` | `{job_id, status}` |
| GET | `/status/{job_id}` | ì‘ì—… ìƒíƒœ ë° ê²°ê³¼ ì¡°íšŒ | - | `{status, progress, images, metrics}` |
| POST | `/stop/{job_id}` | ì‘ì—… ê°•ì œ ì¤‘ë‹¨ | - | `{job_id, status}` |
| GET | `/health` | ì„œë²„ ìƒíƒœ í™•ì¸ | - | `{status, gpu_available}` |
| GET | `/fonts` | ì‚¬ìš© ê°€ëŠ¥í•œ í°íŠ¸ ëª©ë¡ | - | `[{name, path}]` |

**ìš”ì²­ ì˜ˆì‹œ (ëª¨ë¸ì„œë¹™)**:
```json
{
  "start_step": 1,
  "input_image": "base64_string...",
  "text_content": "Super Sale",
  "bg_prompt": "Wooden table in a cozy cafe, sunlight, realistic",
  "text_model_prompt": "Gold balloon text, 3d render",
  "strength": 0.6,
  "guidance_scale": 3.5,
  "test_mode": false
}
```

**ì‘ë‹µ ì˜ˆì‹œ (ëª¨ë¸ì„œë¹™)**:
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "running",
  "progress_percent": 45,
  "current_step": "step2_text_asset",
  "sub_step": "sdxl_text_generation",
  "message": "Generating 3D text...",
  "elapsed_sec": 67.3,
  "eta_seconds": 85,
  "step_eta_seconds": 42,
  "system_metrics": {
    "cpu_percent": 45.2,
    "ram_used_gb": 12.5,
    "ram_total_gb": 32.0,
    "ram_percent": 39.1,
    "gpu_info": [
      {
        "index": 0,
        "name": "NVIDIA L4",
        "vram_used_mb": 15234,
        "vram_total_mb": 24576,
        "vram_percent": 62.0,
        "utilization": 98
      }
    ]
  },
  "parameters": {
    "start_step": 1,
    "text_content": "Super Sale",
    "bg_prompt": "Wooden table in a cozy cafe..."
  },
  "step1_result": "base64_image_step1...",
  "step2_result": null,
  "final_result": null
}
```

---

## 4. ë°°í¬ ì•„í‚¤í…ì²˜ (Deployment Architecture)

### 4.1. Docker ì»¨í…Œì´ë„ˆ êµ¬ì„±

```mermaid
graph TB
    subgraph "GCP VM Instance (L4 GPU)"
        subgraph "Docker Network: nanococoa-network"
            Container1["Container 1
Frontend
FastAPI + FastAPI
Port: ì™¸ë¶€ ë…¸ì¶œ"]
            Container2["Container 2
Backend
FastAPI
Port: 8080 (ë‚´ë¶€)"]
            Container3["Container 3
nanoCocoa_aiserver
FastAPI + AI Models
Port: 8000 (ì™¸ë¶€)"]
            Container4["Container 4
nanoCocoa_mcpserver
MCP Bridge
Port: 3000 (ì™¸ë¶€)"]
        end

        GPU_Device["NVIDIA L4 GPU
Device: /dev/nvidia0"]
    end

    Internet["Internet"] -->|HTTPS| Container1
    Claude["Claude Desktop/Code"] -.->|MCP Protocol| Container4
    Container1 -->|Internal Network| Container2
    Container2 -->|Internal Network| Container3
    Container4 -->|Internal Network| Container3
    Container3 -.->|GPU Access| GPU_Device
```

### 4.2. Docker Compose êµ¬ì„±

ì‹¤ì œ ë°°í¬ ì¤‘ì¸ êµ¬ì„± (`src/docker-compose.yml`):

```yaml
version: '3.8'

services:
  nanococoa-aiserver:
    build:
      context: ./nanoCocoa_aiserver
      dockerfile: Dockerfile
    image: nanococoa-aiserver:latest
    container_name: nanococoa-aiserver

    # GPU ì„¤ì •
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    # í¬íŠ¸ ë§¤í•‘
    ports:
      - "8000:8000"

    # ë³¼ë¥¨ ë§ˆìš´íŠ¸
    volumes:
      # HuggingFace ìºì‹œ (ì™¸ë¶€ ìŠ¤í† ë¦¬ì§€)
      - /opt/huggingface:/root/.cache/huggingface
      # ì—…ë¡œë“œ/ê²°ê³¼ íŒŒì¼ (ì˜êµ¬ ì €ì¥)
      - ./nanoCocoa_aiserver/static/uploads:/app/static/uploads
      - ./nanoCocoa_aiserver/static/results:/app/static/results
      # ë¡œê·¸ (ì˜êµ¬ ì €ì¥)
      - ./nanoCocoa_aiserver/logs:/app/logs

    # í™˜ê²½ ë³€ìˆ˜
    env_file:
      - .env
    environment:
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - HF_HOME=/root/.cache/huggingface
      - DEVICE=cuda
      - AUTO_UNLOAD_DEFAULT=true

    # ì¬ì‹œì‘ ì •ì±…
    restart: unless-stopped

    # í—¬ìŠ¤ì²´í¬
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

    networks:
      - nanococoa-network

  nanococoa-mcpserver:
    build:
      context: ./nanoCocoa_mcpserver
      dockerfile: Dockerfile
    image: nanococoa-mcpserver:latest
    container_name: nanococoa-mcpserver

    # í¬íŠ¸ ë§¤í•‘
    ports:
      - "3000:3000"

    # ë³¼ë¥¨ ë§ˆìš´íŠ¸ (ì´ë¯¸ì§€ íŒŒì¼ ê³µìœ )
    volumes:
      - ./nanoCocoa_aiserver/static/uploads:/app/static/uploads
      - ./nanoCocoa_aiserver/static/results:/app/static/results

    # í™˜ê²½ ë³€ìˆ˜
    environment:
      - MCP_TRANSPORT=sse
      - MCP_PORT=3000
      - MCP_HOST=0.0.0.0
      - AISERVER_BASE_URL=http://nanococoa-aiserver:8000
      - LOG_LEVEL=INFO

    # ì˜ì¡´ì„± (AI ì„œë²„ê°€ healthy ìƒíƒœì¼ ë•Œë§Œ ì‹œì‘)
    depends_on:
      nanococoa-aiserver:
        condition: service_healthy

    # ì¬ì‹œì‘ ì •ì±…
    restart: unless-stopped

    # í—¬ìŠ¤ì²´í¬
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

    networks:
      - nanococoa-network

networks:
  nanococoa-network:
    name: nanococoa-network
    driver: bridge
```

**ì£¼ìš” íŠ¹ì§•**:
- GPU ë¦¬ì†ŒìŠ¤ë¥¼ AI ì„œë²„ì—ë§Œ í• ë‹¹
- MCP ì„œë²„ëŠ” AI ì„œë²„ì˜ health checkì´ ì„±ê³µí•œ í›„ì— ì‹œì‘ (`depends_on` ì¡°ê±´)
- ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬ (`nanococoa-network`)ë¥¼ í†µí•´ ì„œë¹„ìŠ¤ ê°„ í†µì‹ 
- ë³¼ë¥¨ ë§ˆìš´íŠ¸ë¡œ ë°ì´í„° ì˜êµ¬ ì €ì¥ ë° ê³µìœ 

### 4.3. í¬íŠ¸ êµ¬ì„± (Port Configuration)

| ì»¨í…Œì´ë„ˆ | ë‚´ë¶€ í¬íŠ¸ | ì™¸ë¶€ í¬íŠ¸ | ì ‘ê·¼ ë²”ìœ„ | ìš©ë„ |
|----------|----------|----------|-----------|------|
| **í”„ë¡ íŠ¸ì—”ë“œ** | 8501 | 80 (HTTPS) | Public | ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ |
| **ë°±ì—”ë“œ** | 8080 | 8080 | Internal Only | ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ API |
| **nanoCocoa_aiserver** | 8000 | 8000 | Public | AI ëª¨ë¸ ì¶”ë¡  API |
| **nanoCocoa_mcpserver** | 3000 | 3000 | Public | MCP í”„ë¡œí† ì½œ ë¸Œë¦¿ì§€ |

**ì°¸ê³ **:
- AI ì„œë²„(8000)ì™€ MCP ì„œë²„(3000)ëŠ” ì™¸ë¶€ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ í¬íŠ¸ ë…¸ì¶œ
- MCP ì„œë²„ëŠ” ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ `http://nanococoa-aiserver:8000`ìœ¼ë¡œ AI ì„œë²„ ì ‘ê·¼

---

## 5. í”„ë¡ íŠ¸ì—”ë“œ ì„¤ê³„ (Frontend Design)

### 5.1. ê¸°ìˆ  ìŠ¤íƒ

- **í”„ë ˆì„ì›Œí¬**: FastAPI
- **ì–¸ì–´**: Python 3.11+
- **ì„œë²„**: FastAPI (FastAPI ì„ë² ë”©)
- **í†µì‹ **: HTTP REST API (ë°±ì—”ë“œ 8080 í¬íŠ¸)

### 5.2. ì£¼ìš” ê¸°ëŠ¥

1. **ì´ë¯¸ì§€ ì—…ë¡œë“œ**
   - ë“œë˜ê·¸ ì•¤ ë“œë¡­ ë˜ëŠ” íŒŒì¼ ì„ íƒ
   - ì§€ì› í˜•ì‹: JPG, PNG
   - ì´ë¯¸ì§€ ë¯¸ë¦¬ë³´ê¸°

2. **ê´‘ê³  ë¬¸êµ¬ ì…ë ¥**
   - í…ìŠ¤íŠ¸ ì…ë ¥ í•„ë“œ
   - ìµœëŒ€ ê¸¸ì´ ì œí•œ (ì˜ˆ: 20ì)

3. **ìŠ¤íƒ€ì¼ ì„ íƒ**
   - ë°°ê²½ ìŠ¤íƒ€ì¼ (ì „í†µì‹œì¥, ê³ ê¸‰ìŠ¤ëŸ¬ìš´, ë¯¸ë‹ˆë©€ ë“±)
   - í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ (ê³¨ë“œ í’ì„ , ë„¤ì˜¨, 3D ë©”íƒˆ ë“±)

4. **ìƒì„± ì§„í–‰ ìƒí™© í‘œì‹œ**
   - ì§„í–‰ë¥  ë°” (Progress Bar)
   - í˜„ì¬ ë‹¨ê³„ í‘œì‹œ (ëˆ„ë¼ ì²˜ë¦¬ ì¤‘, ë°°ê²½ ìƒì„± ì¤‘, í…ìŠ¤íŠ¸ ìƒì„± ì¤‘)
   - ì¤‘ê°„ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°

5. **ê²°ê³¼ ë‹¤ìš´ë¡œë“œ**
   - ìµœì¢… ì´ë¯¸ì§€ í‘œì‹œ
   - ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ (PNG í˜•ì‹)

### 5.3. UI í”Œë¡œìš°

```
[ì´ë¯¸ì§€ ì—…ë¡œë“œ] â†’ [ê´‘ê³  ë¬¸êµ¬ ì…ë ¥] â†’ [ìŠ¤íƒ€ì¼ ì„ íƒ] â†’ [ìƒì„± ë²„íŠ¼ í´ë¦­]
                                                              â†“
                                    [ì§„í–‰ ìƒí™© í‘œì‹œ + ì¤‘ê°„ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°]
                                                              â†“
                                          [ìµœì¢… ê²°ê³¼ í‘œì‹œ + ë‹¤ìš´ë¡œë“œ]
```

### 5.4. ë°±ì—”ë“œ ì—°ë™

```python
import streamlit as st
import requests
import time

BACKEND_URL = "http://backend:8080"

# 1. ìƒì„± ìš”ì²­
def generate_ad(image, text, style):
    response = requests.post(
        f"{BACKEND_URL}/api/generate",
        json={
            "product_image": image,
            "ad_text": text,
            "background_style": style["background"],
            "text_style": style["text"]
        }
    )
    return response.json()["job_id"]

# 2. ìƒíƒœ í´ë§
def poll_status(job_id):
    while True:
        response = requests.get(f"{BACKEND_URL}/api/status/{job_id}")
        data = response.json()

        if data["status"] == "completed":
            return data["result"]
        elif data["status"] == "failed":
            raise Exception(data["error"])

        # ì§„í–‰ ìƒí™© í‘œì‹œ
        st.progress(data["progress"] / 100)
        st.text(data["message"])

        time.sleep(2)  # 2ì´ˆë§ˆë‹¤ í´ë§
```

---

## 6. ë°±ì—”ë“œ ì„¤ê³„ (Backend Design)

### 6.1. ê¸°ìˆ  ìŠ¤íƒ

- **í”„ë ˆì„ì›Œí¬**: FastAPI
- **ì–¸ì–´**: Python 3.11+
- **ì™¸ë¶€ API**: OpenAI GPT-4o
- **í†µì‹ **: HTTP REST API

### 6.2. ì£¼ìš” ê¸°ëŠ¥

1. **í”„ë¡¬í”„íŠ¸ ìƒì„± (LLM ì—°ë™)**
   - ì‚¬ìš©ì ì…ë ¥ (í•œê¸€ í…ìŠ¤íŠ¸ + ìŠ¤íƒ€ì¼ ì„ íƒ)ì„ ë¶„ì„
   - GPT-4oë¥¼ í†µí•´ AI ëª¨ë¸ì— ì í•©í•œ ì˜ë¬¸ í”„ë¡¬í”„íŠ¸ ìƒì„±

   **ì˜ˆì‹œ**:
   ```
   ì…ë ¥: "ê±´ì–´ë¬¼ ëŒ€ë°• ì„¸ì¼", ìŠ¤íƒ€ì¼: "ì „í†µì‹œì¥ ë¶„ìœ„ê¸°"

   LLM í”„ë¡¬í”„íŠ¸:
   "Generate a professional English prompt for an AI image generation model.
   Input: Korean text 'ê±´ì–´ë¬¼ ëŒ€ë°• ì„¸ì¼', style: traditional market atmosphere.
   Output: Detailed prompt for background generation and text style."

   LLM ì‘ë‹µ:
   {
     "background_prompt": "Traditional Korean market stall with dried seafood products, wooden display, warm lighting, authentic atmosphere, photorealistic, 8k",
     "text_prompt": "3D render of bold Korean text 'ëŒ€ë°• ì„¸ì¼', red and gold colors, festive style, hanging banner effect"
   }
   ```

2. **ëª¨ë¸ ì„œë²„ í˜¸ì¶œ**
   - ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì ì´ë¯¸ì§€ë¥¼ ëª¨ë¸ ì„œë²„ë¡œ ì „ì†¡
   - Job ID ë°˜í™˜

3. **ìƒíƒœ ê´€ë¦¬**
   - ëª¨ë¸ ì„œë²„ì˜ `/status/{job_id}` í´ë§
   - ì§„í–‰ ìƒí™©ì„ í”„ë¡ íŠ¸ì—”ë“œì— ì „ë‹¬

4. **ì—ëŸ¬ í•¸ë“¤ë§**
   - ëª¨ë¸ ì„œë²„ ì¥ì•  ì‹œ ì¬ì‹œë„ ë¡œì§
   - ì‚¬ìš©ìì—ê²Œ ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜

### 6.3. API êµ¬í˜„ ì˜ˆì‹œ

```python
from FastAPI import FastAPI, HTTPException
from openai import OpenAI
import httpx

app = FastAPI()
client = OpenAI()

MODEL_SERVER_URL = "http://model-serving:8000"

@app.post("/api/generate")
async def generate_ad(request: GenerateRequest):
    # 1. LLMì„ í†µí•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompts = await generate_prompts_with_llm(
        request.ad_text,
        request.background_style,
        request.text_style
    )

    # 2. ëª¨ë¸ ì„œë²„ í˜¸ì¶œ
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{MODEL_SERVER_URL}/generate",
            json={
                "input_image": request.product_image,
                "text_content": request.ad_text,
                "bg_prompt": prompts["background"],
                "text_model_prompt": prompts["text"],
                "start_step": 1
            },
            timeout=10.0
        )

    return response.json()

async def generate_prompts_with_llm(text, bg_style, text_style):
    completion = client.chat.completions.create(
        model="gpt-5-mini",
        messages=[
            {
                "role": "system",
                "content": "You are an expert prompt engineer for AI image generation."
            },
            {
                "role": "user",
                "content": f"""Generate detailed English prompts for:
                - Text: {text}
                - Background style: {bg_style}
                - Text style: {text_style}

                Return JSON with 'background' and 'text' keys."""
            }
        ]
    )

    # JSON íŒŒì‹±
    result = completion.choices[0].message.content
    return eval(result)  # ì‹¤ì œë¡œëŠ” json.loads() ì‚¬ìš©
```

---

## 7. ëª¨ë¸ì„œë¹™ ì„¤ê³„ (Model Serving Design)

### 7.1. ìƒì„¸ ì„¤ê³„ ë¬¸ì„œ

ëª¨ë¸ì„œë¹™ ê³„ì¸µì˜ ìƒì„¸í•œ ì•„í‚¤í…ì²˜ëŠ” ë³„ë„ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”:

**ğŸ“„ [nanoCocoa_AI_Server_ì•„í‚¤í…ì²˜ì„¤ê³„.md](./nanoCocoa_AI_Server_ì•„í‚¤í…ì²˜ì„¤ê³„.md)**

### 7.2. ì£¼ìš” íŠ¹ì§• ìš”ì•½

- **JIT (Just-In-Time) ëª¨ë¸ ë¡œë”©**: ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ìœ„í•´ í•„ìš”í•  ë•Œë§Œ ëª¨ë¸ì„ GPUì— ë¡œë“œ
- **ë¹„ë™ê¸° ì²˜ë¦¬**: `multiprocessing`ì„ í™œìš©í•œ Non-blocking ì¶”ë¡ 
- **3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸**: ë°°ê²½ ìƒì„± â†’ í…ìŠ¤íŠ¸ ìƒì„± â†’ ìµœì¢… í•©ì„±
- **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: CPU/RAM/GPU ì‚¬ìš©ë¥  ì‹¤ì‹œê°„ ì œê³µ
- **ë‹¨ê³„ë³„ ì¬ì‹œì‘**: ì‹¤íŒ¨ ì‹œ íŠ¹ì • ë‹¨ê³„ë¶€í„° ì¬ì‹¤í–‰ ê°€ëŠ¥

### 7.3. í•µì‹¬ API

```python
# ëª¨ë¸ ì„œë²„ í•µì‹¬ ë¡œì§ (ê°„ëµí™”)
@router.post("/generate")
async def generate_ad(req: GenerateRequest):
    job_id = str(uuid.uuid4())

    # Worker Process ìƒì„± (ë¹„ë™ê¸° ì²˜ë¦¬)
    p = multiprocessing.Process(
        target=worker_process,
        args=(job_id, req.model_dump(), shared_jobs[job_id])
    )
    p.start()

    return {"job_id": job_id, "status": "started"}

def worker_process(job_id, input_data, shared_state):
    # Step 1: ë°°ê²½ ìƒì„±
    step1_result = generate_background(
        input_data["input_image"],
        input_data["bg_prompt"]
    )
    shared_state["step1_result"] = step1_result

    # Step 2: í…ìŠ¤íŠ¸ ìƒì„±
    step2_result = generate_text(
        input_data["text_content"],
        input_data["text_model_prompt"]
    )
    shared_state["step2_result"] = step2_result

    # Step 3: ìµœì¢… í•©ì„±
    final_result = compose_layers(step1_result, step2_result)
    shared_state["final_result"] = final_result
    shared_state["status"] = "completed"
```

---

## 8. ë³´ì•ˆ ë° ì•ˆì •ì„± (Security & Reliability)

### 8.1. ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

1. **API í‚¤ ê´€ë¦¬**
   - í™˜ê²½ ë³€ìˆ˜ë¡œ ê´€ë¦¬ (`.env` íŒŒì¼)
   - Docker secrets í™œìš©
   - GitHubì— ì—…ë¡œë“œ ê¸ˆì§€

2. **ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬**
   - ëª¨ë¸ì„œë¹™ê³¼ ë°±ì—”ë“œëŠ” ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬ì—ì„œë§Œ ì ‘ê·¼
   - í”„ë¡ íŠ¸ì—”ë“œë§Œ ì™¸ë¶€ ë…¸ì¶œ

3. **ì…ë ¥ ê²€ì¦**
   - ì´ë¯¸ì§€ í¬ê¸° ì œí•œ (ì˜ˆ: 10MB)
   - í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
   - íŒŒì¼ í˜•ì‹ ê²€ì¦

### 8.2. ì•ˆì •ì„± í™•ë³´

1. **ì—ëŸ¬ í•¸ë“¤ë§**
   - GPU OOM ë°œìƒ ì‹œ ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬ ë° ì¬ì‹œë„
   - ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ ìƒì„¸ ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜

2. **ë¦¬ì†ŒìŠ¤ ê´€ë¦¬**
   - ë‹¨ì¼ ì‘ì—…ë§Œ ì²˜ë¦¬ (ë™ì‹œì„± ì œì–´)
   - ì‘ì—… í êµ¬í˜„ (í–¥í›„ í™•ì¥)

3. **ëª¨ë‹ˆí„°ë§**
   - ì‹¤ì‹œê°„ GPU/CPU/RAM ì‚¬ìš©ë¥  ì¶”ì 
   - ì‘ì—… ì‹œê°„ ë¡œê¹…

---

## 9. í–¥í›„ í™•ì¥ ê³„íš (Future Enhancements)

### 9.1. ë‹¨ê¸° ê³„íš (1~2ì£¼)

- [ ] ì‘ì—… í ì‹œìŠ¤í…œ êµ¬í˜„ (ë‹¤ì¤‘ ì‚¬ìš©ì ì§€ì›)
- [ ] Redis ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬ (ì¸ë©”ëª¨ë¦¬ â†’ ì˜êµ¬ ì €ì¥)
- [ ] ì‚¬ìš©ì ì¸ì¦ ë° ì„¸ì…˜ ê´€ë¦¬

### 9.2. ì¤‘ê¸° ê³„íš (1~2ê°œì›”)

- [ ] ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ (ì‘ì—… ì´ë ¥ ì €ì¥)
- [ ] ì‚¬ìš©ì ê°¤ëŸ¬ë¦¬ ê¸°ëŠ¥
- [ ] A/B í…ŒìŠ¤íŠ¸ ì§€ì› (ì—¬ëŸ¬ ë²„ì „ ìƒì„± ë° ë¹„êµ)

---

## 10. ì°¸ê³  ë¬¸ì„œ (References)

- [ê³ ê¸‰_í”„ë¡œì íŠ¸_ìˆ˜í–‰_ê³„íš_ë°_í™˜ê²½_ê²€í† _ë³´ê³ ì„œ.md](./ê³ ê¸‰_í”„ë¡œì íŠ¸_ìˆ˜í–‰_ê³„íš_ë°_í™˜ê²½_ê²€í† _ë³´ê³ ì„œ.md)
- [nanoCocoa_AI_Server_ì•„í‚¤í…ì²˜ì„¤ê³„.md](./nanoCocoa_AI_Server_ì•„í‚¤í…ì²˜ì„¤ê³„.md)

---

**ë¬¸ì„œ ë³€ê²½ ì´ë ¥**

| ë²„ì „ | ë‚ ì§œ | ì‘ì„±ì | ë³€ê²½ ë‚´ìš© |
|------|------|--------|-----------|
| v1.0 | 2026.01.01 | ê¹€ëª…í™˜ | ì´ˆì•ˆ ì‘ì„± |
