---
layout: default
title: "LLM ê´‘ê³  ìƒì„± ê°€ì´ë“œ"
description: "LLM ê´‘ê³  ìƒì„± ê°€ì´ë“œ"
date: 2026-01-13
author: "ê¹€ëª…í™˜"
cache-control: no-cache
expires: 0
pragma: no-cache
---

# LLM ê´‘ê³  ìƒì„± ê°€ì´ë“œ

## 1. ê°œìš”

ë³¸ ë¬¸ì„œëŠ” LLMAdapterë¥¼ ì‚¬ìš©í•˜ì—¬ ìì—°ì–´ë¡œ ê´‘ê³  ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

### 1.1. íŠ¹ì§•

- ìì—°ì–´ ì…ë ¥ë§Œìœ¼ë¡œ ê´‘ê³  ìƒì„±
- ë³µì¡í•œ API íŒŒë¼ë¯¸í„° ìë™ ìƒì„±
- OpenAI LLMì´ MCP ë„êµ¬ë¥¼ ìë™ í˜¸ì¶œ
- ë¹„ë™ê¸° ì‘ì—… ì§€ì› (job_id ê¸°ë°˜ í´ë§)

### 1.2. êµ¬ì¡°

```
ì‚¬ìš©ì (ìì—°ì–´) â†’ LLMAdapter â†’ OpenAI LLM â†’ MCP ë„êµ¬ í˜¸ì¶œ â†’ AI ì„œë²„
```

---

## 2. ê¸°ë³¸ ì‚¬ìš©ë²•

### 2.1. í™˜ê²½ ì„¤ì •

```python
import os
from pathlib import Path
from mcpadapter import LLMAdapter

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
openai_api_key = os.getenv("OPENAI_API_KEY")
mcp_server_url = "http://localhost:3000"

# ê²½ë¡œ ì„¤ì •
product_image_path = "/path/to/product.png"
output_image_path = "/path/to/result.png"
```

### 2.2. ê°„ë‹¨í•œ ê´‘ê³  ìƒì„±

```python
async def simple_ad_generation():
    """ê°€ì¥ ê°„ë‹¨í•œ ê´‘ê³  ìƒì„± ì˜ˆì œ"""
    async with LLMAdapter(
        openai_api_key=openai_api_key,
        mcp_server_url=mcp_server_url,
        model="gpt-5-mini"
    ) as adapter:
        
        # ìì—°ì–´ ìš”ì²­
        response = await adapter.chat(
            "product.pngë¡œ ì—¬ë¦„ ì„¸ì¼ ê´‘ê³  ë§Œë“¤ì–´ì¤˜"
        )
        print(response)
```

---

## 3. ìì—°ì–´ ìš”ì²­ ì‘ì„± ê·œì¹™

### 3.1. ìš”ì²­ êµ¬ì¡°

```python
user_request = f"""
ì‚¬ìš©ì: {ìš”ì²­_ë‚´ìš©}

- product_image_path: "{product_image_path}"
- save_output_path: "{output_image_path}"
- text_content: "{ê´‘ê³ _ë¬¸êµ¬}"
- wait_for_completion: {true|false}
- composition_mode: "{overlay|natural_blend}"

ëª¨ë“  í”„ë¡¬í”„íŠ¸ëŠ” ì˜ë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
```

### 3.2. í•„ìˆ˜ ì •ë³´

| í•­ëª© | ì„¤ëª… | ì˜ˆì‹œ |
|------|------|------|
| `ìš”ì²­_ë‚´ìš©` | ê´‘ê³  ìœ í˜• ë° ì˜ë„ | "ë°”ë‚˜ë‚˜ íŠ¹ê°€ ê´‘ê³  ë§Œë“¤ì–´ì¤˜" |
| `product_image_path` | ì œí’ˆ ì´ë¯¸ì§€ ê²½ë¡œ (ì ˆëŒ€ê²½ë¡œ) | "/app/static/uploads/banana.png" |
| `save_output_path` | ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ì ˆëŒ€ê²½ë¡œ) | "/app/static/results/ad.png" |
| `text_content` | ê´‘ê³  ë¬¸êµ¬ (í•œê¸€/ì˜ë¬¸) | "ë§›ìˆëŠ”ë°”ë‚˜ë‚˜ 2500ì›" |

### 3.3. ì„ íƒ íŒŒë¼ë¯¸í„°

| í•­ëª© | ì„¤ëª… | ê¸°ë³¸ê°’ | ì˜µì…˜ |
|------|------|--------|------|
| `wait_for_completion` | ì™„ë£Œ ëŒ€ê¸° ì—¬ë¶€ | `true` | `true`, `false` |
| `composition_mode` | í•©ì„± ëª¨ë“œ | `"overlay"` | `"overlay"`, `"natural_blend"` |
| `ad_type` | ê´‘ê³  ìœ í˜• | (ìë™ ì¶”ë¡ ) | `"sale"`, `"premium"`, `"casual"` |
| `font_name` | í°íŠ¸ íŒŒì¼ëª… | (ìë™ ì„ íƒ) | `"NanumGothicBold.ttf"` |

---

## 4. ì‹¤í–‰ ëª¨ë“œ

### 4.1. ì¦‰ì‹œ ì‹¤í–‰ ëª¨ë“œ (max_tool_calls=1)

**íŠ¹ì§•**
- ì‚¬ìš©ì í™•ì¸ ì—†ì´ ì¦‰ì‹œ ë„êµ¬ í˜¸ì¶œ
- ì¶”ê°€ ì§ˆë¬¸ ì—†ìŒ
- job_id ë°˜í™˜ í›„ ì¢…ë£Œ

**ì‚¬ìš© ì˜ˆ**

```python
async def immediate_generation():
    """ì¦‰ì‹œ ì‹¤í–‰ ëª¨ë“œ ì˜ˆì œ"""
    async with LLMAdapter(
        openai_api_key=openai_api_key,
        mcp_server_url=mcp_server_url,
        model="gpt-5-mini"
    ) as adapter:
        
        user_request = f"""
ì‚¬ìš©ì: ë°”ë‚˜ë‚˜ íŠ¹ê°€ ê´‘ê³  ë§Œë“¤ì–´ì¤˜

- product_image_path: "{product_image_path}"
- save_output_path: "{output_image_path}"
- text_content: "ë§›ìˆëŠ”ë°”ë‚˜ë‚˜ 2500ì›"
- wait_for_completion: false
- composition_mode: "overlay"

ëª¨ë“  í”„ë¡¬í”„íŠ¸ëŠ” ì˜ë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
        
        # max_tool_calls=1: ì¦‰ì‹œ ì‹¤í–‰
        response = await adapter.chat(user_request, max_tool_calls=1)
        
        # job_id ì¶”ì¶œ (ë„êµ¬ ì‘ë‹µì—ì„œ)
        tool_response = None
        for msg in reversed(adapter.conversation_history):
            if msg.get("role") == "tool":
                tool_response = msg.get("content")
                break
        
        # JSON íŒŒì‹±
        import json
        tool_data = json.loads(tool_response)
        job_id = tool_data["job_id"]
        
        return job_id
```

### 4.2. ëŒ€í™” ëª¨ë“œ (max_tool_calls > 1)

**íŠ¹ì§•**
- LLMì´ ì˜µì…˜ ì œì‹œ ë° ì‚¬ìš©ì í™•ì¸ ìš”ì²­
- ì—¬ëŸ¬ ë²ˆì˜ ëŒ€í™” ê°€ëŠ¥
- ìµœì¢… ìŠ¹ì¸ í›„ ë„êµ¬ í˜¸ì¶œ

**ì‚¬ìš© ì˜ˆ**

```python
async def interactive_generation():
    """ëŒ€í™” ëª¨ë“œ ì˜ˆì œ"""
    async with LLMAdapter(
        openai_api_key=openai_api_key,
        mcp_server_url=mcp_server_url,
        model="gpt-5-mini"
    ) as adapter:
        
        # ì²« ìš”ì²­
        response1 = await adapter.chat(
            "ì—¬ë¦„ ì„¸ì¼ ê´‘ê³  ë§Œë“¤ê³  ì‹¶ì–´",
            max_tool_calls=5
        )
        print(response1)  # LLMì´ ì˜µì…˜ ì œì‹œ
        
        # ì‚¬ìš©ì ì„ íƒ
        response2 = await adapter.chat(
            "Aì•ˆìœ¼ë¡œ ì§„í–‰í•´ì¤˜",
            max_tool_calls=5
        )
        print(response2)  # ê´‘ê³  ìƒì„± ì™„ë£Œ
```

---

## 5. ì‘ì—… ìƒíƒœ í™•ì¸ (ë¹„ë™ê¸° í´ë§)

### 5.1. wait_for_completion=false ì‚¬ìš© ì´ìœ 

- ì¥ì‹œê°„ ì‘ì—…(30ì´ˆ~5ë¶„)ì—ì„œ íƒ€ì„ì•„ì›ƒ ë°©ì§€
- ì„œë²„ ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì  ì‚¬ìš©
- í´ë¼ì´ì–¸íŠ¸ê°€ ì£¼ê¸°ì ìœ¼ë¡œ ìƒíƒœ í™•ì¸

### 5.2. í´ë§ êµ¬í˜„

```python
import asyncio
import json
from mcpadapter import MCPClient

async def check_ad_generation_status(
    job_id: str,
    save_result_path: str,
    max_attempts: int = 300,
    interval: int = 10
):
    """
    ì‘ì—… ìƒíƒœ í™•ì¸ ë° ì™„ë£Œ ì‹œ ì´ë¯¸ì§€ ì €ì¥
    
    Args:
        job_id: ì‘ì—… ID (generate_ad_imageì—ì„œ ë°˜í™˜)
        save_result_path: ì™„ë£Œ ì‹œ ì €ì¥í•  ì´ë¯¸ì§€ ê²½ë¡œ
        max_attempts: ìµœëŒ€ ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 300)
        interval: í™•ì¸ ê°„ê²©(ì´ˆ, ê¸°ë³¸ê°’: 10)
    
    Returns:
        ìµœì¢… ìƒíƒœ ê²°ê³¼ (dict)
    """
    async with MCPClient(
        base_url=mcp_server_url,
        timeout=30
    ) as client:
        
        attempt = 0
        while attempt < max_attempts:
            await asyncio.sleep(interval)
            attempt += 1
            
            # ìƒíƒœ í™•ì¸ (save_result_path í•„ìˆ˜!)
            status_result = await client.call_tool(
                "check_generation_status",
                {
                    "job_id": job_id,
                    "save_result_path": save_result_path
                }
            )
            
            # JSON íŒŒì‹±
            status_data = json.loads(status_result) if isinstance(status_result, str) else status_result
            status = status_data.get("status")
            progress = status_data.get("progress_percent", 0)
            
            print(f"[{attempt}/{max_attempts}] status={status}, progress={progress}%")
            
            if status == "completed":
                print(f"âœ… ì‘ì—… ì™„ë£Œ! ì´ë¯¸ì§€ ì €ì¥: {save_result_path}")
                return status_data
            elif status == "failed":
                print(f"âŒ ì‘ì—… ì‹¤íŒ¨: {status_data.get('message')}")
                return status_data
            else:
                print(f"â³ ì§„í–‰ ì¤‘... (ë‹¨ê³„: {status_data.get('current_step')})")
        
        print(f"â° íƒ€ì„ì•„ì›ƒ: {max_attempts * interval}ì´ˆ ì´ˆê³¼")
        return {"status": "timeout"}
```

### 5.3. ì „ì²´ ì›Œí¬í”Œë¡œìš°

```python
async def full_workflow():
    """ì „ì²´ ê´‘ê³  ìƒì„± ì›Œí¬í”Œë¡œìš°"""
    
    # Step 1: ê´‘ê³  ìƒì„± ìš”ì²­ (ì¦‰ì‹œ ì‹¤í–‰)
    job_id = await immediate_generation()
    print(f"Job ID: {job_id}")
    
    # Step 2: ìƒíƒœ í™•ì¸ (í´ë§)
    status_result = await check_ad_generation_status(
        job_id=job_id,
        save_result_path=output_image_path
    )
    
    # Step 3: ê²°ê³¼ í™•ì¸
    if status_result["status"] == "completed":
        print(f"ê´‘ê³  ìƒì„± ì™„ë£Œ: {output_image_path}")
        # ì´ë¯¸ì§€ í‘œì‹œ (Jupyter)
        from IPython.display import Image, display
        display(Image(filename=str(output_image_path)))
    else:
        print(f"ê´‘ê³  ìƒì„± ì‹¤íŒ¨: {status_result}")
```

---

## 6. ìì—°ì–´ ìš”ì²­ í…œí”Œë¦¿

### 6.1. ì„¸ì¼ ê´‘ê³ 

```python
user_request = f"""
ì‚¬ìš©ì: ì—¬ë¦„ ì„¸ì¼ ê´‘ê³  ë§Œë“¤ì–´ì¤˜

- product_image_path: "{product_image_path}"
- save_output_path: "{output_image_path}"
- text_content: "50% í• ì¸"
- ad_type: "sale"
- wait_for_completion: false

ìš”êµ¬ì‚¬í•­:
- ë°ê³  ì—­ë™ì ì¸ ë°°ê²½
- êµµì€ í°íŠ¸ ì‚¬ìš©
- ë¹¨ê°„ìƒ‰/ë…¸ë€ìƒ‰ ê³„ì—´

ëª¨ë“  í”„ë¡¬í”„íŠ¸ëŠ” ì˜ë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
```

### 6.2. í”„ë¦¬ë¯¸ì—„ ê´‘ê³ 

```python
user_request = f"""
ì‚¬ìš©ì: ëª…í’ˆ í”„ë¦¬ë¯¸ì—„ ê´‘ê³  ë§Œë“¤ì–´ì¤˜

- product_image_path: "{product_image_path}"
- save_output_path: "{output_image_path}"
- text_content: "Limited Edition"
- ad_type: "premium"
- wait_for_completion: false

ìš”êµ¬ì‚¬í•­:
- ê³ ê¸‰ìŠ¤ëŸ¬ìš´ ê²€ì€ ë°°ê²½
- ìš°ì•„í•œ ì„¸ë¦¬í”„ í°íŠ¸
- ê¸ˆìƒ‰ ê°•ì¡°

ëª¨ë“  í”„ë¡¬í”„íŠ¸ëŠ” ì˜ë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
```

### 6.3. ìºì£¼ì–¼ ê´‘ê³ 

```python
user_request = f"""
ì‚¬ìš©ì: ì¹œêµ¬ë“¤ê³¼ í•¨ê»˜í•˜ëŠ” ì¹´í˜ ê´‘ê³  ë§Œë“¤ì–´ì¤˜

- product_image_path: "{product_image_path}"
- save_output_path: "{output_image_path}"
- text_content: "í•¨ê»˜ ì¦ê¸°ëŠ” ì‹œê°„"
- ad_type: "casual"
- wait_for_completion: false

ìš”êµ¬ì‚¬í•­:
- ë”°ëœ»í•˜ê³  ì¹œê·¼í•œ ë¶„ìœ„ê¸°
- ì†ê¸€ì”¨ ìŠ¤íƒ€ì¼ í°íŠ¸
- íŒŒìŠ¤í…” í†¤

ëª¨ë“  í”„ë¡¬í”„íŠ¸ëŠ” ì˜ë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
```

---

## 7. ìì—°ì–´ ìš”ì²­ ì‘ì„± íŒ

### 7.1. í•µì‹¬ ì›ì¹™

1. **ëª…í™•í•œ ì˜ë„**: "ë°”ë‚˜ë‚˜ íŠ¹ê°€ ê´‘ê³  ë§Œë“¤ì–´ì¤˜"
2. **í•„ìˆ˜ ì •ë³´ ì œê³µ**: ì´ë¯¸ì§€ ê²½ë¡œ, ê´‘ê³  ë¬¸êµ¬
3. **ìŠ¤íƒ€ì¼ ê°€ì´ë“œ**: ìƒ‰ìƒ, í°íŠ¸, ë¶„ìœ„ê¸° ëª…ì‹œ
4. **ì˜ë¬¸ í”„ë¡¬í”„íŠ¸ ì§€ì‹œ**: "ëª¨ë“  í”„ë¡¬í”„íŠ¸ëŠ” ì˜ë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”"

### 7.2. ì¢‹ì€ ì˜ˆì‹œ

```python
# âœ… ì¢‹ì€ ì˜ˆì‹œ
user_request = """
ì‚¬ìš©ì: ë°”ë‚˜ë‚˜ íŠ¹ê°€ ê´‘ê³  ë§Œë“¤ì–´ì¤˜

- product_image_path: "/app/static/uploads/banana.png"
- save_output_path: "/app/static/results/banana_ad.png"
- text_content: "ë§›ìˆëŠ”ë°”ë‚˜ë‚˜ 2500ì›"
- wait_for_completion: false
- composition_mode: "overlay"

ìš”êµ¬ì‚¬í•­:
- ë°ê³  í™œê¸°ì°¬ ì‹œì¥ ë°°ê²½
- ë…¸ë€ìƒ‰/ì´ˆë¡ìƒ‰ ê³„ì—´
- êµµì€ í•œê¸€ í°íŠ¸

ëª¨ë“  í”„ë¡¬í”„íŠ¸ëŠ” ì˜ë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
```

### 7.3. ë‚˜ìœ ì˜ˆì‹œ

```python
# âŒ ë‚˜ìœ ì˜ˆì‹œ 1: ì •ë³´ ë¶€ì¡±
user_request = "ê´‘ê³  ë§Œë“¤ì–´ì¤˜"  # ì–´ë–¤ ê´‘ê³ ? ì–´ë–¤ ì´ë¯¸ì§€?

# âŒ ë‚˜ìœ ì˜ˆì‹œ 2: ê²½ë¡œ ëˆ„ë½
user_request = "ë°”ë‚˜ë‚˜ ê´‘ê³  ë§Œë“¤ì–´ì¤˜"  # ì´ë¯¸ì§€ ê²½ë¡œ ì—†ìŒ

# âŒ ë‚˜ìœ ì˜ˆì‹œ 3: ëª¨í˜¸í•œ ìš”êµ¬ì‚¬í•­
user_request = "ë©‹ì§„ ê´‘ê³  ë§Œë“¤ì–´ì¤˜"  # ì–´ë–¤ ìŠ¤íƒ€ì¼?
```

---

## 8. ê³ ê¸‰ ì‚¬ìš©ë²•

### 8.1. í°íŠ¸ ìë™ ì¶”ì²œ

```python
async def auto_font_selection():
    """ê´‘ê³  ìœ í˜•ë³„ í°íŠ¸ ìë™ ì„ íƒ"""
    async with LLMAdapter(
        openai_api_key=openai_api_key,
        mcp_server_url=mcp_server_url,
        model="gpt-5-mini"
    ) as adapter:
        
        # í°íŠ¸ ì¶”ì²œ ìš”ì²­
        response = await adapter.chat(
            "50% í• ì¸ ì„¸ì¼ ê´‘ê³ ì— ì–´ìš¸ë¦¬ëŠ” êµµì€ í°íŠ¸ ì¶”ì²œí•´ì¤˜"
        )
        print(response)
```

### 8.2. ë°°ê²½ ì´ë¯¸ì§€ ì—†ì´ í…ìŠ¤íŠ¸ë§Œ ìƒì„±

```python
user_request = f"""
ì‚¬ìš©ì: í…ìŠ¤íŠ¸ ì—ì…‹ë§Œ ìƒì„±í•´ì¤˜

- text_content: "SUMMER SALE"
- save_output_path: "{output_path}"
- font_name: "NanumGothicExtraBold.ttf"

ëª¨ë“  í”„ë¡¬í”„íŠ¸ëŠ” ì˜ë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
```

### 8.3. ë°°ê²½ ì´ë¯¸ì§€ë§Œ ìƒì„±

```python
user_request = f"""
ì‚¬ìš©ì: ì—¬ë¦„ í•´ë³€ ë°°ê²½ ì´ë¯¸ì§€ë§Œ ìƒì„±í•´ì¤˜

- save_output_path: "{output_path}"
- background_prompt: "Bright summer beach scene with blue sky"

ëª¨ë“  í”„ë¡¬í”„íŠ¸ëŠ” ì˜ë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
```

---

## 9. ê°œë°œìë¥¼ ìœ„í•œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì „ëµ

### 9.1. ë¬¸ì œ ìƒí™©

**ì¼ë°˜ ê´‘ê³  ìƒì„± ì‹œê°„: 15~20ë¶„**

ê°œë°œ ì¤‘ ê°€ì¥ í° ì¥ì• ë¬¼ì€ ê¸´ ì‘ì—… ì‹œê°„ì…ë‹ˆë‹¤:
- ë°°ê²½ ìƒì„±: 5~7ë¶„
- ì œí’ˆ í•©ì„±: 3~5ë¶„
- í…ìŠ¤íŠ¸ ìƒì„±: 4~6ë¶„
- ìµœì¢… í•©ì„±: 3~5ë¶„

ì´ë¡œ ì¸í•´:
- íŒŒë¼ë¯¸í„° ì¡°ì • ì‹œ ë§¤ë²ˆ 20ë¶„ ëŒ€ê¸°
- ë²„ê·¸ ìˆ˜ì • í›„ ê²€ì¦ì— 20ë¶„ ì†Œìš”
- í•˜ë£¨ì— í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•œ íšŸìˆ˜ ì œí•œ (3~4íšŒ)

### 9.2. ì „ëµ 1: í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‚¬ìš© (ê¶Œì¥)

**íŠ¹ì§•**
- ì‹¤ì œ ëª¨ë¸ ì¶”ë¡  ìƒëµ
- ë”ë¯¸ ì´ë¯¸ì§€ ì¦‰ì‹œ ë°˜í™˜
- ì‘ì—… ì‹œê°„: **1~2ì´ˆ**
- API/íŒŒë¼ë¯¸í„° ê²€ì¦ì— ìµœì 

**ì‚¬ìš©ë²•**

```python
user_request = f"""
ì‚¬ìš©ì: ë°”ë‚˜ë‚˜ íŠ¹ê°€ ê´‘ê³  ë§Œë“¤ì–´ì¤˜

- product_image_path: "{product_image_path}"
- save_output_path: "{output_image_path}"
- text_content: "ë§›ìˆëŠ”ë°”ë‚˜ë‚˜ 2500ì›"
- test_mode: true  # â­ í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™œì„±í™”
- wait_for_completion: false

ëª¨ë“  í”„ë¡¬í”„íŠ¸ëŠ” ì˜ë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""

async with LLMAdapter(...) as adapter:
    response = await adapter.chat(user_request, max_tool_calls=1)
```

**ì–¸ì œ ì‚¬ìš©í•˜ë‚˜ìš”?**
- API ì—°ë™ ê°œë°œ ì´ˆê¸° ë‹¨ê³„
- ìš”ì²­/ì‘ë‹µ êµ¬ì¡° ê²€ì¦
- íŒŒë¼ë¯¸í„° ì „ë‹¬ í…ŒìŠ¤íŠ¸
- CI/CD íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸

**ì£¼ì˜ì‚¬í•­**
- ë”ë¯¸ ì´ë¯¸ì§€ëŠ” í’ˆì§ˆ ê²€ì¦ ë¶ˆê°€
- í”„ë¡¬í”„íŠ¸ íš¨ê³¼ í™•ì¸ ë¶ˆê°€
- ì‹¤ì œ ë°°í¬ ì „ `test_mode: false`ë¡œ ì „í™˜ í•„ìˆ˜

### 9.3. ì „ëµ 2: ì‘ì—… ê°•ì œ ì¤‘ë‹¨ í›„ í…ŒìŠ¤íŠ¸

**íŠ¹ì§•**
- ì´ì „ ì‘ì—… ê°•ì œ ì¤‘ë‹¨
- ìƒˆ ì‘ì—… ì¦‰ì‹œ ì‹œì‘
- ì„œë²„ ë¦¬ì†ŒìŠ¤ ì¦‰ì‹œ í™•ë³´

**ê¸°ë³¸ êµ¬í˜„**

```python
async def force_stop_all_and_start():
    """ëª¨ë“  ì‘ì—… ì¤‘ë‹¨ í›„ ìƒˆ ê´‘ê³  ìƒì„±"""
    
    async with MCPClient(
        base_url=mcp_server_url,
        timeout=30
    ) as client:
        
        # Step 1: ëª¨ë“  ì‘ì—… ëª©ë¡ ì¡°íšŒ
        all_jobs = await client.call_tool("get_all_jobs", {})
        jobs_data = json.loads(all_jobs)
        
        # Step 2: ì‹¤í–‰ ì¤‘/ëŒ€ê¸° ì¤‘ ì‘ì—… ê°•ì œ ì¤‘ë‹¨
        for job in jobs_data.get("jobs", []):
            status = job.get("status")
            job_id = job.get("job_id")
            
            if status in ["pending", "running"]:
                print(f"ê°•ì œ ì¤‘ë‹¨ ì‹œë„: {job_id} (status={status})")
                await client.call_tool("stop_generation", {"job_id": job_id})
        
        # Step 3: ìƒˆ ê´‘ê³  ìƒì„± ì‹œì‘
        # ... (ì´ì „ ì˜ˆì œ ì½”ë“œ)
```

**ë¬¸ì œì : ì¤‘ë‹¨ì´ ì¦‰ì‹œ ë˜ì§€ ì•ŠìŒ**

ëª¨ë¸ ë¡œë”© ì¤‘ì—ëŠ” ì¤‘ë‹¨ ë¶ˆê°€:
- Stable Diffusion ë¡œë”©: 30~60ì´ˆ
- ControlNet ë¡œë”©: 20~40ì´ˆ
- Shap-E ë¡œë”©: 15~30ì´ˆ

### 9.4. ì „ëµ 3: ì¬ì‹œë„ ê¸°ë°˜ ê°•ì œ ì¤‘ë‹¨ (ê¶Œì¥)

**íŠ¹ì§•**
- ì‘ì—… ìƒíƒœë¥¼ ì§€ì†ì ìœ¼ë¡œ í™•ì¸
- ì¤‘ë‹¨ë  ë•Œê¹Œì§€ ë°˜ë³µ ìš”ì²­
- íƒ€ì„ì•„ì›ƒ ì„¤ì •ìœ¼ë¡œ ì•ˆì „ì¥ì¹˜ ì¶”ê°€

**ê°œì„ ëœ êµ¬í˜„**

```python
import asyncio
import json
from mcpadapter import MCPClient

async def force_stop_with_retry(
    max_attempts: int = 30,
    interval: int = 2,
    timeout: int = 60
):
    """
    ì¬ì‹œë„ ê¸°ë°˜ ì‘ì—… ê°•ì œ ì¤‘ë‹¨
    
    Args:
        max_attempts: ìµœëŒ€ ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 30)
        interval: ì¬ì‹œë„ ê°„ê²©(ì´ˆ, ê¸°ë³¸ê°’: 2)
        timeout: ì „ì²´ íƒ€ì„ì•„ì›ƒ(ì´ˆ, ê¸°ë³¸ê°’: 60)
    
    Returns:
        ëª¨ë“  ì‘ì—…ì´ ì¤‘ë‹¨ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€
    """
    async with MCPClient(
        base_url=mcp_server_url,
        timeout=30
    ) as client:
        
        start_time = asyncio.get_event_loop().time()
        attempt = 0
        
        while attempt < max_attempts:
            elapsed = asyncio.get_event_loop().time() - start_time
            
            # íƒ€ì„ì•„ì›ƒ ì²´í¬
            if elapsed > timeout:
                print(f"â° íƒ€ì„ì•„ì›ƒ: {timeout}ì´ˆ ì´ˆê³¼")
                return False
            
            # Step 1: í˜„ì¬ ì‘ì—… ëª©ë¡ ì¡°íšŒ
            all_jobs = await client.call_tool("get_all_jobs", {})
            jobs_data = json.loads(all_jobs)
            
            # Step 2: ì‹¤í–‰/ëŒ€ê¸° ì¤‘ì¸ ì‘ì—… í•„í„°ë§
            active_jobs = [
                job for job in jobs_data.get("jobs", [])
                if job.get("status") in ["pending", "running"]
            ]
            
            if not active_jobs:
                print("âœ… ëª¨ë“  ì‘ì—…ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return True
            
            # Step 3: ê° ì‘ì—…ì— ì¤‘ë‹¨ ìš”ì²­
            for job in active_jobs:
                job_id = job.get("job_id")
                status = job.get("status")
                
                print(f"[{attempt+1}/{max_attempts}] ì¤‘ë‹¨ ìš”ì²­: {job_id} (status={status}, ê²½ê³¼: {elapsed:.1f}ì´ˆ)")
                
                try:
                    result = await client.call_tool(
                        "stop_generation",
                        {"job_id": job_id}
                    )
                    print(f"   ì¤‘ë‹¨ ì‘ë‹µ: {result}")
                except Exception as e:
                    print(f"   ì¤‘ë‹¨ ì‹¤íŒ¨: {e}")
            
            # Step 4: ì¬ì‹œë„ ëŒ€ê¸°
            await asyncio.sleep(interval)
            attempt += 1
        
        print(f"âš ï¸ ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ë„ë‹¬: {max_attempts}íšŒ")
        return False

# ì‚¬ìš© ì˜ˆ
success = await force_stop_with_retry(
    max_attempts=30,  # 30íšŒ ì‹œë„
    interval=2,       # 2ì´ˆë§ˆë‹¤
    timeout=60        # ì´ 60ì´ˆ ì œí•œ
)

if success:
    print("ìƒˆ ê´‘ê³  ìƒì„± ì‹œì‘ ê°€ëŠ¥")
else:
    print("ê°•ì œ ì¤‘ë‹¨ ì‹¤íŒ¨ - ì„œë²„ ì¬ì‹œì‘ ê³ ë ¤")
```

### 9.5. ì „ëµ 4: ì „ì²´ ì‘ì—… ì‚­ì œ (ì™„ë£Œ/ì‹¤íŒ¨ ì‘ì—… ì •ë¦¬)

**íŠ¹ì§•**
- ì™„ë£Œ/ì‹¤íŒ¨í•œ ì‘ì—… ì´ë ¥ ì‚­ì œ
- ì‹¤í–‰/ëŒ€ê¸° ì¤‘ì¸ ì‘ì—…ì€ ìë™ ê±´ë„ˆëœ€
- ì„œë²„ ë©”ëª¨ë¦¬ ì •ë¦¬

**ì‚¬ìš©ë²•**

```python
async def cleanup_completed_jobs():
    """ì™„ë£Œëœ ì‘ì—… ì •ë¦¬"""
    async with MCPClient(
        base_url=mcp_server_url,
        timeout=30
    ) as client:
        
        result = await client.call_tool("delete_all_jobs", {})
        print(result)
```

### 9.6. ì¶”ì²œ ê°œë°œ ì›Œí¬í”Œë¡œìš°

#### Phase 1: ì´ˆê¸° ê°œë°œ (test_mode)

```python
# âœ… ë¹ ë¥¸ ë°˜ë³µ (1~2ì´ˆ/íšŒ)
user_request = f"""
- test_mode: true
- wait_for_completion: true  # ì¦‰ì‹œ ì™„ë£Œ
"""
```

**ì¥ì **: API êµ¬ì¡° ê²€ì¦, íŒŒë¼ë¯¸í„° ì „ë‹¬ í…ŒìŠ¤íŠ¸

#### Phase 2: í”„ë¡¬í”„íŠ¸ íŠœë‹ (ê°•ì œ ì¤‘ë‹¨)

```python
# 1. ì´ì „ ì‘ì—… ê°•ì œ ì¤‘ë‹¨
await force_stop_with_retry()

# 2. ìƒˆ ê´‘ê³  ìƒì„± (ì‹¤ì œ ëª¨ë¸)
user_request = f"""
- test_mode: false
- wait_for_completion: false
"""
```

**ì¥ì **: ì‹¤ì œ ì´ë¯¸ì§€ í’ˆì§ˆ í™•ì¸, í”„ë¡¬í”„íŠ¸ íš¨ê³¼ ê²€ì¦

#### Phase 3: ìµœì¢… ê²€ì¦ (ì „ì²´ í”„ë¡œì„¸ìŠ¤)

```python
# ì‘ì—… ì •ë¦¬ í›„ ì²˜ìŒë¶€í„° ëê¹Œì§€ ì‹¤í–‰
await cleanup_completed_jobs()

user_request = f"""
- test_mode: false
- wait_for_completion: true  # ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
"""
```

**ì¥ì **: ì‹¤ì œ ìš´ì˜ í™˜ê²½ê³¼ ë™ì¼í•œ ì¡°ê±´

### 9.7. ì‹œê°„ ë¹„êµí‘œ

| ì „ëµ | ì‘ì—… ì‹œê°„ | ì´ë¯¸ì§€ í’ˆì§ˆ | ìš©ë„ |
|------|-----------|-------------|------|
| ì¼ë°˜ ì‹¤í–‰ | **15~20ë¶„** | â­â­â­â­â­ | ìµœì¢… ê²€ì¦ |
| í…ŒìŠ¤íŠ¸ ëª¨ë“œ | **1~2ì´ˆ** | âŒ (ë”ë¯¸) | ì´ˆê¸° ê°œë°œ |
| ê°•ì œ ì¤‘ë‹¨ + ì¬ì‹œì‘ | **1~2ë¶„** | â­â­â­â­â­ | í”„ë¡¬í”„íŠ¸ íŠœë‹ |
| ì‘ì—… ì •ë¦¬ | **1ì´ˆ** | N/A | í™˜ê²½ ì´ˆê¸°í™” |

### 9.8. ì‹¤ì „ íŒ

1. **ê°œë°œ ì´ˆê¸°**: `test_mode=true`ë¡œ ì‹œì‘
2. **í”„ë¡¬í”„íŠ¸ ì¡°ì •**: ê°•ì œ ì¤‘ë‹¨ í›„ ì¦‰ì‹œ ì¬ì‹œì‘
3. **ì¤‘ë‹¨ ì•ˆ ë  ë•Œ**: 2ì´ˆë§ˆë‹¤ ì¬ì‹œë„ (ìµœëŒ€ 60ì´ˆ)
4. **í•˜ë£¨ ë§ˆë¬´ë¦¬**: `delete_all_jobs()`ë¡œ ì •ë¦¬
5. **ìµœì¢… ë°°í¬ ì „**: `test_mode=false` + ì „ì²´ í”„ë¡œì„¸ìŠ¤ ê²€ì¦

### 9.9. ì„œë²„ ì¦‰ì‹œ ì´ˆê¸°í™” (Server Reset API) â­â­â­

**ê°€ì¥ ë¹ ë¥´ê³  í™•ì‹¤í•œ ì´ˆê¸°í™” ë°©ë²•**

ëª¨ë“  ì‘ì—… ì¤‘ë‹¨ + ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ í•œ ë²ˆì˜ API í˜¸ì¶œë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.

**íŠ¹ì§•**
- ì†Œìš” ì‹œê°„: **1~3ì´ˆ** (ëª¨ë¸ ë¡œë”© ì¤‘ì¼ ê²½ìš° ìµœëŒ€ 10ì´ˆ)
- ëª¨ë“  ì‘ì—… ê°•ì œ ì¤‘ë‹¨ + ì‚­ì œ + GPU ë©”ëª¨ë¦¬ ì •ë¦¬
- ì¬ì‹œë„ ë¡œì§ ë‚´ì¥ (í”„ë¡œì„¸ìŠ¤ killê¹Œì§€ ìˆ˜í–‰)
- 100% í™•ì‹¤í•œ ì´ˆê¸°í™” ë³´ì¥

**REST API ì§ì ‘ í˜¸ì¶œ**

```bash
# cURL
curl -X POST http://localhost:8000/server-reset

# HTTPie
http POST http://localhost:8000/server-reset
```

**Python ì½”ë“œ**

```python
import httpx
import json

async def reset_server_and_start_new():
    """ì„œë²„ ì´ˆê¸°í™” í›„ ìƒˆ ê´‘ê³  ìƒì„±"""
    
    async with httpx.AsyncClient() as client:
        # Step 1: ì„œë²„ ì´ˆê¸°í™”
        reset_resp = await client.post("http://localhost:8000/server-reset")
        result = reset_resp.json()
        
        print("=" * 60)
        print("ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ")
        print("=" * 60)
        print(f"ìƒíƒœ: {result['status']}")
        print(f"ì¤‘ë‹¨ëœ ì‘ì—…: {result['statistics']['stopped_jobs']}ê°œ")
        print(f"ì‚­ì œëœ ì‘ì—…: {result['statistics']['deleted_jobs']}ê°œ")
        print(f"ì¢…ë£Œëœ í”„ë¡œì„¸ìŠ¤: {result['statistics']['terminated_processes']}ê°œ")
        print(f"GPU ë©”ëª¨ë¦¬: {result['statistics']['gpu_memory_mb']} MB")
        print(f"ì†Œìš” ì‹œê°„: {result['statistics']['elapsed_sec']}ì´ˆ")
        print("=" * 60)
        
        # Step 2: ì¦‰ì‹œ ìƒˆ ê´‘ê³  ìƒì„± ì‹œì‘
        # ... (ì´ì „ ì˜ˆì œ ì½”ë“œ)

# ì‹¤í–‰
await reset_server_and_start_new()
```

**LLMAdapterì™€ í•¨ê»˜ ì‚¬ìš©**

```python
import httpx
from mcpadapter import LLMAdapter

async def quick_reset_and_generate():
    """ì´ˆê¸°í™” í›„ ì¦‰ì‹œ ê´‘ê³  ìƒì„± (ì „ì²´ ì›Œí¬í”Œë¡œìš°)"""
    
    # Step 1: ì„œë²„ ì´ˆê¸°í™”
    async with httpx.AsyncClient() as client:
        await client.post("http://localhost:8000/server-reset")
        print("âœ… ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # Step 2: ì¦‰ì‹œ ìƒˆ ê´‘ê³  ìƒì„±
    async with LLMAdapter(
        openai_api_key=openai_api_key,
        mcp_server_url=mcp_server_url,
        model="gpt-4o"
    ) as adapter:
        
        user_request = f"""
ì‚¬ìš©ì: ë°”ë‚˜ë‚˜ íŠ¹ê°€ ê´‘ê³  ë§Œë“¤ì–´ì¤˜

- product_image_path: "{product_image_path}"
- save_output_path: "{output_image_path}"
- text_content: "ë§›ìˆëŠ”ë°”ë‚˜ë‚˜ 2500ì›"
- wait_for_completion: false

ëª¨ë“  í”„ë¡¬í”„íŠ¸ëŠ” ì˜ë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
        
        response = await adapter.chat(user_request, max_tool_calls=1)
        print("âœ… ê´‘ê³  ìƒì„± ì‹œì‘")

# ì‹¤í–‰
await quick_reset_and_generate()
```

**ì–¸ì œ ì‚¬ìš©í•˜ë‚˜ìš”?**
- â­ **í”„ë¡¬í”„íŠ¸ ë³€ê²½ í›„ ì¦‰ì‹œ ì¬í…ŒìŠ¤íŠ¸** (ê°€ì¥ ë§ì´ ì‚¬ìš©)
- ê°•ì œ ì¤‘ë‹¨(stop_job)ì´ ì‹¤íŒ¨í•  ë•Œ
- ì—¬ëŸ¬ ì‘ì—…ì´ ìŒ“ì˜€ì„ ë•Œ
- ê°œë°œ ì„¸ì…˜ ì‹œì‘/ì¢…ë£Œ ì‹œ
- ì„œë²„ ì¬ì‹œì‘ì´ ë¶€ë‹´ìŠ¤ëŸ¬ìš¸ ë•Œ

**ì‘ë‹µ ì˜ˆì‹œ**

```json
{
  "status": "success",
  "message": "Server reset completed successfully",
  "statistics": {
    "stopped_jobs": 2,
    "deleted_jobs": 5,
    "terminated_processes": 2,
    "gpu_memory_mb": 234.56,
    "elapsed_sec": 2.34
  }
}
```

**ì£¼ì˜ì‚¬í•­**
- **ê°œë°œ ì „ìš©**: ìš´ì˜ í™˜ê²½ ì‚¬ìš© ê¸ˆì§€
- ëª¨ë“  ì‘ì—… ê²°ê³¼ ì‚­ì œ (ë³µêµ¬ ë¶ˆê°€)
- ì‹¤í–‰ ì¤‘ì¸ ì‘ì—…ì´ ì¦‰ì‹œ ì¤‘ë‹¨ë¨
- GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œê°„ì´ ì¶”ê°€ë¡œ ì†Œìš”ë  ìˆ˜ ìˆìŒ

**ì „ëµ 4 (delete_all_jobs)ì™€ì˜ ì°¨ì´**

| ê¸°ëŠ¥ | delete_all_jobs | server-reset |
|------|-----------------|--------------|
| ì™„ë£Œëœ ì‘ì—… ì‚­ì œ | âœ… | âœ… |
| ì‹¤í–‰ ì¤‘ ì‘ì—… ì¤‘ë‹¨ | âŒ | âœ… |
| í”„ë¡œì„¸ìŠ¤ ê°•ì œ ì¢…ë£Œ | âŒ | âœ… |
| GPU ë©”ëª¨ë¦¬ ì •ë¦¬ | âŒ | âœ… |
| ì†Œìš” ì‹œê°„ | 1ì´ˆ | 1~3ì´ˆ |
| í™•ì‹¤ì„± | ì¤‘ê°„ | **100%** |

**ì‹¤ì „ ê°œë°œ ì›Œí¬í”Œë¡œìš° (ì—…ë°ì´íŠ¸)**

```python
# ğŸ”¥ ê¶Œì¥: ì„œë²„ ì´ˆê¸°í™” ì‚¬ìš©
await reset_server_and_start_new()

# ê¸°ì¡´ ë°©ì‹ (ë¹„êµ)
# await force_stop_with_retry()  # 30~60ì´ˆ ì†Œìš”
# await cleanup_completed_jobs()  # ì‹¤í–‰ ì¤‘ ì‘ì—…ì€ ë‚¨ìŒ
```

---

## 10. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 10.1. ê°•ì œ ì¤‘ë‹¨ì´ ì•ˆ ë˜ëŠ” ê²½ìš°

**ì¦ìƒ**: `stop_generation()` í˜¸ì¶œí–ˆìœ¼ë‚˜ ì‘ì—…ì´ ê³„ì† ì‹¤í–‰ë¨

**ì›ì¸**:
- ëª¨ë¸ ë¡œë”© ì¤‘ (Stable Diffusion, ControlNet, Shap-E)
- GPU ë©”ëª¨ë¦¬ í• ë‹¹ ì¤‘
- ì¶”ë¡  ë‹¨ê³„ ì „í™˜ ì¤‘

**í•´ê²°**:
```python
# ì¬ì‹œë„ ê¸°ë°˜ ê°•ì œ ì¤‘ë‹¨ ì‚¬ìš© (ì„¹ì…˜ 9.4 ì°¸ê³ )
await force_stop_with_retry(
    max_attempts=30,
    interval=2,
    timeout=60
)
```

### 10.2. job_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ

**ì¦ìƒ**: `job_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤` ê²½ê³ 

**ì›ì¸**:
- LLM ì‘ë‹µì— JSON í˜•ì‹ì´ ì—†ìŒ
- ë„êµ¬ í˜¸ì¶œ ì‹¤íŒ¨

**í•´ê²°**:
1. `adapter.conversation_history` í™•ì¸
2. `max_tool_calls=1` ì„¤ì • í™•ì¸
3. ë„êµ¬ ì‘ë‹µì—ì„œ JSON íŒŒì‹±

```python
# ë„êµ¬ ì‘ë‹µ ì¶”ì¶œ
tool_response = None
for msg in reversed(adapter.conversation_history):
    if msg.get("role") == "tool":
        tool_response = msg.get("content")
        break

# JSON íŒŒì‹±
import json
tool_data = json.loads(tool_response)
job_id = tool_data["job_id"]
```

### 9.2. íƒ€ì„ì•„ì›ƒ ë°œìƒ

**ì¦ìƒ**: `â° íƒ€ì„ì•„ì›ƒ: 3000ì´ˆ ë™ì•ˆ ì‘ì—…ì´ ì™„ë£Œë˜ì§€ ì•ŠìŒ`

**ì›ì¸**:
- ì„œë²„ ê³¼ë¶€í•˜
- ë³µì¡í•œ í”„ë¡¬í”„íŠ¸
- ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ

**í•´ê²°**:
1. `max_attempts` ì¦ê°€
2. ì„œë²„ ìƒíƒœ í™•ì¸: `check_server_health()`
3. ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ í…ŒìŠ¤íŠ¸

### 9.3. ì´ë¯¸ì§€ê°€ ì €ì¥ë˜ì§€ ì•ŠìŒ

**ì¦ìƒ**: ì‘ì—… ì™„ë£Œë˜ì—ˆìœ¼ë‚˜ ì´ë¯¸ì§€ ì—†ìŒ

**ì›ì¸**:
- `save_result_path` ëˆ„ë½
- ë””ë ‰í† ë¦¬ ê¶Œí•œ ë¬¸ì œ

**í•´ê²°**:
```python
# save_result_path í•„ìˆ˜ ì „ë‹¬
status_result = await client.call_tool(
    "check_generation_status",
    {
        "job_id": job_id,
        "save_result_path": save_result_path  # í•„ìˆ˜!
    }
)

# ë””ë ‰í† ë¦¬ ê¶Œí•œ í™•ì¸
import os
output_dir = Path(save_result_path).parent
os.chmod(output_dir, 0o777)
```

---

## 10. ì°¸ê³  ìë£Œ

### 10.1. ê´€ë ¨ íŒŒì¼

- `src/mcpadapter/llm_adapter.py`: LLMAdapter êµ¬í˜„
- `src/mcpadapter/mcp_client.py`: MCPClient êµ¬í˜„
- `src/nanoCocoa_mcpserver/server.py`: MCP ë„êµ¬ ì •ì˜
- `script/ê¹€ëª…í™˜/test_llm_mcp.ipynb`: ì „ì²´ ì˜ˆì œ ë…¸íŠ¸ë¶

### 10.2. MCP ë„êµ¬ ëª©ë¡

| ë„êµ¬ ì´ë¦„ | ì„¤ëª… | ì£¼ìš” íŒŒë¼ë¯¸í„° |
|-----------|------|---------------|
| `generate_ad_image` | ê´‘ê³  ì´ë¯¸ì§€ ìƒì„± | `product_image_path`, `text_content` |
| `check_generation_status` | ì‘ì—… ìƒíƒœ í™•ì¸ | `job_id`, `save_result_path` |
| `recommend_font_for_ad` | í°íŠ¸ ì¶”ì²œ | `text_content`, `ad_type` |
| `list_available_fonts` | í°íŠ¸ ëª©ë¡ ì¡°íšŒ | (ì—†ìŒ) |
| `check_server_health` | ì„œë²„ ìƒíƒœ í™•ì¸ | (ì—†ìŒ) |
| `stop_generation` | ì‘ì—… ì¤‘ë‹¨ | `job_id` |
| `delete_job` | ì‘ì—… ì‚­ì œ | `job_id` |

### 10.3. ê´‘ê³  ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸ ê°€ì´ë“œ

**ì„¸ì¼ ê´‘ê³  (sale)**
- background: vibrant, dynamic, energetic
- text: bold, large, eye-catching
- color: red, yellow, orange
- font: bold sans-serif

**í”„ë¦¬ë¯¸ì—„ ê´‘ê³  (premium)**
- background: elegant, minimalist, dark
- text: sophisticated, refined
- color: gold, silver, black
- font: serif, thin, elegant

**ìºì£¼ì–¼ ê´‘ê³  (casual)**
- background: warm, friendly, cozy
- text: handwritten, playful
- color: pastel, soft tones
- font: script, handwriting

---

## 11. ì „ì²´ ì˜ˆì œ ì½”ë“œ

```python
import os
import asyncio
import json
from pathlib import Path
from mcpadapter import LLMAdapter, MCPClient

# í™˜ê²½ ì„¤ì •
openai_api_key = os.getenv("OPENAI_API_KEY")
mcp_server_url = "http://localhost:3000"
product_image_path = "/app/static/uploads/banana.png"
output_image_path = "/app/static/results/banana_ad.png"

async def main():
    """ì „ì²´ ê´‘ê³  ìƒì„± ì›Œí¬í”Œë¡œìš°"""
    
    # Step 1: ê´‘ê³  ìƒì„± ìš”ì²­
    async with LLMAdapter(
        openai_api_key=openai_api_key,
        mcp_server_url=mcp_server_url,
        model="gpt-5-mini"
    ) as adapter:
        
        user_request = f"""
ì‚¬ìš©ì: ë°”ë‚˜ë‚˜ íŠ¹ê°€ ê´‘ê³  ë§Œë“¤ì–´ì¤˜

- product_image_path: "{product_image_path}"
- save_output_path: "{output_image_path}"
- text_content: "ë§›ìˆëŠ”ë°”ë‚˜ë‚˜ 2500ì›"
- wait_for_completion: false
- composition_mode: "overlay"

ëª¨ë“  í”„ë¡¬í”„íŠ¸ëŠ” ì˜ë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
        
        # ì¦‰ì‹œ ì‹¤í–‰
        response = await adapter.chat(user_request, max_tool_calls=1)
        
        # job_id ì¶”ì¶œ
        tool_response = None
        for msg in reversed(adapter.conversation_history):
            if msg.get("role") == "tool":
                tool_response = msg.get("content")
                break
        
        tool_data = json.loads(tool_response)
        job_id = tool_data["job_id"]
        print(f"Job ID: {job_id}")
    
    # Step 2: ìƒíƒœ í™•ì¸ (í´ë§)
    async with MCPClient(
        base_url=mcp_server_url,
        timeout=30
    ) as client:
        
        max_attempts = 300
        interval = 10
        attempt = 0
        
        while attempt < max_attempts:
            await asyncio.sleep(interval)
            attempt += 1
            
            status_result = await client.call_tool(
                "check_generation_status",
                {
                    "job_id": job_id,
                    "save_result_path": output_image_path
                }
            )
            
            status_data = json.loads(status_result)
            status = status_data.get("status")
            progress = status_data.get("progress_percent", 0)
            
            print(f"[{attempt}/{max_attempts}] status={status}, progress={progress}%")
            
            if status == "completed":
                print(f"âœ… ê´‘ê³  ìƒì„± ì™„ë£Œ: {output_image_path}")
                break
            elif status == "failed":
                print(f"âŒ ì‘ì—… ì‹¤íŒ¨: {status_data.get('message')}")
                break

# ì‹¤í–‰
await main()
```

---

## 12. ìš”ì•½

### 12.1. í•µì‹¬ í¬ì¸íŠ¸

1. **ìì—°ì–´ ìš”ì²­**: ë³µì¡í•œ API íŒŒë¼ë¯¸í„° ë¶ˆí•„ìš”
2. **ì¦‰ì‹œ ì‹¤í–‰**: `max_tool_calls=1`ë¡œ ì¦‰ì‹œ ë„êµ¬ í˜¸ì¶œ
3. **ë¹„ë™ê¸° í´ë§**: `wait_for_completion=false` + `check_generation_status`
4. **ê²½ë¡œ í•„ìˆ˜**: ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©
5. **ì˜ë¬¸ í”„ë¡¬í”„íŠ¸**: "ëª¨ë“  í”„ë¡¬í”„íŠ¸ëŠ” ì˜ë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”" í•„ìˆ˜

### 12.2. ê¸°ë³¸ ì›Œí¬í”Œë¡œìš°

```
1. LLMAdapter ì´ˆê¸°í™”
2. ìì—°ì–´ ìš”ì²­ ì‘ì„± (í•„ìˆ˜ ì •ë³´ í¬í•¨)
3. adapter.chat() í˜¸ì¶œ (max_tool_calls=1)
4. job_id ì¶”ì¶œ
5. check_generation_status()ë¡œ í´ë§
6. ì™„ë£Œ ì‹œ ì´ë¯¸ì§€ ì €ì¥ í™•ì¸
```
