# nanoCocoa AI Server - Docker ë°°í¬ ê°€ì´ë“œ

## ğŸ“¦ ì‚¬ì „ ì¤€ë¹„

### 1. Docker ë° NVIDIA Container Toolkit ì„¤ì¹˜

```bash
# Docker ì„¤ì¹˜ (Ubuntu)
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# NVIDIA Container Toolkit ì„¤ì¹˜
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker

# ì„¤ì¹˜ í™•ì¸
sudo docker run --rm --gpus all nvidia/cuda:12.9.1-base-ubuntu22.04 nvidia-smi
```

### 2. HuggingFace ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±

```bash
# ì™¸ë¶€ ë³¼ë¥¨ ë””ë ‰í† ë¦¬ ìƒì„± (200GB ë””ìŠ¤í¬)
sudo mkdir -p /opt/huggingface
sudo chown -R $USER:$USER /opt/huggingface
```

---

## ğŸš€ ë¹Œë“œ ë° ì‹¤í–‰

### ë°©ë²• 1: docker-compose ì‚¬ìš© (ê¶Œì¥)

Docker ComposeëŠ” AI ì„œë²„ì™€ MCP ì„œë²„ë¥¼ í•¨ê»˜ ë°°í¬í•©ë‹ˆë‹¤.

```bash
# src ë””ë ‰í† ë¦¬ë¡œ ì´ë™ (ì¤‘ìš”!)
cd /home/spai0433/codeit-ai-3team-ad-content/src

# ë˜ëŠ” ìƒëŒ€ ê²½ë¡œë¡œ
cd codeit-ai-3team-ad-content/src

# ëª¨ë“  ì„œë¹„ìŠ¤ ë¹Œë“œ ë° ì‹¤í–‰ (aiserver + mcpserver)
sudo docker-compose up -d --build

# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
docker-compose ps

# ë¡œê·¸ í™•ì¸ (ëª¨ë“  ì„œë¹„ìŠ¤)
sudo docker-compose logs -f

# íŠ¹ì • ì„œë¹„ìŠ¤ ë¡œê·¸ë§Œ í™•ì¸
sudo docker-compose logs -f nanococoa-aiserver
sudo docker-compose logs -f nanococoa-mcpserver

# ì¤‘ì§€
sudo docker-compose down

# ì¬ì‹œì‘
sudo docker-compose restart

# íŠ¹ì • ì„œë¹„ìŠ¤ë§Œ ì¬ì‹œì‘
sudo docker-compose restart nanococoa-aiserver
sudo docker-compose restart nanococoa-mcpserver
```

**ë°°í¬ë˜ëŠ” ì„œë¹„ìŠ¤**:
- `nanococoa-aiserver`: AI ëª¨ë¸ ì„œë¹™ ì„œë²„ (í¬íŠ¸ 8000)
- `nanococoa-mcpserver`: MCP í”„ë¡œí† ì½œ ë¸Œë¦¿ì§€ ì„œë²„ (í¬íŠ¸ 3000)

**Docker ë„¤íŠ¸ì›Œí¬**: `nanococoa-network` (ë‚´ë¶€ í†µì‹ ìš©)

### ë°©ë²• 2: Docker ëª…ë ¹ì–´ ì§ì ‘ ì‚¬ìš© (AI ì„œë²„ë§Œ)

ê°œë³„ ì„œë¹„ìŠ¤ë¥¼ Docker ëª…ë ¹ì–´ë¡œ ì§ì ‘ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
# nanoCocoa_aiserver ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd /home/spai0433/codeit-ai-3team-ad-content/src/nanoCocoa_aiserver

# ì´ë¯¸ì§€ ë¹Œë“œ
sudo docker build -t nanococoa-aiserver:latest .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
sudo docker run -d \
  --name nanococoa-aiserver \
  --gpus all \
  -p 8000:8000 \
  -v /opt/huggingface:/root/.cache/huggingface \
  -v $(pwd)/static/uploads:/app/static/uploads \
  -v $(pwd)/static/results:/app/static/results \
  -v $(pwd)/logs:/app/logs \
  -e PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
  -e HF_HOME=/root/.cache/huggingface \
  -e DEVICE=cuda \
  -e AUTO_UNLOAD_DEFAULT=true \
  --restart unless-stopped \
  nanococoa-aiserver:latest

# ë¡œê·¸ í™•ì¸
sudo docker logs -f nanococoa-aiserver

# ì¤‘ì§€ ë° ì‚­ì œ
sudo docker stop nanococoa-aiserver
sudo docker rm nanococoa-aiserver
```

**ì°¸ê³ **: MCP ì„œë²„ë„ í•¨ê»˜ ì‚¬ìš©í•˜ë ¤ë©´ ë°©ë²• 1 (docker-compose)ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.

---

## ğŸ“ ë³¼ë¥¨ ë§¤í•‘

| í˜¸ìŠ¤íŠ¸ ê²½ë¡œ | ì»¨í…Œì´ë„ˆ ê²½ë¡œ | ìš©ë„ |
|------------|-------------|------|
| `/opt/huggingface` | `/root/.cache/huggingface` | HuggingFace ëª¨ë¸ ìºì‹œ (ì˜êµ¬) |
| `./static/uploads` | `/app/static/uploads` | ì—…ë¡œë“œëœ ì´ë¯¸ì§€ |
| `./static/results` | `/app/static/results` | ìƒì„±ëœ ê²°ê³¼ ì´ë¯¸ì§€ |
| `./logs` | `/app/logs` | ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê·¸ |

**ì¤‘ìš”**: `/opt/huggingface`ëŠ” 200GB ë””ìŠ¤í¬ì— ìƒì„±í•˜ì—¬ ëª¨ë¸ì„ ì˜êµ¬ ì €ì¥í•©ë‹ˆë‹¤.

---

## ğŸ” í—¬ìŠ¤ì²´í¬ ë° ëª¨ë‹ˆí„°ë§

### í—¬ìŠ¤ì²´í¬ API

```bash
# AI ì„œë²„ ìƒíƒœ í™•ì¸
curl http://localhost:8000/health

# ì‘ë‹µ ì˜ˆì‹œ
{
  "status": "healthy",
  "server_time": 1234567890.123,
  "total_jobs": 0,
  "active_jobs": 0,
  "system_metrics": {
    "cpu_percent": 12.5,
    "ram_used_gb": 8.2,
    "gpu_info": [...]
  }
}

# MCP ì„œë²„ ìƒíƒœ í™•ì¸
curl http://localhost:3000/health

# ì‘ë‹µ ì˜ˆì‹œ
{
  "status": "healthy",
  "aiserver_status": "connected",
  "aiserver_url": "http://nanococoa-aiserver:8000"
}
```

### ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸

```bash
# ëª¨ë“  ì»¨í…Œì´ë„ˆ ìƒíƒœ
sudo docker ps
# ë˜ëŠ” docker-compose ì‚¬ìš©
docker-compose ps

# ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ (ì‹¤ì‹œê°„, ëª¨ë“  ì»¨í…Œì´ë„ˆ)
sudo docker stats

# íŠ¹ì • ì»¨í…Œì´ë„ˆë§Œ
sudo docker stats nanococoa-aiserver nanococoa-mcpserver

# GPU ì‚¬ìš©ëŸ‰
nvidia-smi

# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ì ‘ì†
sudo docker exec -it nanococoa-aiserver bash
sudo docker exec -it nanococoa-mcpserver bash
```

### ë¡œê·¸ í™•ì¸

**docker-compose ì‚¬ìš©**:
```bash
# ëª¨ë“  ì„œë¹„ìŠ¤ ë¡œê·¸
sudo docker-compose logs -f

# íŠ¹ì • ì„œë¹„ìŠ¤ ë¡œê·¸
sudo docker-compose logs -f nanococoa-aiserver
sudo docker-compose logs -f nanococoa-mcpserver

# ìµœê·¼ 100ì¤„
sudo docker-compose logs --tail=100

# íŠ¹ì • ì‹œê°„ ì´í›„
sudo docker-compose logs --since 10m
```

**Docker ëª…ë ¹ì–´ ì‚¬ìš©**:
```bash
# ì „ì²´ ë¡œê·¸
sudo docker logs nanococoa-aiserver

# ì‹¤ì‹œê°„ ë¡œê·¸ (tail -f)
sudo docker logs -f nanococoa-aiserver

# ìµœê·¼ 100ì¤„
sudo docker logs --tail 100 nanococoa-aiserver

# íŠ¹ì • ì‹œê°„ ì´í›„ ë¡œê·¸
sudo docker logs --since 10m nanococoa-aiserver
```

---

## ğŸ› ë¬¸ì œ í•´ê²°

### 1. GPUê°€ ì¸ì‹ë˜ì§€ ì•ŠìŒ

```bash
# NVIDIA Container Toolkit ì¬ì‹œì‘
sudo systemctl restart docker

# GPU í…ŒìŠ¤íŠ¸
sudo docker run --rm --gpus all nvidia/cuda:12.9.1-base-ubuntu22.04 nvidia-smi
```

### 2. í¬íŠ¸ê°€ ì´ë¯¸ ì‚¬ìš© ì¤‘

```bash
# 8000 í¬íŠ¸ ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸
sudo lsof -i :8000
sudo netstat -tulpn | grep :8000

# ë‹¤ë¥¸ í¬íŠ¸ë¡œ ë³€ê²½ (ì˜ˆ: 8001)
sudo docker run -p 8001:8000 ...
```

### 3. ë””ìŠ¤í¬ ìš©ëŸ‰ ë¶€ì¡±

```bash
# ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” Docker ì´ë¯¸ì§€/ì»¨í…Œì´ë„ˆ ì •ë¦¬
sudo docker system prune -a

# ë³¼ë¥¨ í™•ì¸
df -h /opt/huggingface

# ìºì‹œ ì •ë¦¬ (í•„ìš”ì‹œ)
rm -rf /opt/huggingface/hub/*
```

### 4. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ëŠë¦¼

```bash
# HuggingFace í† í° ì„¤ì • (ì„ íƒ)
sudo docker run -e HF_TOKEN=your_token_here ...

# ë˜ëŠ” .env íŒŒì¼ ìƒì„±
echo "HF_TOKEN=your_token_here" > .env
sudo docker-compose up -d
```

### 5. ì»¨í…Œì´ë„ˆê°€ ê³„ì† ì¬ì‹œì‘ë¨

```bash
# ì—ëŸ¬ ë¡œê·¸ í™•ì¸
sudo docker logs nanococoa-aiserver

# í—¬ìŠ¤ì²´í¬ ë¹„í™œì„±í™” í›„ ì¬ì‹œì‘
sudo docker run --no-healthcheck ...
```

---

## ğŸ”§ ì„±ëŠ¥ íŠœë‹

### 1. Worker ìˆ˜ ì¡°ì •

```yaml
# docker-compose.yml ìˆ˜ì •
command: ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "2"]
```

### 2. ë©”ëª¨ë¦¬ ì œí•œ

```yaml
# docker-compose.ymlì— ì¶”ê°€
deploy:
  resources:
    limits:
      memory: 32G
    reservations:
      memory: 16G
```

### 3. CUDA ë©”ëª¨ë¦¬ ìµœì í™”

```bash
# í™˜ê²½ ë³€ìˆ˜ ì¶”ê°€
-e PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,expandable_segments:True
```

---

## ğŸ“Š ìš´ì˜ ê°€ì´ë“œ

### 1. ë°±ì—…

```bash
# ëª¨ë¸ ìºì‹œ ë°±ì—…
tar -czf huggingface_cache_backup.tar.gz /opt/huggingface

# ê²°ê³¼ íŒŒì¼ ë°±ì—…
tar -czf results_backup.tar.gz ./static/results
```

### 2. ì—…ë°ì´íŠ¸

```bash
# ì½”ë“œ ì—…ë°ì´íŠ¸
git pull origin main

# ì´ë¯¸ì§€ ì¬ë¹Œë“œ
sudo docker-compose up -d --build

# ë˜ëŠ”
sudo docker-compose build --no-cache
sudo docker-compose up -d
```

### 3. ìŠ¤ì¼€ì¼ë§ (ë‹¤ì¤‘ ì¸ìŠ¤í„´ìŠ¤)

```yaml
# docker-compose.yml
services:
  nanococoa-aiserver:
    # ... (ê¸°ì¡´ ì„¤ì •)
    deploy:
      replicas: 2  # ì¸ìŠ¤í„´ìŠ¤ ìˆ˜
```

---

## ğŸ” ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

### 1. ë°©í™”ë²½ ì„¤ì •

```bash
# UFW ì‚¬ìš© ì‹œ
sudo ufw allow 8000/tcp
sudo ufw enable
```

### 2. HTTPS ì„¤ì • (Nginx ë¦¬ë²„ìŠ¤ í”„ë¡ì‹œ)

```nginx
server {
    listen 443 ssl;
    server_name your-domain.com;

    ssl_certificate /etc/ssl/certs/cert.pem;
    ssl_certificate_key /etc/ssl/private/key.pem;

    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

### 3. API í‚¤ ì¸ì¦ (ì„ íƒ)

```python
# main.pyì— ì¶”ê°€
from fastapi import Security, HTTPException
from fastapi.security.api_key import APIKeyHeader

API_KEY = os.getenv("API_KEY", "your-secret-key")
api_key_header = APIKeyHeader(name="X-API-Key")

async def verify_api_key(api_key: str = Security(api_key_header)):
    if api_key != API_KEY:
        raise HTTPException(status_code=403, detail="Invalid API Key")
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [Docker ê³µì‹ ë¬¸ì„œ](https://docs.docker.com/)
- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/)
- [Docker Compose](https://docs.docker.com/compose/)
- [FastAPI Deployment](https://fastapi.tiangolo.com/deployment/)

---

## ğŸ’¡ ë¹ ë¥¸ ëª…ë ¹ì–´ ìš”ì•½

```bash
# ë””ë ‰í† ë¦¬ ì´ë™ (ì¤‘ìš”!)
cd /home/spai0433/codeit-ai-3team-ad-content/src

# ë¹Œë“œ ë° ì‹¤í–‰ (ëª¨ë“  ì„œë¹„ìŠ¤)
sudo docker-compose up -d --build

# ì„œë¹„ìŠ¤ ìƒíƒœ
docker-compose ps

# ë¡œê·¸ í™•ì¸
sudo docker-compose logs -f

# íŠ¹ì • ì„œë¹„ìŠ¤ ë¡œê·¸
sudo docker-compose logs -f nanococoa-aiserver
sudo docker-compose logs -f nanococoa-mcpserver

# ì¤‘ì§€
sudo docker-compose down

# ì¬ì‹œì‘
sudo docker-compose restart

# Health Check
curl http://localhost:8000/health  # AI ì„œë²„
curl http://localhost:3000/health  # MCP ì„œë²„

# GPU í™•ì¸
nvidia-smi

# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ì ‘ì†
sudo docker exec -it nanococoa-aiserver bash
sudo docker exec -it nanococoa-mcpserver bash
```
