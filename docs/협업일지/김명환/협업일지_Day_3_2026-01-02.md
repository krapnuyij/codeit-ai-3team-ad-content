---
layout: default
title: "협업일지 Day 3 (2026-01-02) - 코드잇 AI 4기 3팀 김명환"
description: "협업일지 Day 3 (2026-01-02) - 코드잇 AI 4기 3팀 김명환"
date: 2026-01-02
author: "김명환"
cache-control: no-cache
expires: 0
pragma: no-cache
---

# 일일 협업일지 - Day 3 (2026-01-02)

## [1] 기본 정보
**날짜**: 2026-01-02
**이름**: 김명환
**팀명**: 코드잇 AI 4기 3팀

---

## [2] 오늘 맡은 역할 및 구체적인 작업 내용
**답변**:
```
1. 전체 시스템 아키텍처 설계
   - 프론트엔드, 백엔드, 모델 서빙 3계층 아키텍처 설계
   - Docker 기반 마이크로서비스 구조 정의
   - 계층 간 통신 프로토콜 및 포트 매핑 설계

2. FastAPI 기반 모델 서빙 설계 완성
   - 3단계 파이프라인 구조 검토 및 문서화
   - REST API 엔드포인트 및 데이터 포맷 정의
   - 동적 메모리 관리 및 비동기 처리 구조 확인

3. 아키텍처 설계 문서 작성
   - docs/아키텍처설계.md 작성 완료
   - 전체 시스템 구조도 (Mermaid 다이어그램)
   - 계층별 역할 및 API 설계 명세
   - 데이터 플로우 및 시퀀스 다이어그램
   - Docker 배포 아키텍처 설계

4. 모델 서빙 아키텍처 문서 검토
   - nanoCocoa_AI_Server_아키텍처설계.md 검토
   - 현재 구현 코드와 문서 일치성 확인
   - 워커 프로세스 및 모델 엔진 구조 분석
```

---

## [3] 오늘 작업 완료도 체크 (하나만 체크)
- [ ] 0% (시작 못함)
- [ ] 25% (시작은 했지만 진척 없음)
- [ ] 50% (진행 중, 절반 이하)
- [x] 75% (거의 완료됨)
- [ ] 100% (완료 및 점검까지 완료)

**간단한 근거**:
```
전체 시스템 아키텍처 설계 및 문서화 완료.
모델 서빙 계층은 기존 구현 활용으로 완료.
백엔드 및 프론트엔드 API 설계 완료.
실제 백엔드/프론트엔드 구현은 담당자(이건희)와 협업 필요.
```

---

## [4] 오늘 협업 중 제안하거나 피드백한 내용이 있다면?
**답변**:
```
1. 마이크로서비스 아키텍처 제안
   - 프론트엔드, 백엔드, 모델 서빙을 독립적인 컨테이너로 분리
   - 각 계층의 독립적인 개발 및 배포 가능
   - 확장성 및 유지보수성 향상

2. REST API 기반 통신 구조 제안
   - Frontend ↔ Backend (Port 8080)
   - Backend ↔ Model Serving (Port 8000)
   - 명확한 인터페이스 정의로 협업 효율성 증대

3. 기존 nanoCocoa_aiserver 활용 제안
   - 이미 구현된 모델 서빙 시스템 활용
   - 파이프라인 중간 단계 검토 및 커스터마이징 가능
   - 개발 시간 단축 및 안정성 확보
```

---

## [5] 오늘 분석/실험 중 얻은 인사이트나 발견한 문제점은?
**답변**:
```
[인사이트]
1. 3단계 파이프라인 구조의 장점
   - Step 1: 배경 생성 및 합성
   - Step 2: 3D 텍스트 생성
   - Step 3: 최종 합성
   - 각 단계별 중간 결과 저장으로 재시도 및 수정 용이

2. 동적 메모리 관리의 효율성
   - JIT 로딩/언로딩으로 L4 GPU 24GB VRAM 효율적 활용
   - 모델별 순차 로딩으로 메모리 충돌 방지
   - flush_gpu() 함수로 완전한 메모리 정리

3. Multiprocessing 기반 비동기 처리
   - FastAPI 메인 스레드와 AI 추론 작업 완전 분리
   - 워커 프로세스의 독립적인 메모리 공간
   - 안전한 작업 중단 및 상태 관리

[발견한 문제점]
1. 모델 서빙 단일 작업 처리 제약
   - 현재 구조는 동시에 하나의 작업만 처리 가능
   - 향후 다중 사용자 환경 시 대기 시간 발생 가능
   - 해결 방안: 작업 큐 시스템 또는 GPU 증설 고려

2. 백엔드-모델 서빙 간 통신 최적화 필요
   - Base64 인코딩/디코딩으로 인한 오버헤드
   - 대용량 이미지 전송 시 네트워크 부하
   - 해결 방안: 이미지 압축 또는 스트리밍 고려
```

---

## [6] 일정 지연이나 협업 중 어려웠던 점이 있다면?
**답변**:
```
- 특별한 지연 사항 없음
- 아키텍처 설계 단계에서는 개별 작업 위주로 진행
- 향후 백엔드/프론트엔드 구현 시 담당자와의 긴밀한 협업 필요
```

---

## [7] 오늘 발표 준비나 커뮤니케이션에서 기여한 부분은?
**답변**:
```
1. 아키텍처 설계 문서 작성
   - 전체 시스템 구조 시각화 (Mermaid 다이어그램)
   - 계층별 역할 및 API 명세 문서화
   - 향후 발표 자료 및 보고서 작성에 활용 가능

2. 기술 문서 정리
   - 모델 서빙 API 엔드포인트 정리
   - 요청/응답 데이터 포맷 예시 작성
   - Docker 배포 구성 예시 작성

3. 협업 기반 마련
   - 백엔드 개발자를 위한 API 명세 제공
   - 프론트엔드 개발자를 위한 통신 프로토콜 정의
   - 명확한 인터페이스로 병렬 개발 가능
```

---

## [8] 내일 목표 / 할 일
**답변**:
```
1. 모델 서빙을 위한 통합 파이프라인 개발 및 검토
   - 3단계 파이프라인 동작 테스트
   - 중간 단계 결과물 확인 및 검증
   - 파라미터 튜닝 및 최적화

2. 백엔드 API 연동 지원
   - 백엔드 개발자와 협업하여 모델 서빙 API 연동 지원
   - API 호출 예시 코드 작성
   - 에러 핸들링 및 예외 처리 가이드 제공

3. 프론트엔드-백엔드 인터페이스 검토
   - 데이터 포맷 및 프로토콜 검증
   - 실시간 상태 업데이트 방식 논의
   - 이미지 전송 최적화 방안 검토

4. Docker 컨테이너 구성 및 테스트
   - docker-compose.yml 작성
   - 각 계층별 Dockerfile 작성
   - 로컬 환경에서 통합 테스트
```

---

## [9] 추가 기록 (선택사항)

```
[아키텍처 설계 주요 결정사항]
1. 3계층 마이크로서비스 구조 채택
   - Frontend (FastAPI) - Port 8080
   - Backend (FastAPI) - Port 8080 (내부)
   - Model Serving (FastAPI) - Port 8000

2. REST API 기반 통신
   - HTTP/JSON 프로토콜
   - Base64 인코딩 이미지 전송
   - 폴링 방식 상태 확인

3. Docker 기반 배포
   - 각 계층별 독립 컨테이너
   - docker-compose를 통한 오케스트레이션
   - GPU 자원은 모델 서빙 컨테이너에만 할당

[참고 문서]
- docs/아키텍처설계.md (신규 작성)
- docs/doc/nanoCocoa_AI_Server_아키텍처설계.md (기존)
- src/nanoCocoa_aiserver/* (모델 서빙 구현 참고)
```

---

**작성 시간**: 15분