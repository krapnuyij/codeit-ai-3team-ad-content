---
layout: default
title: "발표 자료 - 이솔형 (AI Modeling) - 이미지 생성 모델 최적화"
description: "코드잇 AI 4기 3팀 최종 발표 - 이솔형 - 이미지 생성 및 특성 추출 모델 개발"
date: 2026-01-27
author: "이솔형"
cache-control: no-cache
expires: 0
pragma: no-cache
---

# 발표 자료 - 이솔형 (AI Modeling)
## 이미지 생성 모델 최적화 및 특성 추출

**발표자**: 이솔형 (AI Modeling - 이미지 생성 담당)

**발표 시간**: 8분

**발표 순서**: 4부 - AI 모델링 (이미지)

---

## 목차

1. [역할 소개](#1-역할-소개)
2. [이미지 생성 모델 선정](#2-이미지-생성-모델-선정)
3. [BiRefNet 누끼 제거](#3-birefnet-누끼-제거)
4. [FLUX 배경 생성](#4-flux-배경-생성)
5. [이미지 합성 및 후처리](#5-이미지-합성-및-후처리)
6. [Qwen2-VL 이미지 분석](#6-qwen2-vl-이미지-분석)
7. [모델 비교 및 최적화](#7-모델-비교-및-최적화)
8. [성과 및 향후 계획](#8-성과-및-향후-계획)

---

## 1. 역할 소개

### 1.1. 담당 역할

**AI Modeling - 이미지 생성 담당**
- 이미지 생성 모델 선정 및 최적화
- 이미지 특성 추출 모델 개발
- BiRefNet 누끼 제거 구현
- FLUX 배경 생성 파이프라인 구축
- 이미지 합성 및 후처리
- Qwen2-VL 이미지 분석 통합

### 1.2. 주요 책임

#### 모델 선정
- SDXL vs FLUX 비교 분석
- 품질, 속도, 안정성 평가
- 최적 모델 선택 및 파라미터 튜닝

#### 이미지 처리
- 배경 제거 (누끼 작업)
- 고품질 배경 이미지 생성
- 이미지 합성 및 리터칭
- 색보정 및 샤프닝

#### 품질 관리
- 이미지 품질 평가
- 파라미터 최적화
- A/B 테스트 수행

### 1.3. 개발 과정

| 기간 | 주요 활동 | 산출물 |
|------|-----------|--------|
| Week 1 (12/29~1/2) | 모델 조사, 초기 테스트 | 모델 후보 리스트 |
| Week 2 (1/3~1/9) | SDXL vs FLUX 비교 | 모델 비교 보고서 |
| Week 3 (1/10~1/16) | FLUX 파이프라인 구현 | FLUX 생성 파이프라인 |
| Week 4 (1/17~1/23) | 최적화, Qwen 통합 | 완성된 이미지 파이프라인 |
| Week 5 (1/24~1/27) | FLUX2 테스트, 양자화 | FLUX2 통합, 문서화 |

---

## 2. 이미지 생성 모델 선정

### 2.1. 후보 모델 비교

#### 주요 후보 모델

| 모델 | 파라미터 | VRAM | 속도 | 품질 | 비고 |
|------|----------|------|------|------|------|
| **SDXL** | 3.5B | 8GB | 빠름 (20초) | 중간 | Stable Diffusion 계열 |
| **FLUX.1-dev** | 12B | 16GB | 느림 (80초) | 높음 | Black Forest Labs |
| **FLUX.2** | 12B+ | 18GB | 중간 (60초) | 매우 높음 | 최신 모델 |
| **SDXL-Turbo** | 3.5B | 8GB | 매우 빠름 (5초) | 낮음 | 속도 최적화 |

### 2.2. SDXL vs FLUX 상세 비교

#### 테스트 조건
- 프롬프트: 동일한 영문 프롬프트 사용
- 제품: 건어물, 사과
- 생성 횟수: 각 10회

#### 정량적 비교

| 항목 | SDXL | FLUX.1-dev | 선정 |
|------|------|------------|------|
| **CLIP Score (평균)** | 0.52 | 0.78 | ✅ FLUX |
| **생성 시간** | 20초 | 80초 | ✅ SDXL |
| **VRAM 사용** | 8GB | 16GB | ✅ SDXL |
| **안정성** | 60% | 95% | ✅ FLUX |
| **프롬프트 이해도** | 중간 | 높음 | ✅ FLUX |

#### 정성적 비교

**SDXL의 문제점**
1. **프롬프트 이해도 낮음**
   - "사과" 입력 → 캔디 모양으로 생성됨
   - 위치 제어 어려움
   - 일관성 없는 결과

2. **이미지 왜곡**
   - 제품 비율 부자연스러움
   - 색감 과도하게 채도 높음
   - 배경과 조화 부족

3. **세밀한 제어 불가**
   - 프롬프트 상세도 낮아도 품질 개선 미미
   - 네거티브 프롬프트 효과 제한적

**FLUX.1-dev의 장점**
1. **뛰어난 프롬프트 이해**
   - 긴 프롬프트(100단어+)도 정확히 반영
   - 세밀한 디테일 묘사 가능
   - 스타일 지시 잘 따름

2. **고품질 생성**
   - 사실적인 질감 표현
   - 자연스러운 색감
   - 배경과 제품의 조화

3. **안정적인 결과**
   - 동일 프롬프트에 일관된 결과
   - 원하는 방향의 결과물 높은 확률

### 2.3. 최종 결정

**선택: FLUX.1-dev ✅**

**근거**
1. **품질 우선**: 소상공인 광고는 품질이 가장 중요
2. **안정성**: 일관된 결과로 사용자 만족도 향상
3. **프롬프트 이해도**: 다양한 요구사항 대응 가능
4. **속도 허용**: 2~3분 대기는 수용 가능한 범위

**트레이드오프**
- 속도: 80초 (SDXL 대비 4배 느림)
- VRAM: 16GB (SDXL 대비 2배 많음)
- 결론: 품질과 안정성이 더 중요 ✅

---

## 3. BiRefNet 누끼 제거

### 3.1. BiRefNet 개요

#### 모델 정보
- **이름**: BiRefNet (Bilateral Reference Network)
- **용도**: 배경 제거 (Segmentation)
- **파라미터**: ~500M
- **VRAM**: 약 7GB
- **추론 시간**: 3~5초
- **정확도**: 매우 높음 (Segment Anything 계열)

#### 특징
- 다양한 객체 타입 지원 (사람, 제품, 동물 등)
- 높은 정확도의 경계선 추출
- 후처리 없이도 깔끔한 결과
- Alpha 채널 자동 생성

### 3.2. 구현 상세

#### 기본 흐름

```python
import torch
from PIL import Image
import numpy as np

class BiRefNetSegmenter:
    def __init__(self, device="cuda"):
        self.device = device
        self.model = self.load_model()
    
    def load_model(self):
        """BiRefNet 모델 로드"""
        model = torch.hub.load(
            'ZhengPeng7/BiRefNet',
            'birefnet',
            trust_repo=True
        )
        model.to(self.device)
        model.eval()
        return model
    
    def preprocess(self, image: Image.Image) -> torch.Tensor:
        """이미지 전처리"""
        # 1024x1024로 리사이즈
        image = image.resize((1024, 1024))
        
        # NumPy 배열로 변환
        img_array = np.array(image)
        
        # 정규화 (0~1)
        img_array = img_array.astype(np.float32) / 255.0
        
        # Tensor로 변환 (B, C, H, W)
        img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)
        
        return img_tensor.to(self.device)
    
    def postprocess(self, mask: torch.Tensor) -> np.ndarray:
        """마스크 후처리"""
        # Tensor → NumPy
        mask = mask.squeeze().cpu().numpy()
        
        # 0~255 범위로 변환
        mask = (mask * 255).astype(np.uint8)
        
        return mask
    
    def remove_background(
        self,
        image: Image.Image
    ) -> tuple[Image.Image, np.ndarray]:
        """배경 제거"""
        # 전처리
        input_tensor = self.preprocess(image)
        
        # 추론
        with torch.no_grad():
            mask_tensor = self.model(input_tensor)
        
        # 후처리
        mask = self.postprocess(mask_tensor)
        
        # Alpha 채널 적용
        image_rgba = image.convert("RGBA")
        image_array = np.array(image_rgba)
        image_array[:, :, 3] = mask  # Alpha 채널에 마스크 적용
        
        nobg_image = Image.fromarray(image_array, mode="RGBA")
        
        return nobg_image, mask
```

#### 최적화 기법

**1. 배치 처리**
```python
def batch_remove_background(
    self,
    images: List[Image.Image]
) -> List[Image.Image]:
    """여러 이미지 동시 처리"""
    # 전처리 (배치)
    batch_tensor = torch.stack([
        self.preprocess(img) for img in images
    ])
    
    # 추론 (배치)
    with torch.no_grad():
        mask_batch = self.model(batch_tensor)
    
    # 후처리
    results = []
    for img, mask in zip(images, mask_batch):
        nobg_image, _ = self.apply_mask(img, mask)
        results.append(nobg_image)
    
    return results
```

**2. 엣지 스무딩**
```python
def smooth_edges(self, mask: np.ndarray, kernel_size: int = 5) -> np.ndarray:
    """마스크 경계선 부드럽게"""
    import cv2
    
    # Gaussian Blur
    mask_smooth = cv2.GaussianBlur(mask, (kernel_size, kernel_size), 0)
    
    return mask_smooth
```

### 3.3. 결과 예시

#### Before (원본 이미지)
- 제품 + 배경이 함께 있음
- 배경이 복잡하거나 노이즈 포함

#### After (누끼 이미지)
- 제품만 깔끔하게 분리
- Alpha 채널로 투명 배경
- 경계선이 자연스러움

#### 품질 지표
- **정확도**: 95% 이상
- **경계선 품질**: 매우 높음
- **처리 시간**: 3~5초
- **실패율**: 5% 미만

---

## 4. FLUX 배경 생성

### 4.1. FLUX.1-dev 개요

#### 모델 정보
- **개발사**: Black Forest Labs (Stable Diffusion 개발팀)
- **출시일**: 2024년 8월
- **파라미터**: 12B (120억 개)
- **VRAM**: 16GB
- **추론 시간**: 60~90초 (50 steps)
- **라이선스**: Apache 2.0 (상업적 사용 가능)

#### 기술적 특징
- **Transformer 기반**: Attention 메커니즘 개선
- **Flow Matching**: 새로운 생성 방식 (Diffusion 개선)
- **긴 프롬프트 지원**: 77토큰 제한 완화
- **고품질**: 8k 이미지 생성 가능

### 4.2. 구현 상세

#### 기본 파이프라인

```python
from diffusers import FluxPipeline
import torch

class FLUXGenerator:
    def __init__(self, device="cuda"):
        self.device = device
        self.pipeline = self.load_pipeline()
    
    def load_pipeline(self):
        """FLUX 파이프라인 로드"""
        pipeline = FluxPipeline.from_pretrained(
            "black-forest-labs/FLUX.1-dev",
            torch_dtype=torch.bfloat16
        )
        pipeline.to(self.device)
        pipeline.enable_model_cpu_offload()  # 메모리 최적화
        return pipeline
    
    def generate(
        self,
        prompt: str,
        height: int = 1024,
        width: int = 1024,
        num_inference_steps: int = 50,
        guidance_scale: float = 3.5,
        seed: int = None
    ) -> Image.Image:
        """이미지 생성"""
        
        # 재현성을 위한 시드 설정
        if seed is not None:
            generator = torch.Generator(device=self.device).manual_seed(seed)
        else:
            generator = None
        
        # 생성
        with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
            image = self.pipeline(
                prompt=prompt,
                height=height,
                width=width,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                generator=generator
            ).images[0]
        
        return image
```

#### 파라미터 최적화

**num_inference_steps (추론 스텝 수)**
```python
# 테스트 결과
steps_20 = generate(prompt, num_inference_steps=20)  # 30초, 품질: 중간
steps_30 = generate(prompt, num_inference_steps=30)  # 45초, 품질: 좋음
steps_50 = generate(prompt, num_inference_steps=50)  # 80초, 품질: 매우 좋음 ✅
steps_100 = generate(prompt, num_inference_steps=100)  # 160초, 품질: 약간 향상

# 결론: 50 steps가 품질/속도 최적점
```

**guidance_scale (가이던스 스케일)**
```python
# 프롬프트 충실도 조절
scale_1_0 = generate(prompt, guidance_scale=1.0)  # 자유로움, 창의적
scale_3_5 = generate(prompt, guidance_scale=3.5)  # 균형 ✅
scale_7_5 = generate(prompt, guidance_scale=7.5)  # 프롬프트 충실, 경직
scale_15 = generate(prompt, guidance_scale=15.0)  # 과도한 충실도, 부자연스러움

# 결론: 3.5가 최적 (자연스러우면서 프롬프트 잘 따름)
```

### 4.3. Img2Img 기능

#### 개념
- 기존 이미지를 참고하여 새로운 이미지 생성
- 레이아웃, 구도, 색감 유지하면서 스타일 변경

#### 구현

```python
def img2img_generate(
    self,
    init_image: Image.Image,
    prompt: str,
    strength: float = 0.6,
    **kwargs
) -> Image.Image:
    """이미지 기반 생성"""
    
    image = self.pipeline(
        prompt=prompt,
        image=init_image,
        strength=strength,  # 0.0 (원본 유지) ~ 1.0 (완전 새로 생성)
        **kwargs
    ).images[0]
    
    return image

# strength 파라미터 예시
strength_0_3 = img2img(init_image, prompt, strength=0.3)  # 원본과 매우 유사
strength_0_6 = img2img(init_image, prompt, strength=0.6)  # 균형 ✅
strength_0_9 = img2img(init_image, prompt, strength=0.9)  # 거의 새로 생성
```

### 4.4. 네거티브 프롬프트

#### 개념
- 원하지 않는 요소를 명시하여 제외

#### 구현

```python
negative_prompt = """
ugly, deformed, noisy, blurry, distorted, out of focus, 
bad anatomy, extra limbs, poorly drawn face, poorly drawn hands, 
missing fingers, low quality, worst quality, watermark, text, 
signature, username, jpeg artifacts, pixelated
"""

image = self.pipeline(
    prompt=prompt,
    negative_prompt=negative_prompt,
    **kwargs
).images[0]
```

---

## 5. 이미지 합성 및 후처리

### 5.1. 이미지 합성 (Compositing)

#### Alpha Blending

```python
def composite_images(
    foreground: Image.Image,  # RGBA (누끼 이미지)
    background: Image.Image,  # RGB (배경 이미지)
    position: tuple = (0, 0)
) -> Image.Image:
    """이미지 합성"""
    
    # 배경을 RGBA로 변환
    background_rgba = background.convert("RGBA")
    
    # 전경(누끼)을 배경 위에 붙이기
    background_rgba.paste(foreground, position, foreground)
    
    # RGB로 변환 (최종 출력)
    result = background_rgba.convert("RGB")
    
    return result
```

#### 스마트 배치

```python
def smart_position(
    foreground: Image.Image,
    background: Image.Image,
    position: str = "center"
) -> tuple:
    """제품을 배경의 적절한 위치에 배치"""
    
    fg_width, fg_height = foreground.size
    bg_width, bg_height = background.size
    
    if position == "center":
        x = (bg_width - fg_width) // 2
        y = (bg_height - fg_height) // 2
    elif position == "bottom":
        x = (bg_width - fg_width) // 2
        y = bg_height - fg_height - 50  # 하단 여백 50px
    elif position == "top":
        x = (bg_width - fg_width) // 2
        y = 50  # 상단 여백 50px
    else:
        x, y = 0, 0
    
    return (x, y)
```

### 5.2. 리터칭 (Retouching)

#### 경계선 블러 처리

```python
def blur_edges(
    composite_image: Image.Image,
    mask: np.ndarray,
    blur_radius: int = 5
) -> Image.Image:
    """제품과 배경 경계를 부드럽게"""
    import cv2
    from PIL import ImageFilter
    
    # 마스크 경계 추출
    kernel = np.ones((blur_radius, blur_radius), np.uint8)
    dilated = cv2.dilate(mask, kernel, iterations=1)
    eroded = cv2.erode(mask, kernel, iterations=1)
    edge = dilated - eroded
    
    # 경계 부분만 블러
    blurred = composite_image.filter(ImageFilter.GaussianBlur(blur_radius))
    
    # 원본과 블러 이미지 합성
    edge_mask = Image.fromarray(edge, mode="L")
    result = Image.composite(blurred, composite_image, edge_mask)
    
    return result
```

#### 색보정 (Color Correction)

```python
def color_correction(
    composite_image: Image.Image,
    foreground_mask: np.ndarray,
    ambient_color: tuple = (255, 240, 220)  # 따뜻한 조명
) -> Image.Image:
    """제품이 배경의 조명에 맞도록 색보정"""
    from PIL import ImageEnhance
    
    # RGB → HSV 변환
    hsv_image = composite_image.convert("HSV")
    hsv_array = np.array(hsv_image)
    
    # 마스크 영역만 색온도 조정
    for i in range(3):
        hsv_array[:, :, i] = np.where(
            foreground_mask > 128,
            hsv_array[:, :, i] * ambient_color[i] / 255,
            hsv_array[:, :, i]
        )
    
    # HSV → RGB 변환
    result = Image.fromarray(hsv_array, mode="HSV").convert("RGB")
    
    return result
```

#### 샤프닝 (Sharpening)

```python
def sharpen_image(
    image: Image.Image,
    factor: float = 1.5
) -> Image.Image:
    """이미지 선명도 향상"""
    from PIL import ImageEnhance
    
    enhancer = ImageEnhance.Sharpness(image)
    sharpened = enhancer.enhance(factor)
    
    return sharpened
```

### 5.3. 전체 후처리 파이프라인

```python
def full_postprocessing(
    composite_image: Image.Image,
    foreground_mask: np.ndarray
) -> Image.Image:
    """전체 후처리 적용"""
    
    # 1. 경계선 블러
    image = blur_edges(composite_image, foreground_mask, blur_radius=3)
    
    # 2. 색보정
    image = color_correction(image, foreground_mask)
    
    # 3. 샤프닝
    image = sharpen_image(image, factor=1.3)
    
    # 4. 밝기/대비 조정
    from PIL import ImageEnhance
    
    # 밝기 약간 증가
    enhancer = ImageEnhance.Brightness(image)
    image = enhancer.enhance(1.05)
    
    # 대비 약간 증가
    enhancer = ImageEnhance.Contrast(image)
    image = enhancer.enhance(1.1)
    
    return image
```

---

## 6. Qwen2-VL 이미지 분석

### 6.1. Qwen2-VL 개요

#### 모델 정보
- **개발사**: Alibaba Cloud
- **모델명**: Qwen2-VL-7B-Instruct
- **파라미터**: 7B (70억 개)
- **VRAM**: 약 10GB
- **추론 시간**: 5~10초
- **용도**: Vision-Language 분석

#### 특징
- 이미지 이해 및 설명
- 공간 구조 파악
- 색상 및 분위기 분석
- 텍스트 위치 추천

### 6.2. 구현

```python
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
import torch

class Qwen2VLAnalyzer:
    def __init__(self, device="cuda"):
        self.device = device
        self.model, self.processor = self.load_model()
    
    def load_model(self):
        """Qwen2-VL 모델 로드"""
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            "Qwen/Qwen2-VL-7B-Instruct",
            torch_dtype=torch.bfloat16
        )
        model.to(self.device)
        
        processor = AutoProcessor.from_pretrained(
            "Qwen/Qwen2-VL-7B-Instruct"
        )
        
        return model, processor
    
    def analyze_image(
        self,
        image: Image.Image,
        prompt: str = "이 이미지를 분석하여 공간 구조, 색상 분위기, 텍스트 배치하기 좋은 위치를 설명해줘."
    ) -> str:
        """이미지 분석"""
        
        # 입력 준비
        inputs = self.processor(
            text=prompt,
            images=image,
            return_tensors="pt"
        ).to(self.device)
        
        # 추론
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=300,
                temperature=0.7
            )
        
        # 디코딩
        analysis = self.processor.decode(
            outputs[0],
            skip_special_tokens=True
        )
        
        return analysis
```

### 6.3. 분석 예시

#### 입력 이미지
- Stage 1 결과 (배경 + 제품 합성 이미지)

#### 분석 프롬프트
```
이 이미지를 분석하여 다음 정보를 제공해줘:
1. 전체적인 색상 톤 (따뜻한/차가운/중성)
2. 주요 색상 (RGB 근사값)
3. 공간 구조 (제품 위치, 여백 위치)
4. 분위기 (밝은/어두운, 활기찬/차분한)
5. 텍스트 배치 추천 위치 (상단/중앙/하단)
6. 텍스트 색상 추천 (배경과 대비되는 색)
```

#### 분석 결과 예시
```
이 이미지는 따뜻한 조명의 전통 시장 분위기를 담고 있습니다.

1. 색상 톤: 따뜻한 톤
2. 주요 색상: 갈색(#8B4513), 황갈색(#D2691E), 밝은 노란색(#FFD700)
3. 공간 구조: 
   - 중앙 하단에 제품이 배치되어 있습니다
   - 상단에 넓은 여백(약 200px)이 있습니다
   - 좌우 여백도 충분합니다
4. 분위기: 밝고 활기차며, 신뢰감을 주는 전통적인 느낌
5. 텍스트 추천 위치: 상단 여백 (top)
6. 텍스트 색상 추천: 밝은 금색(#FFD700) 또는 흰색(#FFFFFF)
   - 이유: 갈색 배경과 대비가 좋고, 고급스러운 느낌
```

### 6.4. 텍스트 배치 활용

#### 자동 위치 결정

```python
def determine_text_position_from_analysis(analysis: str) -> str:
    """Qwen 분석 결과로 텍스트 위치 자동 결정"""
    
    analysis_lower = analysis.lower()
    
    # 키워드 매칭
    if "상단" in analysis or "top" in analysis_lower:
        return "top"
    elif "하단" in analysis or "bottom" in analysis_lower:
        return "bottom"
    elif "중앙" in analysis or "center" in analysis_lower:
        return "center"
    else:
        # 기본값
        return "top"
```

#### LLM에 분석 결과 전달

```python
# Qwen 분석 결과를 HTML 생성 프롬프트에 포함
html_prompt = f"""
이미지 분석 정보:
{qwen_analysis}

위 분석 결과를 바탕으로 텍스트를 배치할 HTML/CSS를 생성해주세요.
- 추천된 위치에 텍스트 배치
- 추천된 색상 사용
- 배경과 조화롭게
"""

html_code = llm_service.generate_html(html_prompt)
```

---

## 7. 모델 비교 및 최적화

### 7.1. SDXL vs FLUX 실전 테스트

#### 테스트 시나리오

**프롬프트 (동일)**
```
A traditional Korean market scene with dried seafood products 
displayed on a rustic wooden table. Warm lighting creates an 
inviting atmosphere. Photorealistic, high quality, 8k resolution.
```

**제품 이미지**
- 건어물 사진 (1024x1024)

#### 결과 비교

| 평가 항목 | SDXL | FLUX.1-dev | 승자 |
|-----------|------|------------|------|
| **프롬프트 충실도** | 60% | 95% | ✅ FLUX |
| **배경 품질** | 중간 | 매우 높음 | ✅ FLUX |
| **제품 조화** | 어색함 | 자연스러움 | ✅ FLUX |
| **색감** | 과도한 채도 | 자연스러움 | ✅ FLUX |
| **안정성** | 결과 편차 큼 | 일관됨 | ✅ FLUX |
| **생성 시간** | 20초 | 80초 | ✅ SDXL |

#### 실제 사용자 평가 (팀원 5명)

| 평가자 | SDXL 점수 | FLUX 점수 | 선호 모델 |
|--------|-----------|-----------|-----------|
| 김명환 | 3.2 / 5 | 4.8 / 5 | FLUX |
| 김민혁 | 2.8 / 5 | 4.6 / 5 | FLUX |
| 박지윤 | 3.0 / 5 | 4.7 / 5 | FLUX |
| 이건희 | 3.5 / 5 | 4.5 / 5 | FLUX |
| 이솔형 | 2.5 / 5 | 4.9 / 5 | FLUX |
| **평균** | **3.0 / 5** | **4.7 / 5** | **FLUX (100%)** |

### 7.2. FLUX.1 vs FLUX.2

#### FLUX.2 개선 사항
- 더 빠른 생성 속도 (80초 → 60초)
- 더 높은 품질
- 더 나은 세밀한 제어

#### 테스트 결과

| 항목 | FLUX.1 | FLUX.2 | 비고 |
|------|--------|--------|------|
| 생성 시간 | 80초 | 60초 | 25% 개선 |
| CLIP Score | 0.78 | 0.82 | 5% 향상 |
| 품질 (주관) | 매우 좋음 | 탁월함 | 약간 향상 |
| 안정성 | 95% | 97% | 약간 향상 |

**결론**
- FLUX.2가 모든 면에서 우수
- 단, FLUX.1도 충분히 높은 품질
- 프로젝트는 FLUX.1으로 진행 (안정성 확보)
- FLUX.2는 옵션으로 제공

### 7.3. 최적화 작업

#### GPU 메모리 최적화

**문제**
- FLUX: 16GB, Qwen: 10GB → 총 26GB 필요
- 사용 가능: 24GB

**해결**
```python
# JIT 로딩/언로딩 (김명환 담당)
# 1. FLUX 로드 → 배경 생성 → FLUX 언로드
# 2. Qwen 로드 → 이미지 분석 → Qwen 언로드

# 결과: 24GB로 모든 작업 가능 ✅
```

#### 추론 속도 최적화

**최적화 기법**
1. **bfloat16 사용**
```python
model = FluxPipeline.from_pretrained(
    "black-forest-labs/FLUX.1-dev",
    torch_dtype=torch.bfloat16  # float32 대비 2배 빠름
)
```

2. **CPU Offload**
```python
pipeline.enable_model_cpu_offload()  # VRAM 절약, 속도 약간 저하
```

3. **Steps 최적화**
```python
# 50 steps가 품질/속도 최적점
num_inference_steps = 50
```

#### 품질 최적화

**프롬프트 엔지니어링 (김민혁과 협업)**
- 100단어 이상 상세 프롬프트
- 품질 키워드 포함
- 네거티브 프롬프트 활용

**파라미터 튜닝**
- guidance_scale: 3.5 (최적값)
- num_inference_steps: 50 (최적값)
- strength (img2img): 0.6 (최적값)

---

## 8. 성과 및 향후 계획

### 8.1. 주요 성과

#### 정량적 성과
- **CLIP Score 평균**: 0.78 (SDXL 대비 50% 향상)
- **사용자 만족도**: 4.7 / 5.0
- **이미지 생성 성공률**: 95% 이상
- **배경 제거 정확도**: 95% 이상

#### 정성적 성과
- 이미지 생성 모델 비교 및 선정 경험
- GPU 메모리 최적화 경험
- 이미지 처리 파이프라인 구축
- Vision-Language 모델 활용 경험

### 8.2. 개선이 필요한 부분

#### 속도
- FLUX 추론 시간 80초 → 목표 30초 이하
- 양자화, TensorRT 최적화 필요

#### 품질
- 일부 카테고리(예: 의류)에서 품질 저하
- Fine-tuning 필요

#### 안정성
- 5% 실패율 개선 필요
- 에러 핸들링 강화

### 8.3. 향후 계획

#### 단기 (1~3개월)
- [x] FLUX.2 통합 완료
- [ ] 모델 양자화 (INT8) 적용
- [ ] TensorRT 최적화
- [ ] Inpainting 기능 추가

#### 중기 (3~6개월)
- [ ] Fine-tuning (한국 전통시장 특화)
- [ ] LoRA 학습 (스타일별)
- [ ] 영상 생성 (Veo2, Sora)
- [ ] 멀티 GPU 지원

#### 장기 (6개월 이상)
- [ ] 자체 모델 개발
- [ ] 실시간 생성 (< 10초)
- [ ] 3D 생성
- [ ] AR/VR 통합

### 8.4. 배운 점

#### 기술적 학습
- 생성 모델 비교 및 선정 방법론
- GPU 메모리 관리 및 최적화
- 이미지 합성 및 후처리 기법
- Vision-Language 모델 활용

#### 문제 해결
- SDXL 품질 문제 → FLUX로 전환
- GPU 메모리 부족 → JIT 로딩
- 속도 저하 → 파라미터 최적화

#### 협업
- 김민혁 (프롬프트)와 긴밀한 협업
- 김명환 (아키텍처)와 통합 작업
- 이건희 (백엔드)와 API 연동

---

## 결론

### 핵심 성과
- FLUX.1-dev 선정으로 고품질 이미지 생성 실현
- BiRefNet + FLUX + 합성 파이프라인 완성
- Qwen2-VL 통합으로 지능형 텍스트 배치
- CLIP Score 0.78 달성

### 기술적 의의
- 생성 모델 실전 적용 경험
- 이미지 처리 파이프라인 구축
- Vision-Language 멀티모달 AI 활용

### 향후 발전 방향
- 단기: 속도 최적화 (양자화, TensorRT)
- 중기: Fine-tuning 및 영상 생성
- 장기: 자체 모델 개발 및 실시간 생성

---

**감사합니다!**

다음 순서: 이건희 - 백엔드 및 프론트엔드 개발
