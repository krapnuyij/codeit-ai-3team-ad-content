---
layout: default
title: "발표 자료 - 김명환 (아키텍처) - 시스템 설계 및 모델 서버"
description: "코드잇 AI 4기 3팀 최종 발표 - 김명환 - 시스템 아키텍처 설계 및 모델 서버 구축"
date: 2026-01-27
author: "김명환"
cache-control: no-cache
expires: 0
pragma: no-cache
---

# 발표 자료 - 김명환 (아키텍처/Data)
## 시스템 아키텍처 설계 및 모델 서버 구축

**발표자**: 김명환 (아키텍처/Data 담당)

**발표 시간**: 10분

**발표 순서**: 2부 - 시스템 설계

---

## 목차

1. [역할 소개](#1-역할-소개)
2. [시스템 아키텍처 설계](#2-시스템-아키텍처-설계)
3. [MCP 서버 및 어댑터](#3-mcp-서버-및-어댑터)
4. [모델 서버 구축](#4-모델-서버-구축)
5. [AI 모델 파이프라인](#5-ai-모델-파이프라인)
6. [GPU 메모리 최적화](#6-gpu-메모리-최적화)
7. [Docker 배포 환경](#7-docker-배포-환경)
8. [API 설계](#8-api-설계)
9. [기술적 도전과 해결](#9-기술적-도전과-해결)
10. [성과 및 향후 계획](#10-성과-및-향후-계획)

---

## 1. 역할 소개

### 1.1. 담당 역할

**아키텍처/Data 담당**
- 시스템 아키텍처 총괄 설계
- 데이터 파이프라인 구성
- 모델 서버(nanoCocoa_aiserver) 설계 및 구현
- Docker 배포 환경 구성
- API 설계 및 문서화

### 1.2. 주요 책임

#### 시스템 설계
- 마이크로서비스 아키텍처 설계
- 각 계층 간 통신 방식 정의
- 데이터 흐름 설계
- 확장 가능한 구조 설계

#### 모델 서버 구축
- AI 모델 서빙 시스템 구축
- GPU 리소스 관리 및 최적화
- 비동기 처리 파이프라인 구현
- 모델 로딩/언로딩 전략 수립

#### 인프라 구성
- Docker 기반 배포 환경 구성
- Docker Compose 오케스트레이션
- 네트워크 구성 및 포트 관리
- 볼륨 마운트 및 데이터 관리

### 1.3. 개발 기간별 주요 활동

| 기간 | 주요 활동 | 산출물 |
|------|-----------|--------|
| Week 1 (12/29~1/2) | 아키텍처 초안 설계, 기술 스택 선정 | 아키텍처 설계 초안 |
| Week 2 (1/3~1/9) | 마이크로서비스 설계, API 설계 | 아키텍처 설계서, API 명세서 |
| Week 3 (1/10~1/16) | 모델 서버 구현, GPU 최적화 | nanoCocoa_aiserver v1.0 |
| Week 4 (1/17~1/23) | Docker 배포, 시스템 통합 | Docker Compose 설정 |
| Week 5 (1/24~1/27) | 최적화, 문서화, 양자화 적용 | 최종 아키텍처, 기술 문서 |

---

## 2. 시스템 아키텍처 설계

### 2.1. 아키텍처 선택 배경

#### 왜 마이크로서비스인가?

**모놀리식 아키텍처의 한계**
- 단일 장애점: 한 부분의 오류가 전체 서비스 중단
- 확장의 어려움: 특정 기능만 확장 불가능
- 기술 스택 고정: 모든 부분이 동일한 기술 사용
- 배포 리스크: 작은 변경에도 전체 재배포 필요

**마이크로서비스의 장점**
- 독립적인 배포: 각 서비스를 개별적으로 배포
- 장애 격리: 한 서비스 오류가 다른 서비스에 영향 최소화
- 기술 스택 유연성: 서비스별 최적 기술 선택
- 확장 용이성: 필요한 서비스만 스케일링

### 2.2. 전체 시스템 구조

#### 계층별 구성

```mermaid
graph TB
    subgraph Client["클라이언트 계층"]
        Web["웹 브라우저<br/>(HTTP)"]
        Desktop["LLM Desktop<br/>(MCP Protocol)"]
    end
    
    subgraph Presentation["프레젠테이션 계층 (Port: 8890)"]
        Backend["Backend<br/>(FastAPI + Jinja2)<br/>- 사용자 인터페이스 제공<br/>- 입력 검증 및 요청 라우팅<br/>- 결과 표시 및 파일 다운로드"]
    end
    
    subgraph Business["비즈니스 로직 계층"]
        BL["비즈니스 로직<br/>(Port: 8080)<br/>- LLM 연동<br/>- 프롬프트 생성<br/>- 작업 관리"]
        HP["홈페이지 생성<br/>(Port: 8891)<br/>- LangGraph<br/>- Multi-Agent<br/>- HTML 생성"]
        MCP["MCP 서버<br/>(Port: 3000)<br/>- MCP Bridge<br/>- SSE 통신<br/>- 도구 제공"]
    end
    
    subgraph ModelServing["모델 서빙 계층 (Port: 8000)"]
        subgraph AIServer["nanoCocoa_aiserver"]
            subgraph Pipeline["AI 모델 파이프라인"]
                BiRefNet["BiRefNet<br/>(누끼)"]
                FLUX["FLUX.1<br/>(배경)"]
                Qwen["Qwen2-VL<br/>(분석)"]
                GPT["OpenAI GPT-5-mini<br/>(HTML/CSS)"]
            end
            GPU["NVIDIA L4 GPU<br/>24GB VRAM"]
        end
    end
    
    Web --> Backend
    Desktop --> Backend
    Backend --> BL
    Backend --> HP
    Backend --> MCP
    BL --> AIServer
    HP --> AIServer
    MCP --> AIServer
    BiRefNet --> FLUX
    FLUX --> Qwen
    Qwen --> GPT
    GPT --> GPU
```

### 2.3. 계층별 상세 설계

#### 2.3.1. 프레젠테이션 계층 (Backend)

**기술 스택**
- FastAPI (웹 프레임워크)
- Jinja2 (템플릿 엔진)
- HTML/CSS/JavaScript (프론트엔드)

**주요 기능**
- 사용자 인터페이스 제공
- 이미지 업로드 처리
- 폼 데이터 검증
- 실시간 진행 상황 표시 (폴링)
- 결과물 표시 및 다운로드

**설계 포인트**
- RESTful API 설계
- 비동기 처리 (async/await)
- 에러 핸들링 및 사용자 피드백
- 반응형 UI (모바일 지원)

#### 2.3.2. 비즈니스 로직 계층

**Backend Service (Port: 8080)**
- LLM(GPT-5-mini) 연동
- 프롬프트 생성 및 변환
- 작업 상태 관리
- 데이터베이스 연동

**Homepage Generator (Port: 8891)**
- LangGraph 기반 멀티에이전트
- 기획/디자인/개발 에이전트 협업
- HTML/CSS/JavaScript 생성
- Playwright 렌더링

**MCP Server (Port: 3000)**
- MCP 프로토콜 브릿지
- SSE (Server-Sent Events) 통신
- LLM Desktop/Code 연동
- 8개 도구 제공
- REST API 기반 통합

**MCP Adapter**
- REST API를 MCP 프로토콜로 변환
- nanoCocoa_aiserver와 통신
- LLM 도구로 광고 서비스 제공

#### 2.3.3. 모델 서빙 계층 (nanoCocoa_aiserver)

**핵심 설계 원칙**
1. **독립성**: 다른 서비스와 독립적으로 동작
2. **확장성**: GPU 추가 시 쉽게 확장 가능
3. **효율성**: JIT 로딩으로 메모리 최적화
4. **안정성**: 에러 발생 시 graceful degradation

**주요 컴포넌트**
- FastAPI REST API 서버
- 비동기 작업 큐 (멀티프로세싱)
- GPU 리소스 매니저
- 모델 로더/언로더
- 이미지 처리 파이프라인

### 2.4. 통신 방식 설계

#### 2.4.1. 동기 vs 비동기

| 통신 유형 | 방식 | 사용 사례 | 이유 |
|-----------|------|-----------|------|
| 동기 (Synchronous) | HTTP Request/Response | - API 헬스 체크<br>- 간단한 데이터 조회 | 즉시 응답 필요 |
| 비동기 (Asynchronous) | Job ID + Polling | - 이미지 생성<br>- 홈페이지 생성 | 처리 시간 2~3분 |
| 스트리밍 (Streaming) | SSE | - MCP 프로토콜<br>- 실시간 이벤트 | 지속적 연결 필요 |

#### 2.4.2. 비동기 처리 흐름

```mermaid
sequenceDiagram
    participant Client as 클라이언트
    participant Backend as 백엔드
    participant Model as 모델 서버
    
    Client->>Backend: POST /generate
    Backend->>Model: POST /generate
    Note over Model: Job ID 생성<br/>Worker 시작
    Model-->>Backend: {job_id}
    Backend-->>Client: {job_id}
    
    Note over Model: 이미지 생성 중...
    
    Client->>Backend: GET /status/{id}
    Backend->>Model: GET /status
    Model-->>Backend: {progress: 50%}
    Backend-->>Client: {progress: 50%}
    
    Note over Client: 반복 폴링
    
    Client->>Backend: GET /status/{id}
    Backend->>Model: GET /status
    Model-->>Backend: {status: done}
    Backend-->>Client: 최종 결과
```

#### 2.4.3. Docker 내부 네트워크

**네트워크 설계**
- 네트워크명: `nanococoa-network`
- 드라이버: bridge
- 서비스 간 통신: 서비스명으로 직접 통신
- 외부 노출: 필요한 포트만 호스트에 매핑

**서비스 간 통신 예시**
```python
# Backend에서 nanoCocoa_aiserver 호출
AISERVER_URL = "http://nanococoa-aiserver:8000"  # 내부 네트워크
response = requests.post(f"{AISERVER_URL}/generate", json=payload)

# 외부에서는 호스트 IP:포트로 접근
# http://34.44.205.198:8000/generate
```

---

## 3. MCP 서버 및 어댑터

### 3.1. MCP (Model Context Protocol) 개요

#### 3.1.1. MCP란?

**정의**
- Anthropic이 개발한 오픈 프로토콜
- LLM과 외부 데이터/도구를 연결하는 표준 인터페이스
- LLM Desktop, Claude Code 등에서 지원

**주요 특징**
- **표준화**: 통일된 프로토콜로 다양한 도구 통합
- **SSE 통신**: Server-Sent Events로 실시간 스트리밍
- **도구 기반**: LLM이 직접 호출할 수 있는 함수 제공
- **확장 가능**: 새로운 도구 추가 용이

#### 3.1.2. 왜 MCP인가?

**기존 방식의 한계**
- 웹 브라우저 기반 UI만 제공 → LLM에서 직접 사용 불가
- REST API는 있지만 LLM이 호출하기 복잡
- 각 LLM마다 다른 통합 방식 필요

**MCP의 장점**
- LLM이 자연어로 서비스 사용 가능
- LLM Desktop/Code에서 바로 사용
- 프롬프트만으로 복잡한 작업 수행
- 다른 MCP 도구와 조합 가능

### 3.2. 시스템 구조

#### 3.2.1. MCP 아키텍처

```mermaid
graph TB
    subgraph Client["클라이언트"]
        Desktop["LLM Desktop<br/>(Claude, GPT)"]
        Code["LLM Code<br/>(VS Code)"]
    end
    
    subgraph MCPLayer["MCP 계층"]
        MCPServer["nanoCocoa_mcpserver<br/>(Port: 3000)"]
        MCPAdapter["MCP Adapter<br/>- 프로토콜 변환<br/>- 요청 검증<br/>- 응답 포맷팅"]
    end
    
    subgraph Backend["백엔드 계층"]
        REST["REST API<br/>nanoCocoa_aiserver<br/>(Port: 8000)"]
    end
    
    Desktop -->|SSE| MCPServer
    Code -->|SSE| MCPServer
    MCPServer --> MCPAdapter
    MCPAdapter -->|HTTP| REST
    REST -->|JSON| MCPAdapter
    MCPAdapter -->|SSE Stream| MCPServer
    
    style MCPLayer stroke:#FF9800,stroke-width:3px
    style MCPAdapter stroke:#4CAF50,stroke-width:2px
```

#### 3.2.2. 데이터 흐름

```mermaid
sequenceDiagram
    participant User as 사용자
    participant LLM as LLM Desktop
    participant MCP as MCP Server
    participant Adapter as MCP Adapter
    participant API as REST API
    
    User->>LLM: "사과 광고 이미지 생성해줘"
    LLM->>MCP: tool call: generate_ad_image(product="사과")
    MCP->>Adapter: MCP 프로토콜 요청
    Adapter->>Adapter: 프로토콜 변환<br/>(MCP → REST)
    Adapter->>API: POST /generate
    Note over API: 이미지 생성 (10~15분)
    API-->>Adapter: {job_id, status}
    Adapter->>Adapter: 응답 변환<br/>(REST → MCP)
    Adapter-->>MCP: MCP 응답
    MCP-->>LLM: 도구 실행 결과
    LLM-->>User: "이미지 생성이 시작되었습니다."
    
    Note over User,API: 폴링으로 상태 확인
    
    LLM->>MCP: tool call: check_status(job_id)
    MCP->>Adapter: 상태 조회
    Adapter->>API: GET /status/{job_id}
    API-->>Adapter: {progress: 50%}
    Adapter-->>MCP: 진행 상황
    MCP-->>LLM: 진행 상황
    LLM-->>User: "50% 완료되었습니다."
```

### 3.3. MCP Adapter 설계

#### 3.3.1. 역할

**프로토콜 변환**
- MCP 프로토콜 → REST API 호출
- REST API 응답 → MCP 프로토콜 응답
- 데이터 타입 변환 (Base64, JSON 등)

**요청 검증**
- MCP 도구 파라미터 검증
- 필수 파라미터 확인
- 데이터 형식 검증

**에러 핸들링**
- REST API 오류를 MCP 형식으로 변환
- 사용자 친화적인 오류 메시지
- 재시도 로직

#### 3.3.2. 제공 도구 (8개)

| 도구명 | 설명 | 파라미터 | 반환값 |
|--------|------|----------|--------|
| `generate_ad_image` | 광고 이미지 생성 | product_image, text, bg_prompt | job_id, status |
| `check_generation_status` | 생성 상태 확인 | job_id | progress, message, result |
| `analyze_image` | 이미지 분석 (Qwen2-VL) | image | 분석 텍스트 |
| `calculate_clip_score` | CLIP Score 계산 | image, prompt | score, interpretation |
| `list_fonts` | 사용 가능 폰트 목록 | - | fonts[] |
| `generate_homepage` | 홈페이지 생성 | business_info, style | homepage_url |
| `get_system_status` | 시스템 상태 확인 | - | gpu_info, queue_size |
| `stop_generation` | 생성 작업 중단 | job_id | stopped |

#### 3.3.3. 구현 예시

**MCP 도구 정의**

```python
# nanoCocoa_mcpserver/tools/generate_ad_image.py
from typing import Dict, Any

async def generate_ad_image(
    product_image: str,
    text_content: str,
    bg_prompt: str,
    use_qwen_analysis: bool = True
) -> Dict[str, Any]:
    """
    광고 이미지를 생성합니다.
    
    Args:
        product_image: Base64 인코딩된 제품 이미지
        text_content: 광고 문구 (한글)
        bg_prompt: 배경 프롬프트 (영문)
        use_qwen_analysis: Qwen2-VL 분석 사용 여부
    
    Returns:
        {
            "job_id": "uuid",
            "status": "pending",
            "message": "작업이 시작되었습니다."
        }
    """
    # MCP Adapter를 통해 REST API 호출
    adapter = MCPAdapter()
    response = await adapter.call_rest_api(
        method="POST",
        endpoint="/generate",
        data={
            "product_image": product_image,
            "text_content": text_content,
            "bg_prompt": bg_prompt,
            "use_qwen_analysis": use_qwen_analysis
        }
    )
    return response
```

**MCP Adapter 구현**

```python
# nanoCocoa_mcpserver/adapter/mcp_adapter.py
import aiohttp
from typing import Dict, Any

class MCPAdapter:
    def __init__(self):
        self.base_url = "http://nanococoa_aiserver:8000"
    
    async def call_rest_api(
        self,
        method: str,
        endpoint: str,
        data: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """REST API 호출 및 응답 변환"""
        async with aiohttp.ClientSession() as session:
            url = f"{self.base_url}{endpoint}"
            
            # REST API 호출
            if method == "POST":
                async with session.post(url, json=data) as resp:
                    result = await resp.json()
            elif method == "GET":
                async with session.get(url) as resp:
                    result = await resp.json()
            
            # MCP 형식으로 변환
            return self._convert_to_mcp_format(result)
    
    def _convert_to_mcp_format(self, rest_response: Dict) -> Dict:
        """REST 응답을 MCP 형식으로 변환"""
        return {
            "success": "error" not in rest_response,
            "data": rest_response,
            "message": rest_response.get("message", "Success")
        }
```

### 3.4. 사용 시나리오

#### 3.4.1. LLM Desktop에서 사용

**시나리오: 사용자가 자연어로 광고 이미지 생성**

```
사용자: "빨간 사과 이미지로 '명절 특가! 한 박스 20,000원' 
        문구가 들어간 광고 이미지를 만들어줘. 
        배경은 한국 전통 시장 느낌으로."

LLM: [generate_ad_image 도구 호출]
     - product_image: [업로드된 사과 이미지]
     - text_content: "명절 특가! 한 박스 20,000원"
     - bg_prompt: "Korean traditional market, wooden table..."
     
MCP Server → Adapter → REST API → 이미지 생성 시작

LLM: "이미지 생성이 시작되었습니다. 작업 ID: abc-123"

[10~15분 후]

사용자: "이미지 생성 완료됐어?"

LLM: [check_generation_status 도구 호출]
     - job_id: "abc-123"
     
MCP Server → Adapter → REST API → 상태 조회

LLM: "완료되었습니다! 결과 이미지를 확인하세요."
```

#### 3.4.2. 다른 MCP 도구와 조합

```
사용자: "사과 광고 이미지 3개 만들어서 CLIP Score 비교해줘"

LLM: 
  1. generate_ad_image(bg_prompt="시장") → job_id_1
  2. generate_ad_image(bg_prompt="과수원") → job_id_2
  3. generate_ad_image(bg_prompt="상점") → job_id_3
  
  [생성 완료 대기]
  
  4. calculate_clip_score(image_1, "사과 광고") → 0.73
  5. calculate_clip_score(image_2, "사과 광고") → 0.68
  6. calculate_clip_score(image_3, "사과 광고") → 0.71
  
  → "시장 배경 이미지가 가장 높은 점수(0.73)입니다."
```

### 3.5. 기술적 이점

#### 3.5.1. REST API 기반 통합

**장점**
- 기존 REST API 재사용 (중복 개발 불필요)
- API 업데이트 시 MCP도 자동 반영
- 모듈화된 구조 (MCP 제거 시 REST API는 정상 작동)

**구조**
```
REST API (nanoCocoa_aiserver)
    ↓ HTTP 호출
MCP Adapter (프로토콜 변환)
    ↓ MCP 프로토콜
MCP Server (nanoCocoa_mcpserver)
    ↓ SSE
LLM Desktop/Code
```

#### 3.5.2. 확장 가능성

**새로운 도구 추가**
1. REST API에 새 엔드포인트 추가
2. MCP Adapter에 변환 로직 추가
3. MCP Server에 도구 정의 추가
→ 3단계만으로 새 기능 LLM에 노출

**다른 서비스 통합**
- Homepage Generator → MCP 도구로 노출
- CLIP Score → MCP 도구로 노출
- 향후 추가 서비스도 동일한 패턴

#### 3.5.3. 사용자 경험 향상

**Before (웹 UI만 제공)**
- 브라우저 열기 → 로그인 → 이미지 업로드 → 폼 작성 → 생성 대기

**After (MCP 지원)**
- LLM에게 자연어로 요청 → 자동 생성 → 결과 확인

**장점**
- 복잡한 UI 학습 불필요
- 자연어로 직관적 사용
- 다른 작업과 동시 진행 가능
- LLM이 최적 파라미터 제안

---

## 4. 모델 서버 구축

### 3.1. nanoCocoa_aiserver 개요

#### 설계 목표
1. **고성능**: GPU 활용 최대화
2. **안정성**: 에러 발생 시 시스템 중단 방지
3. **확장성**: 새로운 모델 추가 용이
4. **모니터링**: 실시간 진행 상황 추적

#### 핵심 기능
- AI 모델 추론 서빙
- GPU 리소스 관리
- 비동기 작업 처리
- 실시간 진행 상황 보고
- 시스템 메트릭 수집

### 3.2. 디렉토리 구조

```
nanoCocoa_aiserver/
├── main.py                 # FastAPI 앱 진입점
├── config.py               # 설정 관리
├── requirements.txt        # 의존성 패키지
├── Dockerfile              # Docker 이미지 빌드
├── docker-compose.yml      # Docker Compose 설정
│
├── models/                 # AI 모델 관련
│   ├── __init__.py
│   ├── birefnet.py         # BiRefNet 누끼 모델
│   ├── flux_model.py       # FLUX.1-dev 배경 생성
│   ├── qwen_model.py       # Qwen2-VL 이미지 분석
│   └── clip_score.py       # CLIP Score 평가
│
├── pipeline/               # 이미지 생성 파이프라인
│   ├── __init__.py
│   ├── stage1.py           # Stage 1: 배경 생성
│   ├── stage2.py           # Stage 2: 텍스트 렌더링
│   └── process.py          # 전체 파이프라인 오케스트레이션
│
├── utils/                  # 유틸리티
│   ├── __init__.py
│   ├── image_utils.py      # 이미지 처리
│   ├── gpu_utils.py        # GPU 관리
│   └── job_manager.py      # 작업 관리
│
├── api/                    # API 엔드포인트
│   ├── __init__.py
│   ├── generate.py         # 이미지 생성 API
│   ├── status.py           # 상태 조회 API
│   └── clip_score.py       # CLIP Score API
│
├── static/                 # 정적 파일
│   ├── uploads/            # 업로드된 이미지
│   └── results/            # 생성된 결과물
│
└── logs/                   # 로그 파일
    └── app.log
```

### 3.3. 핵심 모듈 설계

#### 3.3.1. Job Manager (작업 관리)

**역할**
- Job ID 생성 및 관리
- 작업 상태 추적
- 진행 상황 업데이트
- 결과물 저장 및 조회

**데이터 구조**
```python
class JobStatus:
    job_id: str              # UUID v4
    status: str              # "pending" | "running" | "completed" | "error"
    progress_percent: int    # 0~100
    current_step: str        # "step1_background" | "step2_text"
    message: str             # 상태 메시지
    elapsed_sec: float       # 경과 시간
    eta_seconds: float       # 예상 남은 시간
    step1_result: str        # Base64 중간 결과
    final_result: str        # Base64 최종 결과
    error_message: str       # 에러 메시지
    system_metrics: dict     # CPU, RAM, GPU 메트릭
```

#### 3.3.2. GPU Resource Manager

**역할**
- GPU 메모리 모니터링
- 모델 로딩/언로딩 관리
- VRAM 사용량 추적
- 메모리 부족 방지

**주요 기능**
```python
class GPUManager:
    def get_gpu_info() -> List[Dict]:
        """GPU 정보 조회 (VRAM, 사용률 등)"""
        
    def load_model(model_name: str) -> Model:
        """모델 로드 (메모리 체크 후)"""
        
    def unload_model(model: Model):
        """모델 언로드 및 메모리 해제"""
        
    def clear_cache():
        """GPU 캐시 정리"""
```

#### 3.3.3. Pipeline Orchestrator

**역할**
- 2단계 파이프라인 실행
- 단계별 진행 상황 보고
- 에러 핸들링 및 복구
- 중간 결과물 저장

**실행 흐름**
```python
def execute_pipeline(job_id: str, params: dict):
    # Stage 1: 배경 생성
    update_status(job_id, progress=10, message="BiRefNet 로딩...")
    product_nobg = remove_background(product_image)  # BiRefNet
    
    update_status(job_id, progress=30, message="FLUX 배경 생성...")
    background = generate_background(bg_prompt)  # FLUX.1-dev
    
    update_status(job_id, progress=50, message="이미지 합성...")
    step1_result = composite_images(product_nobg, background)
    
    # Stage 2: 텍스트 렌더링
    if use_qwen_analysis:
        update_status(job_id, progress=60, message="Qwen 이미지 분석...")
        analysis = analyze_image(step1_result)  # Qwen2-VL
    
    update_status(job_id, progress=80, message="HTML 텍스트 생성...")
    html = generate_html(text_content, analysis)  # LLM
    
    update_status(job_id, progress=90, message="최종 합성...")
    final_result = render_and_composite(step1_result, html)
    
    update_status(job_id, progress=100, message="완료!", final_result=final_result)
```

### 3.4. API 설계

#### 3.4.1. 주요 엔드포인트

| 메서드 | 경로 | 설명 | 요청 | 응답 |
|--------|------|------|------|------|
| POST | `/generate` | 이미지 생성 시작 | GenerateRequest | {job_id, status} |
| GET | `/status/{job_id}` | 작업 상태 조회 | - | JobStatus |
| POST | `/stop/{job_id}` | 작업 중단 | - | {status: "stopped"} |
| GET | `/health` | 서버 상태 확인 | - | {status, gpu_available} |
| GET | `/fonts` | 폰트 목록 조회 | - | List[Font] |
| POST | `/clip-score` | CLIP Score 계산 | ClipScoreRequest | ClipScoreResponse |

#### 3.4.2. GenerateRequest 스키마

```python
class GenerateRequest(BaseModel):
    start_step: int = 1                    # 시작 단계 (1 or 2)
    product_image: str                     # Base64 인코딩 이미지
    text_content: str                      # 광고 문구
    bg_prompt: str                         # 배경 프롬프트 (영문)
    html_style_prompt: str = ""            # 텍스트 스타일 프롬프트
    use_qwen_analysis: bool = True         # Qwen 분석 사용 여부
    use_llm_html: bool = True              # LLM HTML 생성 여부
    text_position: str = "auto"            # 텍스트 위치 (auto/top/bottom/center)
    strength: float = 0.6                  # FLUX 강도 (0.0~1.0)
    guidance_scale: float = 3.5            # FLUX 가이던스 스케일
    test_mode: bool = False                # 테스트 모드 (더미 결과)
```

#### 3.4.3. JobStatus 응답 예시

```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "running",
  "progress_percent": 65,
  "current_step": "step2_html_rendering",
  "sub_step": "qwen_analysis",
  "message": "Qwen2-VL로 이미지 분석 중...",
  "elapsed_sec": 87.3,
  "eta_seconds": 45,
  "step_eta_seconds": 25,
  "system_metrics": {
    "cpu_percent": 52.1,
    "ram_used_gb": 14.2,
    "ram_total_gb": 32.0,
    "ram_percent": 44.4,
    "gpu_info": [
      {
        "index": 0,
        "name": "NVIDIA L4",
        "vram_used_mb": 18234,
        "vram_total_mb": 24576,
        "vram_percent": 74.2,
        "utilization": 95
      }
    ]
  },
  "parameters": {
    "start_step": 1,
    "text_content": "명절 특가! 사과 한 박스 20,000원",
    "bg_prompt": "Korean traditional market..."
  },
  "step1_result": "data:image/png;base64,iVBORw0KG...",
  "final_result": null
}
```

---

## 4. AI 모델 파이프라인

### 4.1. 2단계 파이프라인 설계

#### Stage 1: 배경 생성 파이프라인

```mermaid
graph TB
    Input["입력 이미지<br/>(제품)"]
    BiRefNet["BiRefNet<br/>(Segment)<br/>- 제품만 분리<br/>- Alpha 채널 생성"]
    NoBG["제품 누끼 이미지"]
    FLUX["FLUX.1-dev<br/>(Text-to-Image)<br/>- 프롬프트 기반<br/>- 고품질 사실적 배경"]
    BG["배경 이미지"]
    Composite["이미지 합성<br/>(Compositing)<br/>- Alpha blending<br/>- 경계선 blur 처리"]
    Retouch["리터칭<br/>(Post-processing)<br/>- 색보정<br/>- 샤프닝"]
    Stage1["Stage 1 결과<br/>(배경이 합성된 제품 이미지)"]
    
    Input --> BiRefNet
    BiRefNet --> NoBG
    NoBG --> Composite
    NoBG -.-> FLUX
    FLUX --> BG
    BG --> Composite
    Composite --> Retouch
    Retouch --> Stage1
```

#### Stage 2: 텍스트 렌더링 파이프라인

```mermaid
graph TB
    Stage1Input["Stage 1 결과"]
    Qwen["Qwen2-VL<br/>(Vision-Language)<br/>- 공간 구조 파악<br/>- 색상 분위기 분석<br/>- 여백 위치 추천"]
    Analysis["이미지 분석 텍스트"]
    AdText["광고 문구"]
    LLM["OpenAI GPT-5-mini<br/>(LLM)<br/>- 텍스트 위치 결정<br/>- 폰트, 색상 선택<br/>- 그림자, 효과 추가"]
    HTML["HTML/CSS 코드"]
    Playwright["Playwright<br/>(Headless Browser)<br/>- HTML → PNG<br/>- 투명 배경"]
    TextLayer["텍스트 레이어 이미지"]
    FinalComposite["텍스트 합성<br/>(Layer Compositing)<br/>- 이미지 + 텍스트<br/>- Alpha blending"]
    Final["최종 광고 이미지"]
    
    Stage1Input --> Qwen
    Qwen --> Analysis
    Analysis --> LLM
    AdText --> LLM
    LLM --> HTML
    HTML --> Playwright
    Playwright --> TextLayer
    TextLayer --> FinalComposite
    Stage1Input --> FinalComposite
    FinalComposite --> Final
```

### 4.2. 모델별 상세 설명

#### 4.2.1. BiRefNet (누끼 제거)

**모델 정보**
- 이름: BiRefNet (Bilateral Reference Network)
- 용도: 배경 제거 (Segmentation)
- 메모리: 약 7GB VRAM
- 추론 시간: 약 3~5초

**입력/출력**
```python
# 입력
input_image: PIL.Image (RGB, 1024x1024)

# 출력
mask: np.ndarray (grayscale, 1024x1024)  # 0~255
nobg_image: PIL.Image (RGBA)             # Alpha 채널 포함
```

**특징**
- Segment Anything 계열 모델
- 높은 정확도의 경계선 추출
- 다양한 객체 타입 지원
- 후처리 없이도 깔끔한 결과

#### 4.2.2. FLUX.1-dev (배경 생성)

**모델 정보**
- 이름: FLUX.1-dev (Black Forest Labs)
- 용도: Text-to-Image 생성
- 파라미터: 12B (12억 개)
- 메모리: 약 16GB VRAM
- 추론 시간: 약 60~90초 (50 steps)

**입력/출력**
```python
# 입력
prompt: str                    # 영문 프롬프트 (100 단어 이상 권장)
height: int = 1024             # 이미지 높이
width: int = 1024              # 이미지 너비
num_inference_steps: int = 50  # 추론 스텝 수
guidance_scale: float = 3.5    # 가이던스 스케일
strength: float = 0.6          # Img2Img 강도 (0.0~1.0)

# 출력
generated_image: PIL.Image (RGB, 1024x1024)
```

**특징**
- Stable Diffusion 계열 최신 모델
- 프롬프트 이해도 매우 높음
- 사실적인 이미지 생성
- 일관성 있는 결과물

**프롬프트 엔지니어링**
```python
# 나쁜 예
prompt = "wooden table"

# 좋은 예
prompt = """
A traditional Korean wooden table with natural grain patterns, 
positioned in a cozy market setting. Warm sunlight streaming 
through windows creates soft shadows. The table surface is clean 
and polished, with a rustic charm. Background shows blurred 
market stalls with warm lighting. Photorealistic, high detail, 
professional product photography style, 8k resolution.
"""
```

#### 4.2.3. Qwen2-VL (이미지 분석)

**모델 정보**
- 이름: Qwen2-VL-7B-Instruct
- 용도: Vision-Language 분석
- 파라미터: 7B (7억 개)
- 메모리: 약 10GB VRAM
- 추론 시간: 약 5~10초

**입력/출력**
```python
# 입력
image: PIL.Image
prompt: str = "이 이미지를 분석하여 공간 구조, 색상 분위기, 텍스트 배치하기 좋은 위치를 설명해줘."

# 출력
analysis: str
"""
이 이미지는 따뜻한 조명의 전통 시장 분위기입니다. 
- 공간 구조: 중앙에 제품이 배치되어 있고, 상단과 하단에 여백이 있습니다.
- 색상: 갈색 계열의 따뜻한 톤이 주를 이룹니다.
- 텍스트 위치 추천: 상단 여백에 밝은 색상의 텍스트가 적합합니다.
"""
```

**활용 방법**
- LLM에게 이미지 정보 제공
- 텍스트 위치 자동 결정
- 색상/폰트 자동 선택

#### 4.2.4. OpenAI GPT-5-mini (HTML 생성)

**모델 정보**
- 이름: GPT-5-mini
- 용도: HTML/CSS 코드 생성
- 추론 시간: 약 2~5초 (API 호출)

**프롬프트 예시**
```python
system_prompt = """
당신은 광고 디자인 전문가입니다. 
이미지에 텍스트를 배치하기 위한 HTML/CSS를 생성합니다.
"""

user_prompt = f"""
다음 정보를 바탕으로 HTML/CSS를 생성해주세요:

이미지 정보:
{qwen_analysis}

텍스트 내용:
{text_content}

요구사항:
- 텍스트는 {text_position} 위치에 배치
- 배경 이미지와 조화로운 색상 선택
- 그림자 효과로 가독성 향상
- 반응형 폰트 크기
"""

# 응답
html_code = """
<div style="
    position: absolute;
    top: 50px;
    left: 50%;
    transform: translateX(-50%);
    font-size: 72px;
    font-weight: bold;
    color: #FFD700;
    text-shadow: 4px 4px 8px rgba(0,0,0,0.8);
    font-family: 'Nanum Gothic', sans-serif;
">
    명절 특가! 사과 한 박스 20,000원
</div>
"""
```

### 4.3. 파이프라인 실행 시간 분석

| 단계 | 작업 | 시간 | VRAM | 비고 |
|------|------|------|------|------|
| 0 | 이미지 전처리 | 1초 | - | 리사이즈, 정규화 |
| 1-1 | BiRefNet 로드 | 5초 | 7GB | 첫 실행 시만 |
| 1-2 | BiRefNet 추론 | 4초 | 7GB | 누끼 제거 |
| 1-3 | BiRefNet 언로드 | 2초 | - | 메모리 해제 |
| 1-4 | FLUX 로드 | 10초 | 16GB | 첫 실행 시만 |
| 1-5 | FLUX 추론 | 80초 | 16GB | 배경 생성 |
| 1-6 | FLUX 언로드 | 3초 | - | 메모리 해제 |
| 1-7 | 이미지 합성 | 2초 | - | CPU 작업 |
| **Stage 1 합계** | | **~107초** | | |
| 2-1 | Qwen 로드 | 8초 | 10GB | 선택적 |
| 2-2 | Qwen 추론 | 6초 | 10GB | 이미지 분석 |
| 2-3 | Qwen 언로드 | 2초 | - | 메모리 해제 |
| 2-4 | LLM HTML 생성 | 3초 | - | OpenAI API |
| 2-5 | HTML 렌더링 | 5초 | - | Playwright |
| 2-6 | 텍스트 합성 | 2초 | - | CPU 작업 |
| **Stage 2 합계** | | **~26초** | | |
| **전체 합계** | | **~133초** | | **약 2분 20초** |

---

## 5. GPU 메모리 최적화

### 5.1. 메모리 제약 문제

#### 문제 상황
- **사용 가능한 VRAM**: 24GB (NVIDIA L4)
- **필요한 VRAM**:
  - BiRefNet: 7GB
  - FLUX.1-dev: 16GB
  - Qwen2-VL: 10GB
  - **총합**: 33GB ❌ (24GB 초과)

#### 초기 시도와 실패
```python
# ❌ 나쁜 예: 모든 모델을 메모리에 유지
birefnet = load_birefnet()      # 7GB
flux = load_flux()              # 16GB
qwen = load_qwen()              # 10GB
# → Out of Memory Error!
```

### 5.2. JIT (Just-In-Time) 로딩/언로딩 전략

#### 핵심 아이디어
> "필요한 모델만 메모리에 로드하고, 사용 후 즉시 언로드"

#### 구현 방법

```python
def execute_pipeline_with_jit(job_id, params):
    """JIT 로딩/언로딩을 사용한 파이프라인"""
    
    # Stage 1-1: BiRefNet 누끼
    update_progress(job_id, 10, "BiRefNet 로딩...")
    birefnet = load_birefnet()  # 7GB 로드
    
    update_progress(job_id, 20, "배경 제거 중...")
    nobg_image = birefnet(product_image)
    
    # BiRefNet 즉시 언로드
    del birefnet
    torch.cuda.empty_cache()
    gc.collect()
    # → 7GB 해제됨
    
    # Stage 1-2: FLUX 배경 생성
    update_progress(job_id, 30, "FLUX 로딩...")
    flux = load_flux()  # 16GB 로드 (이제 가능!)
    
    update_progress(job_id, 40, "배경 생성 중...")
    background = flux(bg_prompt)
    
    # FLUX 즉시 언로드
    del flux
    torch.cuda.empty_cache()
    gc.collect()
    # → 16GB 해제됨
    
    # 합성 (CPU 작업)
    update_progress(job_id, 60, "이미지 합성...")
    step1_result = composite(nobg_image, background)
    
    # Stage 2-1: Qwen 분석
    if use_qwen:
        update_progress(job_id, 70, "Qwen 로딩...")
        qwen = load_qwen()  # 10GB 로드
        
        update_progress(job_id, 75, "이미지 분석 중...")
        analysis = qwen(step1_result)
        
        # Qwen 언로드
        del qwen
        torch.cuda.empty_cache()
        gc.collect()
        # → 10GB 해제됨
    
    # Stage 2-2: HTML 생성 및 렌더링
    update_progress(job_id, 85, "HTML 생성...")
    html = generate_html(text_content, analysis)
    
    update_progress(job_id, 95, "최종 합성...")
    final_result = render_and_composite(step1_result, html)
    
    return final_result
```

#### 메모리 사용 패턴

```mermaid
---
config:
  themeVariables:
    xyChart:
      plotColorPalette: "#4CAF50, #2196F3, #FF9800, #9C27B0"
---
xychart-beta
    title "GPU VRAM 사용량 변화 (JIT 로딩/언로딩)"
    x-axis ["초기", "BiRefNet 누끼", "언로드", "FLUX 배경생성", "언로드", "Qwen 분석", "언로드", "완료"]
    y-axis "VRAM (GB)" 0 --> 24
    bar [0, 7, 0, 16, 0, 10, 0, 0]
    line [24, 24, 24, 24, 24, 24, 24, 24]
```

**설명**:
- 파란색 막대: 각 단계별 VRAM 사용량
- 빨간색 선: 최대 VRAM 용량 (24GB)
- JIT 로딩으로 최대 16GB만 사용하여 24GB 한계 내에서 동작

### 5.3. 메모리 관리 유틸리티

#### GPU 모니터링

```python
def get_gpu_memory_info():
    """GPU 메모리 정보 조회"""
    if not torch.cuda.is_available():
        return None
    
    gpu_info = []
    for i in range(torch.cuda.device_count()):
        info = {
            "index": i,
            "name": torch.cuda.get_device_name(i),
            "vram_total_mb": torch.cuda.get_device_properties(i).total_memory / 1024**2,
            "vram_used_mb": torch.cuda.memory_allocated(i) / 1024**2,
            "vram_cached_mb": torch.cuda.memory_reserved(i) / 1024**2,
        }
        info["vram_free_mb"] = info["vram_total_mb"] - info["vram_used_mb"]
        info["vram_percent"] = (info["vram_used_mb"] / info["vram_total_mb"]) * 100
        gpu_info.append(info)
    
    return gpu_info
```

#### 메모리 클리어

```python
def clear_gpu_memory():
    """GPU 메모리 강제 정리"""
    import gc
    import torch
    
    # Python 가비지 컬렉션
    gc.collect()
    
    # PyTorch 캐시 비우기
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    
    # 메모리 정리 확인
    print(f"GPU Memory cleared: {get_gpu_memory_info()}")
```

#### 모델 로드/언로드 래퍼

```python
class ModelLoader:
    """모델 로딩/언로딩 관리 클래스"""
    
    def __init__(self):
        self.loaded_models = {}
    
    def load(self, model_name: str, loader_func: Callable):
        """모델 로드"""
        if model_name in self.loaded_models:
            return self.loaded_models[model_name]
        
        # 메모리 체크
        gpu_info = get_gpu_memory_info()
        if gpu_info and gpu_info[0]["vram_free_mb"] < 8000:  # 8GB 여유 필요
            raise MemoryError("Insufficient GPU memory")
        
        # 모델 로드
        model = loader_func()
        self.loaded_models[model_name] = model
        return model
    
    def unload(self, model_name: str):
        """모델 언로드"""
        if model_name not in self.loaded_models:
            return
        
        # 모델 삭제
        del self.loaded_models[model_name]
        
        # 메모리 정리
        clear_gpu_memory()
```

### 5.4. 최적화 결과

#### Before (모든 모델 동시 로드)
- ❌ Out of Memory Error
- 실행 불가능

#### After (JIT 로딩/언로딩)
- ✅ 24GB VRAM으로 모든 모델 실행 가능
- 최대 메모리 사용률: 95% (23GB)
- 안정적인 실행
- 약간의 속도 저하 (모델 로딩 시간)

#### 성능 비교

| 지표 | 이상적 (무한 VRAM) | 현실 (JIT) | 차이 |
|------|-------------------|-----------|------|
| 총 실행 시간 | 108초 | 133초 | +25초 (+23%) |
| 메모리 사용 | 33GB | 23GB | -10GB (-30%) |
| 안정성 | ✅ | ✅ | 동일 |
| 품질 | 100% | 100% | 동일 |

#### 결론
- 약간의 속도 저하는 있지만 실행 가능
- 품질 손실 없음
- 안정성 확보
- **트레이드오프**: 속도 vs 메모리 → 메모리 선택

---

## 6. Docker 배포 환경

### 6.1. Docker 아키텍처

#### 컨테이너 구성

```mermaid
graph TB
    subgraph Host["GCP VM Host<br/>Ubuntu 22.04 LTS<br/>NVIDIA L4 GPU 24GB"]
        GPU["/dev/nvidia0<br/>(GPU Device)"]
        
        subgraph Network["Docker Network: nanococoa-network<br/>(bridge driver)"]
            Backend["backend<br/>(8890)"]
            Homepage["homepage_generator<br/>(8891)"]
            DB["customer_db<br/>(5432)"]
            MCP["nanococoa_mcpserver<br/>(3000)"]
            AIServer["nanococoa_aiserver<br/>(8000)"]
            
            Backend --> MCP
            Backend --> AIServer
            Homepage --> AIServer
            MCP --> AIServer
            Backend --> DB
            Homepage --> MCP
        end
        
        AIServer -.->|GPU Access| GPU
    end
    
    style Host stroke:#f9f9f9,stroke-width:2px
    style Network stroke:#e3f2fd,stroke-width:2px
    style AIServer stroke:#fff3e0,stroke-width:2px
    style GPU stroke:#ffebee,stroke-width:2px
```

### 6.2. Docker Compose 설정

#### docker-compose.yml

```yaml
version: '3.8'

services:
  # PostgreSQL 데이터베이스
  customer_db:
    image: postgres:15
    container_name: customer_db
    environment:
      POSTGRES_USER: owner
      POSTGRES_PASSWORD: owner1234
      POSTGRES_DB: customer_db
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - nanococoa-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U owner"]
      interval: 10s
      timeout: 5s
      retries: 5

  # 메인 백엔드 서버
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: backend
    ports:
      - "8890:8890"
    environment:
      - DATABASE_URL=postgresql://owner:owner1234@customer_db:5432/customer_db
      - HOMEPAGE_GENERATOR_URL=http://homepage_generator:8891
      - NANOCOCOA_URL=http://nanococoa_aiserver:8000
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./backend:/app
      - ./backend/static/uploads:/app/static/uploads
      - ./backend/static/results:/app/static/results
    networks:
      - nanococoa-network
    depends_on:
      customer_db:
        condition: service_healthy
    restart: unless-stopped

  # 홈페이지 생성 서비스
  homepage_generator:
    build:
      context: ./homepage_generator
      dockerfile: Dockerfile
    container_name: homepage_generator
    ports:
      - "8891:8891"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - MCP_SERVER_URL=http://nanococoa_mcpserver:3000
    volumes:
      - ./homepage_generator:/app
      - ./homepage_generator/static/sites:/app/static/sites
    networks:
      - nanococoa-network
    restart: unless-stopped

  # AI 모델 서빙 서버
  nanococoa_aiserver:
    build:
      context: ./nanoCocoa_aiserver
      dockerfile: Dockerfile
    container_name: nanococoa_aiserver
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - HF_TOKEN=${HF_TOKEN}
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
    volumes:
      - /opt/huggingface:/root/.cache/huggingface
      - ./nanoCocoa_aiserver/static/uploads:/app/static/uploads
      - ./nanoCocoa_aiserver/static/results:/app/static/results
      - ./nanoCocoa_aiserver/logs:/app/logs
    networks:
      - nanococoa-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  # MCP 프로토콜 서버
  nanococoa_mcpserver:
    build:
      context: ./nanoCocoa_mcpserver
      dockerfile: Dockerfile
    container_name: nanococoa_mcpserver
    ports:
      - "3000:3000"
    environment:
      - MCP_TRANSPORT=sse
      - MCP_PORT=3000
      - AISERVER_BASE_URL=http://nanococoa-aiserver:8000
    networks:
      - nanococoa-network
    depends_on:
      - nanococoa_aiserver
    restart: unless-stopped

networks:
  nanococoa-network:
    driver: bridge

volumes:
  postgres_data:
```

### 6.3. Docker 볼륨 구조

#### 볼륨 아키텍처

```mermaid
graph TB
    subgraph Host["호스트 파일 시스템"]
        HostHF["/opt/huggingface<br/>(AI 모델 캐시)"]
        HostBackend["./backend/static<br/>(백엔드 정적 파일)"]
        HostHomepage["./homepage_generator/static/sites<br/>(생성된 홈페이지)"]
        HostAIServer["./nanoCocoa_aiserver<br/>(AI 서버 데이터)"]
    end
    
    subgraph Docker["Docker 볼륨"]
        DockerVolume["postgres_data<br/>(Named Volume)<br/>PostgreSQL 데이터"]
    end
    
    subgraph Containers["컨테이너"]
        subgraph DB["customer_db"]
            DBData["/var/lib/postgresql/data"]
        end
        
        subgraph Backend["backend"]
            BackendApp["/app"]
            BackendUploads["/app/static/uploads"]
            BackendResults["/app/static/results"]
        end
        
        subgraph Homepage["homepage_generator"]
            HomepageApp["/app"]
            HomepageSites["/app/static/sites"]
        end
        
        subgraph AIServer["nanococoa_aiserver"]
            AICache["/root/.cache/huggingface"]
            AIUploads["/app/static/uploads"]
            AIResults["/app/static/results"]
            AILogs["/app/logs"]
        end
    end
    
    DockerVolume --> DBData
    HostBackend --> BackendApp
    HostBackend --> BackendUploads
    HostBackend --> BackendResults
    HostHomepage --> HomepageApp
    HostHomepage --> HomepageSites
    HostHF --> AICache
    HostAIServer --> AIUploads
    HostAIServer --> AIResults
    HostAIServer --> AILogs
    
    style Host stroke:#4CAF50,stroke-width:2px
    style Docker stroke:#2196F3,stroke-width:2px
    style Containers stroke:#FF9800,stroke-width:2px
    style DB stroke:#9C27B0,stroke-width:2px
    style Backend stroke:#00BCD4,stroke-width:2px
    style Homepage stroke:#E91E63,stroke-width:2px
    style AIServer stroke:#FFC107,stroke-width:2px
```

#### 볼륨 유형 및 용도

**1. Named Volume (Docker 관리)**

| 볼륨명 | 마운트 위치 | 용도 | 특징 |
|--------|-------------|------|------|
| postgres_data | `/var/lib/postgresql/data` | PostgreSQL 데이터 저장 | Docker가 자동 관리, 백업 용이 |

**특징**:
- Docker가 관리하는 영구 스토리지
- 컨테이너 삭제 후에도 데이터 유지
- `docker volume ls`로 확인 가능
- 백업/복원 용이

**2. Bind Mount (호스트 디렉토리 마운트)**

**Backend 서비스**

| 호스트 경로 | 컨테이너 경로 | 용도 | 이유 |
|-------------|---------------|------|------|
| `./backend` | `/app` | 소스 코드 | 개발 중 실시간 반영 |
| `./backend/static/uploads` | `/app/static/uploads` | 업로드 이미지 | 영구 저장 |
| `./backend/static/results` | `/app/static/results` | 생성 결과물 | 영구 저장 |

**Homepage Generator 서비스**

| 호스트 경로 | 컨테이너 경로 | 용도 | 이유 |
|-------------|---------------|------|------|
| `./homepage_generator` | `/app` | 소스 코드 | 개발 중 실시간 반영 |
| `./homepage_generator/static/sites` | `/app/static/sites` | 생성된 홈페이지 | 영구 저장, 서빙 |

**nanoCocoa_aiserver 서비스**

| 호스트 경로 | 컨테이너 경로 | 용도 | 이유 |
|-------------|---------------|------|------|
| `/opt/huggingface` | `/root/.cache/huggingface` | AI 모델 캐시 | 모델 재다운로드 방지 (대용량) |
| `./nanoCocoa_aiserver/static/uploads` | `/app/static/uploads` | 업로드 이미지 | 영구 저장 |
| `./nanoCocoa_aiserver/static/results` | `/app/static/results` | 생성 결과물 | 영구 저장 |
| `./nanoCocoa_aiserver/logs` | `/app/logs` | 로그 파일 | 디버깅, 모니터링 |

#### 볼륨 사용 이유

**Named Volume을 사용하는 경우 (postgres_data)**
- **영구성**: 컨테이너 삭제 후에도 데이터 보존
- **독립성**: 호스트 파일 시스템과 독립적
- **백업 용이**: Docker 명령어로 간단히 백업/복원
- **이식성**: 다른 호스트로 쉽게 이동

```bash
# 볼륨 백업
docker run --rm \
  -v postgres_data:/data \
  -v $(pwd):/backup \
  ubuntu tar czf /backup/postgres_backup.tar.gz /data

# 볼륨 복원
docker run --rm \
  -v postgres_data:/data \
  -v $(pwd):/backup \
  ubuntu tar xzf /backup/postgres_backup.tar.gz -C /
```

**Bind Mount를 사용하는 경우**

1. **개발 편의성** (`./backend:/app`)
   - 소스 코드 수정 시 즉시 반영
   - 컨테이너 재빌드 불필요
   - Hot-reload 지원

2. **대용량 데이터** (`/opt/huggingface`)
   - AI 모델 파일: 10~20GB
   - 재다운로드 방지
   - 여러 컨테이너 간 공유 가능

3. **영구 저장** (`static/uploads`, `static/results`)
   - 사용자 업로드 이미지 보존
   - 생성된 결과물 보존
   - 컨테이너 재시작 후에도 접근 가능

4. **로그 분석** (`logs/`)
   - 호스트에서 직접 로그 확인
   - 로그 수집 시스템 연동 용이
   - 디버깅 편의성

#### 볼륨 데이터 흐름

```mermaid
sequenceDiagram
    participant User as 사용자
    participant Backend as Backend 컨테이너
    participant HostUploads as 호스트: ./backend/static/uploads
    participant AIServer as AI Server 컨테이너
    participant HostResults as 호스트: ./nanoCocoa_aiserver/static/results
    
    User->>Backend: 이미지 업로드
    Backend->>HostUploads: 파일 저장<br/>(Bind Mount)
    Note over HostUploads: 호스트에 영구 저장
    
    Backend->>AIServer: 이미지 생성 요청
    AIServer->>AIServer: AI 처리
    AIServer->>HostResults: 결과물 저장<br/>(Bind Mount)
    Note over HostResults: 호스트에 영구 저장
    
    AIServer-->>Backend: 결과 URL 반환
    Backend-->>User: 결과 표시
    
    Note over HostUploads,HostResults: 컨테이너 재시작 후에도<br/>데이터 유지됨
```

#### 볼륨 관리 명령어

**Named Volume 관리**

```bash
# 볼륨 목록 확인
docker volume ls

# 볼륨 상세 정보
docker volume inspect postgres_data

# 볼륨 삭제 (주의: 데이터 손실!)
docker volume rm postgres_data

# 사용하지 않는 볼륨 정리
docker volume prune
```

**Bind Mount 디렉토리 확인**

```bash
# 호스트에서 직접 확인
ls -lh /opt/huggingface
ls -lh ./backend/static/uploads
ls -lh ./nanoCocoa_aiserver/static/results

# 컨테이너 내부에서 확인
docker exec -it nanococoa_aiserver ls -lh /root/.cache/huggingface
docker exec -it backend ls -lh /app/static/uploads
```

**디스크 사용량 확인**

```bash
# Docker 전체 디스크 사용량
docker system df

# 볼륨별 사용량
docker system df -v

# 호스트 디렉토리 사용량
du -sh /opt/huggingface
du -sh ./backend/static/*
du -sh ./nanoCocoa_aiserver/static/*
```

#### 데이터 백업 전략

**1. PostgreSQL 데이터 백업**

```bash
# Named Volume 백업
docker run --rm \
  -v postgres_data:/data \
  -v $(pwd):/backup \
  ubuntu tar czf /backup/postgres_$(date +%Y%m%d).tar.gz /data

# 데이터베이스 덤프
docker exec customer_db pg_dump -U owner customer_db > backup_$(date +%Y%m%d).sql
```

**2. AI 모델 캐시 백업**

```bash
# Huggingface 캐시 백업 (대용량: 10~20GB)
tar czf huggingface_cache_$(date +%Y%m%d).tar.gz /opt/huggingface
```

**3. 사용자 데이터 백업**

```bash
# 업로드 이미지 및 결과물 백업
tar czf user_data_$(date +%Y%m%d).tar.gz \
  ./backend/static/uploads \
  ./backend/static/results \
  ./nanoCocoa_aiserver/static/uploads \
  ./nanoCocoa_aiserver/static/results
```

#### 볼륨 관련 문제 해결

**문제 1: 권한 오류 (Permission Denied)**

```bash
# 원인: 컨테이너와 호스트의 UID/GID 불일치
# 해결: 디렉토리 권한 변경
sudo chown -R 1000:1000 ./backend/static
sudo chown -R 1000:1000 ./nanoCocoa_aiserver/static
```

**문제 2: 디스크 용량 부족**

```bash
# 사용하지 않는 볼륨 정리
docker volume prune

# 오래된 이미지 정리
docker image prune -a

# 전체 시스템 정리
docker system prune -a --volumes
```

**문제 3: 볼륨 데이터 손실**

```bash
# Named Volume 데이터 확인
docker volume inspect postgres_data

# Bind Mount 경로 확인
docker inspect backend | grep -A 10 "Mounts"

# 백업에서 복원
tar xzf backup.tar.gz -C /target/path
```

### 6.5. Dockerfile 예시 (nanoCocoa_aiserver)

```dockerfile
# Base image with CUDA support
FROM nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3-pip \
    git \
    wget \
    fonts-nanum \
    fonts-nanum-coding \
    fonts-nanum-extra \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .

# Install Python dependencies
RUN pip3 install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

# Run application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]
```

### 6.6. 배포 절차

#### 1. 환경 변수 설정

```bash
# .env 파일 생성
cat > src/.env << EOF
# Database
POSTGRES_USER=owner
POSTGRES_PASSWORD=owner1234
POSTGRES_DB=customer_db

# External APIs
OPENAI_API_KEY=sk-proj-...
HF_TOKEN=hf_...

# Internal URLs (Docker service names)
DATABASE_URL=postgresql://owner:owner1234@customer_db:5432/customer_db
HOMEPAGE_GENERATOR_URL=http://homepage_generator:8891
NANOCOCOA_URL=http://nanococoa_aiserver:8000
EOF
```

#### 2. 빌드 및 실행

```bash
# src 디렉토리로 이동
cd src

# 빌드 및 실행 (포그라운드)
docker-compose up --build

# 빌드 및 실행 (백그라운드)
docker-compose up --build -d

# 로그 확인
docker-compose logs -f nanococoa_aiserver

# 특정 서비스만 재시작
docker-compose restart nanococoa_aiserver
```

#### 3. 상태 확인

```bash
# 컨테이너 상태 확인
docker-compose ps

# 네트워크 확인
docker network ls
docker network inspect nanococoa-network

# GPU 사용 확인
docker exec -it nanococoa_aiserver nvidia-smi

# 서비스 헬스 체크
curl http://localhost:8000/health
```

#### 4. 중지 및 정리

```bash
# 서비스 중지
docker-compose stop

# 서비스 중지 및 컨테이너 삭제
docker-compose down

# 볼륨까지 삭제
docker-compose down -v

# 이미지까지 삭제
docker-compose down --rmi all
```

### 6.7. Docker 이미지 관리

#### 이미지 목록 및 용도

**프로덕션 이미지 (Docker Hub: c0z0c/nanococoa)**

| 이미지명 | 태그 | 크기 (Disk) | 크기 (Content) | 용도 | 특징 |
|---------|------|-------------|----------------|------|------|
| c0z0c/nanococoa | ad_chat | 1.95GB | 484MB | 채팅 인터페이스 | Streamlit 기반 대화형 UI |
| c0z0c/nanococoa | aiserver | 24.6GB | 8.6GB | AI 모델 서빙 | GPU 지원, CUDA 12.1, PyTorch |
| c0z0c/nanococoa | backend | 1.39GB | 352MB | 웹 백엔드 | FastAPI, Jinja2, PostgreSQL 연동 |
| c0z0c/nanococoa | homepage_generator | 937MB | 203MB | 홈페이지 생성 | LangGraph, Multi-Agent |
| c0z0c/nanococoa | mcpserver | 607MB | 142MB | MCP 프로토콜 | SSE 통신, 8개 도구 제공 |

**베이스 이미지**

| 이미지명 | 버전 | 크기 (Disk) | 용도 |
|---------|------|-------------|------|
| nginx | alpine | 93.1MB | 리버스 프록시, 정적 파일 서빙 |
| postgres | 15 | 633MB | 데이터베이스 |

#### 이미지 크기 분석

```mermaid
---
config:
  themeVariables:
    xyChart:
      plotColorPalette: "#FF6B6B, #4ECDC4, #45B7D1, #FFA07A, #98D8C8, #F7DC6F, #BB8FCE"
---
xychart-beta
    title "Docker 이미지 크기 비교 (GB)"
    x-axis ["aiserver", "ad_chat", "backend", "homepage", "postgres", "mcpserver", "nginx"]
    y-axis "Size (GB)" 0 --> 25
    bar [24.6, 1.95, 1.39, 0.937, 0.633, 0.607, 0.093]
```

**aiserver 이미지가 큰 이유 (24.6GB)**
- CUDA 12.1 런타임: ~5GB
- PyTorch + Transformers: ~8GB
- AI 모델 의존성 (diffusers, optimum 등): ~6GB
- Python 패키지 (numpy, pillow, opencv 등): ~3GB
- 기타 시스템 라이브러리: ~2.6GB

#### 이미지 최적화 전략

**1. 멀티 스테이지 빌드**

```dockerfile
# 빌드 스테이지
FROM python:3.11-slim AS builder
WORKDIR /app
COPY requirements.txt .
RUN pip install --user --no-cache-dir -r requirements.txt

# 실행 스테이지
FROM python:3.11-slim
COPY --from=builder /root/.local /root/.local
COPY . .
ENV PATH=/root/.local/bin:$PATH
```

**2. 레이어 캐싱 활용**

```dockerfile
# ✅ 자주 변경되지 않는 것을 먼저
COPY requirements.txt .
RUN pip install -r requirements.txt

# ✅ 자주 변경되는 것을 나중에
COPY . .
```

**3. .dockerignore 활용**

```
__pycache__/
*.pyc
*.pyo
*.pyd
.git/
.gitignore
*.md
tests/
docs/
*.log
.venv/
venv/
```

#### 이미지 빌드 및 배포

**로컬 빌드**

```bash
# 개별 서비스 빌드
docker build -t c0z0c/nanococoa:backend ./backend
docker build -t c0z0c/nanococoa:aiserver ./nanoCocoa_aiserver

# 모든 서비스 빌드
docker-compose build

# 빌드 시 캐시 무시
docker-compose build --no-cache
```

**Docker Hub 배포**

```bash
# Docker Hub 로그인
docker login

# 이미지 태그 지정
docker tag local-image:latest c0z0c/nanococoa:backend

# 이미지 푸시
docker push c0z0c/nanococoa:backend
docker push c0z0c/nanococoa:aiserver
docker push c0z0c/nanococoa:homepage_generator
docker push c0z0c/nanococoa:mcpserver
docker push c0z0c/nanococoa:ad_chat

# 모든 태그 푸시
docker push c0z0c/nanococoa --all-tags
```

**프로덕션 배포 워크플로우**

```mermaid
graph LR
    Dev["로컬 개발"] --> Build["Docker Build"]
    Build --> Test["이미지 테스트"]
    Test --> Tag["버전 태그"]
    Tag --> Push["Docker Hub Push"]
    Push --> Pull["서버에서 Pull"]
    Pull --> Deploy["서비스 배포"]
    Deploy --> Health["헬스 체크"]
    
    style Dev stroke:#4CAF50,stroke-width:2px
    style Build stroke:#2196F3,stroke-width:2px
    style Push stroke:#FF9800,stroke-width:2px
    style Deploy stroke:#9C27B0,stroke-width:2px
```

#### 이미지 관리 명령어

**이미지 조회**

```bash
# 모든 이미지 조회
docker images

# 특정 이미지만 필터링
docker images c0z0c/nanococoa

# 디스크 사용량 확인
docker system df
docker system df -v

# 댕글링(dangling) 이미지 확인
docker images -f "dangling=true"
```

**이미지 정리**

```bash
# 사용하지 않는 이미지 삭제
docker image prune

# 모든 사용하지 않는 이미지 삭제
docker image prune -a

# 특정 이미지 삭제
docker rmi c0z0c/nanococoa:old_tag

# 모든 이미지 강제 삭제
docker rmi -f $(docker images -q)
```

**이미지 정보 확인**

```bash
# 이미지 상세 정보
docker inspect c0z0c/nanococoa:aiserver

# 이미지 히스토리 (레이어 확인)
docker history c0z0c/nanococoa:aiserver

# 이미지 레이어 분석 (dive 도구)
dive c0z0c/nanococoa:aiserver
```

#### 버전 관리 전략

**태그 명명 규칙**

```bash
# 환경별 태그
c0z0c/nanococoa:dev-backend        # 개발 환경
c0z0c/nanococoa:staging-backend    # 스테이징 환경
c0z0c/nanococoa:prod-backend       # 프로덕션 환경

# 버전별 태그
c0z0c/nanococoa:backend-v1.0.0     # 시맨틱 버저닝
c0z0c/nanococoa:backend-20260127   # 날짜 기반

# 기능별 태그
c0z0c/nanococoa:backend-flux2      # 특정 기능 포함
c0z0c/nanococoa:aiserver-quantized # 최적화 버전
```

**latest 태그 사용 주의**

```yaml
# ❌ 프로덕션에서 피해야 할 방법
services:
  backend:
    image: c0z0c/nanococoa:latest  # 버전 불명확

# ✅ 권장하는 방법
services:
  backend:
    image: c0z0c/nanococoa:backend-v1.0.0  # 명확한 버전
```

#### 이미지 보안

**취약점 스캔**

```bash
# Docker Scout (Docker Desktop 내장)
docker scout cves c0z0c/nanococoa:aiserver

# Trivy 스캔
trivy image c0z0c/nanococoa:aiserver

# Snyk 스캔
snyk container test c0z0c/nanococoa:aiserver
```

**보안 베스트 프랙티스**

1. **최소 권한 원칙**: root 사용자 피하기
2. **최신 베이스 이미지**: 정기적인 업데이트
3. **불필요한 패키지 제거**: 공격 표면 최소화
4. **시크릿 관리**: 이미지에 비밀키 포함 금지

```dockerfile
# ✅ 보안 강화 예시
FROM python:3.11-slim

# 비-root 사용자 생성
RUN useradd -m -u 1000 appuser

# 작업 디렉토리 설정
WORKDIR /app

# 의존성 설치
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# 애플리케이션 복사
COPY --chown=appuser:appuser . .

# 비-root 사용자로 전환
USER appuser

CMD ["python", "main.py"]
```

### 6.8. Docker 네트워크 설정

#### 내부 통신 vs 외부 통신

```python
# Backend 컨테이너 내부에서
# nanoCocoa_aiserver 호출 시

# ✅ 올바른 방법: 서비스명 사용 (내부 네트워크)
AISERVER_URL = "http://nanococoa_aiserver:8000"
response = requests.post(f"{AISERVER_URL}/generate", ...)

# ❌ 잘못된 방법: 호스트 IP 사용
AISERVER_URL = "http://34.44.205.198:8000"
# → 외부로 나갔다가 다시 들어오므로 비효율적
```

#### 포트 매핑

| 서비스 | 내부 포트 | 외부 포트 | 설명 |
|--------|-----------|-----------|------|
| backend | 8890 | 8890 | 사용자 대시보드 |
| homepage_generator | 8891 | 8891 | 홈페이지 생성 API |
| nanococoa_aiserver | 8000 | 8000 | AI 모델 API |
| nanococoa_mcpserver | 3000 | 3000 | MCP 프로토콜 |
| customer_db | 5432 | (내부만) | PostgreSQL |

---

## 7. API 설계

### 7.1. API 설계 원칙

#### RESTful API 설계
- **자원 중심**: URL은 명사 사용 (`/generate`, `/status`)
- **HTTP 메서드**: GET (조회), POST (생성), DELETE (삭제)
- **상태 코드**: 200 (성공), 400 (잘못된 요청), 500 (서버 오류)
- **JSON 응답**: 일관된 응답 형식

#### API 버전 관리
- 현재 버전: v1 (기본)
- 향후 확장: `/v2/generate` 등으로 버전 구분

### 7.2. 주요 API 엔드포인트

#### 7.2.1. POST /generate (이미지 생성)

**요청**
```json
{
  "start_step": 1,
  "product_image": "data:image/png;base64,iVBORw0KG...",
  "text_content": "명절 특가! 사과 한 박스 20,000원",
  "bg_prompt": "Korean traditional market, wooden table...",
  "html_style_prompt": "Bold gold text with shadow",
  "use_qwen_analysis": true,
  "use_llm_html": true,
  "text_position": "top",
  "strength": 0.6,
  "guidance_scale": 3.5,
  "test_mode": false
}
```

**응답 (즉시)**
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "pending",
  "message": "작업이 시작되었습니다."
}
```

#### 7.2.2. GET /status/{job_id} (상태 조회)

**응답 (진행 중)**
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "running",
  "progress_percent": 65,
  "current_step": "step2_html_rendering",
  "sub_step": "qwen_analysis",
  "message": "Qwen2-VL로 이미지 분석 중...",
  "elapsed_sec": 87.3,
  "eta_seconds": 45,
  "step_eta_seconds": 25,
  "system_metrics": {
    "cpu_percent": 52.1,
    "ram_used_gb": 14.2,
    "gpu_info": [
      {
        "index": 0,
        "name": "NVIDIA L4",
        "vram_used_mb": 18234,
        "vram_percent": 74.2,
        "utilization": 95
      }
    ]
  },
  "step1_result": "data:image/png;base64,iVBORw0KG...",
  "final_result": null
}
```

**응답 (완료)**
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "completed",
  "progress_percent": 100,
  "message": "생성이 완료되었습니다!",
  "elapsed_sec": 133.5,
  "step1_result": "data:image/png;base64,iVBORw0KG...",
  "final_result": "data:image/png;base64,iVBORw0KG..."
}
```

#### 7.2.3. POST /clip-score (CLIP Score 계산)

**요청**
```json
{
  "image_base64": "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAY...",
  "prompt": "사과가 그려져 있고 가격과 판매 장소가 적혀 있는 광고 포스터",
  "model_type": "koclip"
}
```

**응답**
```json
{
  "clip_score": 0.7324,
  "prompt": "사과가 그려져 있고 가격과 판매 장소가 적혀 있는 광고 포스터",
  "model_type": "koclip",
  "interpretation": "매우 높은 일치도 - 이미지가 텍스트 설명과 강하게 부합합니다."
}
```

### 7.3. 에러 처리

#### 에러 응답 형식

```json
{
  "error": {
    "code": "INVALID_IMAGE",
    "message": "이미지 형식이 잘못되었습니다.",
    "details": "Base64 디코딩에 실패했습니다.",
    "timestamp": "2026-01-27T10:30:45Z"
  }
}
```

#### 주요 에러 코드

| 코드 | HTTP Status | 설명 | 해결 방법 |
|------|-------------|------|-----------|
| INVALID_IMAGE | 400 | 이미지 형식 오류 | Base64 인코딩 확인 |
| INVALID_PROMPT | 400 | 프롬프트 누락 | 프롬프트 입력 |
| JOB_NOT_FOUND | 404 | 작업 ID 없음 | Job ID 확인 |
| GPU_OUT_OF_MEMORY | 500 | GPU 메모리 부족 | 재시도 또는 관리자 문의 |
| MODEL_LOAD_ERROR | 500 | 모델 로딩 실패 | 서버 재시작 |

---

## 8. 기술적 도전과 해결

### 8.1. GPU 메모리 부족 문제

**문제**
- NVIDIA L4 GPU 24GB VRAM
- 필요한 VRAM: 33GB (BiRefNet 7GB + FLUX 16GB + Qwen 10GB)

**시도한 방법**
1. ❌ 모델 양자화 (INT8): 품질 저하 심함
2. ❌ 모델 교체 (더 작은 모델): 기능 제한
3. ✅ **JIT 로딩/언로딩**: 성공!

**해결 방법**
- 필요한 모델만 순차적으로 로드
- 사용 후 즉시 언로드 및 캐시 클리어
- 약간의 속도 저하는 있지만 안정적

**결과**
- 24GB VRAM으로 모든 모델 실행 가능
- 메모리 사용률 최대 95%
- 안정성 확보

### 8.2. Docker 네트워크 통신 문제

**문제**
- 컨테이너 간 통신 불가
- `Connection refused` 에러 발생

**원인 분석**
- 서비스명과 호스트명 불일치
- 네트워크 드라이버 설정 오류
- 포트 바인딩 문제

**해결 방법**
```yaml
# docker-compose.yml
networks:
  nanococoa-network:
    driver: bridge

services:
  backend:
    networks:
      - nanococoa-network
    environment:
      # ✅ 올바른: 서비스명 사용
      - NANOCOCOA_URL=http://nanococoa_aiserver:8000
      # ❌ 잘못된: localhost 사용
      # - NANOCOCOA_URL=http://localhost:8000
```

**결과**
- 모든 컨테이너 간 통신 정상 작동
- 내부 네트워크로 빠른 통신

### 8.3. HTML 렌더링 한글 깨짐

**문제**
- Playwright로 HTML 렌더링 시 한글 깨짐
- 폰트가 로드되지 않음

**원인**
- Docker 컨테이너에 한글 폰트 미설치

**해결 방법**
```dockerfile
# Dockerfile
RUN apt-get update && apt-get install -y \
    fonts-nanum \
    fonts-nanum-coding \
    fonts-nanum-extra \
    && rm -rf /var/lib/apt/lists/*

# 폰트 캐시 업데이트
RUN fc-cache -f -v
```

**결과**
- 한글 텍스트 정상 렌더링
- 다양한 폰트 사용 가능

### 8.4. 이미지 생성 시간 과다

**문제**
- FLUX 모델 추론 시간: 80~90초
- 사용자 대기 시간 너무 김

**시도한 방법**
1. ❌ 모델 경량화: 품질 저하
2. ❌ Steps 줄이기: 품질 저하
3. ✅ **비동기 처리 + 폴링**: 성공!

**해결 방법**
- 멀티프로세싱으로 백그라운드 실행
- 폴링 방식으로 실시간 진행 상황 표시
- 중간 결과물 미리보기 제공

**결과**
- 사용자는 대기 중 진행 상황 확인 가능
- UX 크게 개선

### 8.5. 프롬프트 품질 문제

**문제**
- 초기 테스트에서 원하는 결과물 생성 실패
- 한글 텍스트가 영어로 나오거나 잘림

**원인**
- FLUX 모델의 토큰 제한 (77토큰)
- 프롬프트 구조 불명확

**해결 방법**
- 프롬프트 엔지니어링 최적화
- 중요 규칙 명시 ("100단어 이상", "한글로 작성")
- LLM을 활용한 프롬프트 자동 생성

**결과**
- 품질 급격히 향상
- 일관성 있는 결과물

---

## 9. 성과 및 향후 계획

### 9.1. 주요 성과

#### 기술적 성과
- ✅ 마이크로서비스 아키텍처 성공적 구축
- ✅ GPU 메모리 최적화 (JIT 로딩/언로딩)
- ✅ Docker 기반 배포 환경 구축
- ✅ 2단계 AI 파이프라인 구현
- ✅ REST API 및 MCP 프로토콜 제공

#### 시스템 성능
- 이미지 생성 시간: 평균 2~3분
- GPU 메모리 사용률: 최대 95%
- API 응답 시간: 100ms 이하
- 시스템 안정성: 99% 이상

#### 문서화
- 아키텍처 설계서
- API 명세서
- Docker 배포 가이드
- 설치 가이드
- 기술 문서 15개 이상

### 9.2. 개선이 필요한 부분

#### 성능 최적화
- 모델 양자화 적용 (INT8)
- TensorRT 최적화
- 배치 처리 구현

#### 기능 확장
- Inpainting (부분 수정)
- 영상 생성 (Veo2, Sora)
- 멀티 GPU 지원

#### 인프라 개선
- 자동 스케일링
- 로드 밸런싱
- 모니터링 대시보드

### 9.3. 향후 계획

#### 단기 (1~3개월)
- [x] Flux2 모델 통합 완료
- [ ] 양자화 적용 (Qwen + Flux 동시 로딩)
- [ ] TensorRT 최적화
- [ ] 배치 처리 구현

#### 중기 (3~6개월)
- [ ] 멀티 GPU 지원
- [ ] 자동 스케일링
- [ ] 모니터링 시스템 구축
- [ ] Fine-tuning

#### 장기 (6개월 이상)
- [ ] Kubernetes 마이그레이션
- [ ] Edge 배포 (경량 모델)
- [ ] 클라우드 네이티브 전환

### 9.4. 배운 점

#### 기술적 학습
- 마이크로서비스 아키텍처 설계 경험
- GPU 리소스 관리 및 최적화 노하우
- Docker 기반 배포 및 오케스트레이션
- REST API 및 MCP 프로토콜 구현

#### 문제 해결 능력
- 메모리 제약 극복 (JIT 로딩/언로딩)
- 네트워크 통신 문제 해결
- 비동기 처리 구현
- 에러 핸들링 및 복구 전략

#### 협업 및 소통
- 명확한 API 설계로 다른 팀원과 협업
- 기술 문서화의 중요성
- 코드 리뷰 및 피드백

---

## 결론

### 핵심 성과
- 확장 가능한 마이크로서비스 아키텍처 구축
- GPU 메모리 최적화로 24GB VRAM에서 모든 모델 실행
- Docker 기반 배포로 재현 가능한 환경 구성
- 2단계 AI 파이프라인으로 고품질 광고 이미지 생성

### 기술적 의의
- 실무 수준의 시스템 아키텍처 설계 경험
- AI 모델 서빙 및 최적화 노하우 축적
- 마이크로서비스 간 통신 및 오케스트레이션 경험

### 향후 발전 방향
- 단기: 성능 최적화 (양자화, TensorRT)
- 중기: 확장성 강화 (멀티 GPU, 자동 스케일링)
- 장기: 클라우드 네이티브 전환 (Kubernetes)

---

**감사합니다!**

다음 순서: 김민혁 / 이솔형 - AI 모델링 (텍스트 생성 / 이미지 생성)
