{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "175262e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6e3a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import io\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import random\n",
    "import spaces\n",
    "import torch\n",
    "from diffusers import Flux2Pipeline, Flux2Transformer2DModel, FluxInpaintPipeline\n",
    "from diffusers import BitsAndBytesConfig as DiffBitsAndBytesConfig\n",
    "import requests\n",
    "from PIL import Image\n",
    "import json\n",
    "import base64\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login, InferenceClient\n",
    "from helper_dev_utils import get_auto_logger\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import helper_plot_hangul\n",
    "import importlib\n",
    "import gc\n",
    "import logging\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from matplotlib.patches import Rectangle\n",
    "from typing import List, Optional, Tuple, Dict, Any\n",
    "from helper_plot_hangul import matplotlib_font_reset, matplotlib_font_get\n",
    "from helper_dev_utils import *\n",
    "from diffusers.utils import load_image\n",
    "logger = get_auto_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddaf28d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-17 07:44:00 I [ipykernel_launcher:6] - Hugging Face Token: hf ... LA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Logged in with HF_TOKEN from environment\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face ë¡œê·¸ì¸ (FLUX ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œ í•„ìš”)\n",
    "load_dotenv()\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ì—ì„œ í† í° ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ì§ì ‘ ì…ë ¥\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "logger.info(f\"Hugging Face Token: {hf_token[:2]} ... {hf_token[-2:]}\")\n",
    "\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"âœ“ Logged in with HF_TOKEN from environment\")\n",
    "else:\n",
    "    # ìˆ˜ë™ ë¡œê·¸ì¸ (í† í° ì…ë ¥ í•„ìš”)\n",
    "    login()\n",
    "    print(\"âœ“ Manual login completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e8dfe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-17 07:46:32 D [ipykernel_launcher:7] - /home/spai0433/codeit-ai-3team-ad-content/src/nanoCocoa_aiserver\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •\n",
    "project_root = Path.cwd().parent.parent / \"src\" / \"nanoCocoa_aiserver\"\n",
    "sys.path.insert(0, str(project_root))\n",
    "logger.debug(project_root)\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "from PIL import Image\n",
    "from typing import Optional\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "from config import TORCH_DTYPE\n",
    "from services.monitor import flush_gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aec84f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import AutoModelForImageSegmentation\n",
    "from config import DEVICE, MODEL_IDS\n",
    "from utils import flush_gpu\n",
    "\n",
    "# timm ë¼ì´ë¸ŒëŸ¬ë¦¬ deprecation ê²½ê³  ì–µì œ\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"timm\")\n",
    "\n",
    "class SegmentationModel:\n",
    "    \"\"\"\n",
    "    BiRefNetì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜(ë°°ê²½ ì œê±°)ì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.device = DEVICE\n",
    "\n",
    "    def run(self, image: Image.Image) -> tuple[Image.Image, Image.Image]:\n",
    "        \"\"\"\n",
    "        ì´ë¯¸ì§€ì˜ ë°°ê²½ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        Args:\n",
    "            image (Image.Image): ì…ë ¥ ì´ë¯¸ì§€\n",
    "            \n",
    "        Returns:\n",
    "            tuple[Image.Image, Image.Image]: (ë°°ê²½ ì œê±°ëœ ì´ë¯¸ì§€, ë§ˆìŠ¤í¬)\n",
    "        \"\"\"\n",
    "        logger.debug(\"[Engine] Loading BiRefNet... (BiRefNet ëª¨ë¸ ë¡œë”© ì¤‘)\")\n",
    "        flush_gpu()\n",
    "        \n",
    "        model = AutoModelForImageSegmentation.from_pretrained(\n",
    "            MODEL_IDS[\"SEG\"], trust_remote_code=True\n",
    "        ).to(self.device).eval()\n",
    "\n",
    "        W, H = image.size\n",
    "        # ê³ í•´ìƒë„ ì²˜ë¦¬ë¥¼ ìœ„í•´ ë¦¬ì‚¬ì´ì¦ˆ (í•„ìš” ì‹œ ì¡°ì • ê°€ëŠ¥)\n",
    "        img_resized = image.resize((1024, 1024), Image.LANCZOS)\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        input_tensor = transform(img_resized).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = model(input_tensor)[-1].sigmoid().cpu()\n",
    "        \n",
    "        pred = preds[0].squeeze()\n",
    "        mask = transforms.ToPILImage()(pred).resize((W, H), Image.LANCZOS)\n",
    "        \n",
    "        # ë§ˆìŠ¤í¬ ì´ì§„í™” (Thresholding)\n",
    "        mask = mask.point(lambda x: 255 if x > 128 else 0)\n",
    "        \n",
    "        # ë§ˆìŠ¤í¬ ë¸”ëŸ¬ ì²˜ë¦¬ (ì™¸ê³½ì„  ë¶€ë“œëŸ½ê²Œ)\n",
    "        from PIL import ImageFilter\n",
    "        mask = mask.filter(ImageFilter.GaussianBlur(radius=5))\n",
    "        \n",
    "        result = image.copy()\n",
    "        result.putalpha(mask)\n",
    "        \n",
    "        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬\n",
    "        del model, input_tensor\n",
    "        flush_gpu()\n",
    "        \n",
    "        return result, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483f2199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ê³µê°„ ë¶„ì„ ëª¨ë“ˆ: Qwen2-VLì„ ì‚¬ìš©í•œ ìµœì ì˜ ê°ì²´ ë°°ì¹˜ ìœ„ì¹˜ íƒì§€\n",
    "Spatial Analysis Module: Detect optimal object placement using Qwen2-VL\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from typing import Union, Dict, List, Tuple\n",
    "import re\n",
    "\n",
    "from utils import flush_gpu\n",
    "\n",
    "\n",
    "class SpatialAnalyzer:\n",
    "    \"\"\"\n",
    "    Qwen2-VLì„ ì‚¬ìš©í•˜ì—¬ ë°°ê²½ ì´ë¯¸ì§€ì—ì„œ ìµœì ì˜ ê°ì²´ ë°°ì¹˜ ìœ„ì¹˜ë¥¼ ì°¾ëŠ” í´ë˜ìŠ¤\n",
    "\n",
    "    ì´ í´ë˜ìŠ¤ëŠ” Vision-Language ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í‘œë©´ì„ ê°ì§€í•˜ê³ ,\n",
    "    ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ê²°ì •í•˜ë©°, ê°ì²´ í•©ì„±ì„ ìœ„í•œ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Attributes:\n",
    "        model_name (str): ì‚¬ìš©í•  HuggingFace ëª¨ë¸ ì´ë¦„\n",
    "        device (str): ëª¨ë¸ì„ ì‹¤í–‰í•  ë””ë°”ì´ìŠ¤\n",
    "        model: Qwen2-VL ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤\n",
    "        processor: ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í”„ë¡œì„¸ì„œ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name: str = \"Qwen/Qwen2-VL-7B-Instruct\", device: str = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        SpatialAnalyzer ì´ˆê¸°í™”\n",
    "\n",
    "        Args:\n",
    "            model_name: HuggingFace ëª¨ë¸ ì‹ë³„ì (ê¸°ë³¸ê°’: Qwen2-VL-7B-Instruct)\n",
    "            device: ëª¨ë¸ ì‹¤í–‰ ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cpu', ê¸°ë³¸ê°’: ìë™ ê°ì§€)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "\n",
    "        print(f\"ğŸ”§ SpatialAnalyzer ì´ˆê¸°í™”: {model_name}\")\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Qwen2-VL ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n",
    "        if self.model is None:\n",
    "            print(f\"  Qwen2-VL ëª¨ë¸ì„ {self.device}ì— ë¡œë“œ ì¤‘...\")\n",
    "\n",
    "            # Vision-Language ëª¨ë¸ ë¡œë“œ\n",
    "            self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.bfloat16,  # ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "                device_map=\"auto\",  # ìë™ ë””ë°”ì´ìŠ¤ ë°°ì¹˜\n",
    "            )\n",
    "\n",
    "            # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í”„ë¡œì„¸ì„œ ë¡œë“œ\n",
    "            self.processor = AutoProcessor.from_pretrained(self.model_name)\n",
    "\n",
    "            print(f\"  âœ“ Qwen2-VL ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "    def _unload_model(self):\n",
    "        \"\"\"VRAM í™•ë³´ë¥¼ ìœ„í•´ ëª¨ë¸ì„ ì–¸ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n",
    "        if self.model is not None:\n",
    "            print(\"  Qwen2-VL ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...\")\n",
    "            # device_map=\"auto\"ë¡œ ë¡œë“œëœ ëª¨ë¸ì€ .to(\"cpu\") ì‚¬ìš© ë¶ˆê°€\n",
    "            del self.model\n",
    "            del self.processor\n",
    "            self.model = None\n",
    "            self.processor = None\n",
    "            flush_gpu()  # GPU ìºì‹œ ì •ë¦¬\n",
    "\n",
    "    def detect_surface(\n",
    "        self,\n",
    "        image: Union[Image.Image, str],\n",
    "        query: str = \"Find the flat surface where I can place an object. Return the bounding box coordinates.\",\n",
    "        auto_unload: bool = True,\n",
    "    ) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        ê°ì²´ ë°°ì¹˜ë¥¼ ìœ„í•œ ìµœì ì˜ í‘œë©´ì„ íƒì§€í•©ë‹ˆë‹¤.\n",
    "\n",
    "        Vision-Language ëª¨ë¸ì— ì´ë¯¸ì§€ì™€ ì§ˆë¬¸ì„ ì œê³µí•˜ì—¬\n",
    "        ê°ì²´ë¥¼ ë†“ì„ ìˆ˜ ìˆëŠ” ìµœì ì˜ ìœ„ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
    "\n",
    "        Args:\n",
    "            image: PIL Image ê°ì²´ ë˜ëŠ” ì´ë¯¸ì§€ ê²½ë¡œ\n",
    "            query: VL ëª¨ë¸ì— ë¬¼ì–´ë³¼ ì§ˆë¬¸ (ìœ„ì¹˜ íƒì§€ ìš”ì²­)\n",
    "\n",
    "        Returns:\n",
    "            ë‹¤ìŒì„ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬:\n",
    "                - 'bbox': [x1, y1, x2, y2] ì •ê·œí™”ëœ ì¢Œí‘œ (0-1000 ë²”ìœ„)\n",
    "                - 'text': ëª¨ë¸ì˜ ì „ì²´ ì‘ë‹µ í…ìŠ¤íŠ¸\n",
    "                - 'image_size': ì…ë ¥ ì´ë¯¸ì§€ì˜ (width, height)\n",
    "\n",
    "        Example:\n",
    "            >>> analyzer = SpatialAnalyzer()\n",
    "            >>> result = analyzer.detect_surface(\n",
    "            ...     bg_image,\n",
    "            ...     \"ë§¥ì£¼ë³‘ì„ ë†“ì„ í…Œì´ë¸” ì¤‘ì•™ì„ ì°¾ì•„ì£¼ì„¸ìš”\"\n",
    "            ... )\n",
    "            >>> bbox = result['bbox']  # [x1, y1, x2, y2]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # ëª¨ë¸ ë¡œë“œ (í•„ìš”ì‹œ)\n",
    "            self._load_model()\n",
    "\n",
    "            # ê²½ë¡œê°€ ì œê³µëœ ê²½ìš° ì´ë¯¸ì§€ ë¡œë“œ\n",
    "            if isinstance(image, str):\n",
    "                from .utils import load_image\n",
    "\n",
    "                image = load_image(image)\n",
    "\n",
    "            image_size = image.size  # (width, height)\n",
    "\n",
    "            print(f\"  ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ({image_size[0]}x{image_size[1]})...\")\n",
    "            print(f\"  ì§ˆë¬¸: {query}\")\n",
    "\n",
    "            # ëª¨ë¸ì„ ìœ„í•œ ë©”ì‹œì§€ ì¤€ë¹„ (ë©€í‹°ëª¨ë‹¬ ì…ë ¥)\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"image\": image,  # ì´ë¯¸ì§€ ì…ë ¥\n",
    "                        },\n",
    "                        {\"type\": \"text\", \"text\": query},  # í…ìŠ¤íŠ¸ ì§ˆë¬¸\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            # ì…ë ¥ ì „ì²˜ë¦¬\n",
    "            text = self.processor.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "            inputs = self.processor(\n",
    "                text=[text],\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            inputs = inputs.to(self.device)\n",
    "\n",
    "            # ì‘ë‹µ ìƒì„± (ì¶”ë¡ )\n",
    "            with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”\n",
    "                generated_ids = self.model.generate(\n",
    "                    **inputs, max_new_tokens=256  # ìµœëŒ€ ì‘ë‹µ ê¸¸ì´\n",
    "                )\n",
    "\n",
    "            # ì…ë ¥ í† í° ì œê±° (ìƒì„±ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ)\n",
    "            generated_ids_trimmed = [\n",
    "                out_ids[len(in_ids) :]\n",
    "                for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "\n",
    "            # ì‘ë‹µ ë””ì½”ë”© (í† í° -> í…ìŠ¤íŠ¸)\n",
    "            output_text = self.processor.batch_decode(\n",
    "                generated_ids_trimmed,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=False,\n",
    "            )[0]\n",
    "\n",
    "            print(f\"  ëª¨ë¸ ì‘ë‹µ: {output_text[:100]}...\")\n",
    "\n",
    "            # ì‘ë‹µì—ì„œ ë°”ìš´ë”© ë°•ìŠ¤ íŒŒì‹±\n",
    "            bbox = self._parse_bbox(output_text)\n",
    "\n",
    "            result = {\"bbox\": bbox, \"text\": output_text, \"image_size\": image_size}\n",
    "\n",
    "            print(f\"  âœ“ í‘œë©´ íƒì§€ ì™„ë£Œ: {bbox}\")\n",
    "\n",
    "            return result\n",
    "\n",
    "        finally:\n",
    "            # VRAM í™•ë³´ë¥¼ ìœ„í•´ í•­ìƒ ëª¨ë¸ ì–¸ë¡œë“œ\n",
    "            if auto_unload:\n",
    "                self._unload_model()\n",
    "\n",
    "    def _parse_bbox(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        ëª¨ë¸ ì¶œë ¥ì—ì„œ ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.\n",
    "\n",
    "        Qwen-VLì€ ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ bboxë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:\n",
    "        - í˜•ì‹ 1: <|box_start|>(x1,y1),(x2,y2)<|box_end|>\n",
    "        - í˜•ì‹ 2: í…ìŠ¤íŠ¸ ë‚´ì˜ ìˆ«ì ë‚˜ì—´\n",
    "\n",
    "        Returns:\n",
    "            ì •ê·œí™”ëœ ì¢Œí‘œì˜ [x1, y1, x2, y2] (0-1000 ë²”ìœ„)\n",
    "        \"\"\"\n",
    "        # íŒ¨í„´ 1: <|box_start|>(x1,y1),(x2,y2)<|box_end|> í˜•íƒœ ì°¾ê¸°\n",
    "        box_pattern = r\"<\\|box_start\\|\\>\\((\\d+),(\\d+)\\),\\((\\d+),(\\d+)\\)<\\|box_end\\|\\>\"\n",
    "        match = re.search(box_pattern, text)\n",
    "\n",
    "        if match:\n",
    "            return [\n",
    "                int(match.group(1)),\n",
    "                int(match.group(2)),\n",
    "                int(match.group(3)),\n",
    "                int(match.group(4)),\n",
    "            ]\n",
    "\n",
    "        # íŒ¨í„´ 2: ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ 4ê°œì˜ ìˆ«ì ì°¾ê¸°\n",
    "        numbers = re.findall(r\"\\b\\d+\\b\", text)\n",
    "        if len(numbers) >= 4:\n",
    "            # ì²˜ìŒ 4ê°œ ìˆ«ìë¥¼ bboxë¡œ ì‚¬ìš©\n",
    "            return [int(numbers[0]), int(numbers[1]), int(numbers[2]), int(numbers[3])]\n",
    "\n",
    "        # ê¸°ë³¸ê°’: íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¤‘ì•™ ì˜ì—­ ì‚¬ìš©\n",
    "        print(\"  âš  bbox íŒŒì‹± ì‹¤íŒ¨, ì¤‘ì•™ ì˜ì—­ ì‚¬ìš©\")\n",
    "        return [400, 400, 600, 600]  # ì¤‘ì•™ ì˜ì—­ (ì •ê·œí™” 0-1000)\n",
    "\n",
    "    def create_mask(\n",
    "        self,\n",
    "        image_size: Tuple[int, int],\n",
    "        bbox: List[int],\n",
    "        mask_color: int = 255,\n",
    "        background_color: int = 0,\n",
    "    ) -> Image.Image:\n",
    "        \"\"\"\n",
    "        ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œë¡œë¶€í„° ì´ì§„ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "        Args:\n",
    "            image_size: ëŒ€ìƒ ì´ë¯¸ì§€ì˜ (width, height)\n",
    "            bbox: ì •ê·œí™”ëœ ì¢Œí‘œì˜ [x1, y1, x2, y2] (0-1000 ë²”ìœ„)\n",
    "            mask_color: ë§ˆìŠ¤í¬ ì˜ì—­ì˜ ìƒ‰ìƒ (ê¸°ë³¸ê°’: 255 = í°ìƒ‰)\n",
    "            background_color: ë°°ê²½ ìƒ‰ìƒ (ê¸°ë³¸ê°’: 0 = ê²€ì •)\n",
    "\n",
    "        Returns:\n",
    "            ì´ì§„ ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ ('L' ëª¨ë“œì˜ PIL.Image)\n",
    "\n",
    "        Example:\n",
    "            >>> mask = analyzer.create_mask((1024, 1024), [400, 400, 600, 600])\n",
    "        \"\"\"\n",
    "        width, height = image_size\n",
    "\n",
    "        # ì •ê·œí™”ëœ ì¢Œí‘œ(0-1000)ë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜\n",
    "        x1 = int(bbox[0] * width / 1000)\n",
    "        y1 = int(bbox[1] * height / 1000)\n",
    "        x2 = int(bbox[2] * width / 1000)\n",
    "        y2 = int(bbox[3] * height / 1000)\n",
    "\n",
    "        # ê²€ì€ ë°°ê²½ ìƒì„±\n",
    "        mask = Image.new(\"L\", image_size, background_color)\n",
    "        draw = ImageDraw.Draw(mask)\n",
    "\n",
    "        # ë§ˆìŠ¤í¬ ì˜ì—­ì— í°ìƒ‰ ì‚¬ê°í˜• ê·¸ë¦¬ê¸°\n",
    "        draw.rectangle([x1, y1, x2, y2], fill=mask_color)\n",
    "\n",
    "        print(\n",
    "            f\"  âœ“ ë§ˆìŠ¤í¬ ìƒì„± ì™„ë£Œ: {image_size[0]}x{image_size[1]}, \"\n",
    "            f\"ì˜ì—­: ({x1},{y1})-({x2},{y2})\"\n",
    "        )\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def visualize_bbox(\n",
    "        self, image: Image.Image, bbox: List[int], color: str = \"red\", width: int = 3\n",
    "    ) -> Image.Image:\n",
    "        \"\"\"\n",
    "        ì´ë¯¸ì§€ì— ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "        Args:\n",
    "            image: ì…ë ¥ PIL Image\n",
    "            bbox: ì •ê·œí™”ëœ ì¢Œí‘œì˜ [x1, y1, x2, y2] (0-1000 ë²”ìœ„)\n",
    "            color: ë°”ìš´ë”© ë°•ìŠ¤ ìƒ‰ìƒ\n",
    "            width: ì„  ë‘ê»˜\n",
    "\n",
    "        Returns:\n",
    "            ë°”ìš´ë”© ë°•ìŠ¤ê°€ ê·¸ë ¤ì§„ ì´ë¯¸ì§€\n",
    "\n",
    "        Example:\n",
    "            >>> bbox_img = analyzer.visualize_bbox(bg_image, [400, 400, 600, 600])\n",
    "        \"\"\"\n",
    "        img_copy = image.copy()\n",
    "        draw = ImageDraw.Draw(img_copy)\n",
    "\n",
    "        # ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜\n",
    "        img_width, img_height = image.size\n",
    "        x1 = int(bbox[0] * img_width / 1000)\n",
    "        y1 = int(bbox[1] * img_height / 1000)\n",
    "        x2 = int(bbox[2] * img_width / 1000)\n",
    "        y2 = int(bbox[3] * img_height / 1000)\n",
    "\n",
    "        # ì‚¬ê°í˜• ê·¸ë¦¬ê¸°\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=color, width=width)\n",
    "\n",
    "        return img_copy\n",
    "\n",
    "    def generate_flux_prompt(\n",
    "        self,\n",
    "        image: Union[Image.Image, str],\n",
    "        query: str = None,\n",
    "        detail_level: str = \"detailed\",\n",
    "        language: str = \"en\",\n",
    "        auto_unload: bool = True,\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ FLUX.1ìš© ìƒì„¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "        Args:\n",
    "            image: PIL Image ê°ì²´ ë˜ëŠ” ì´ë¯¸ì§€ ê²½ë¡œ\n",
    "            query: ì»¤ìŠ¤í…€ ì§ˆë¬¸ (Noneì´ë©´ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)\n",
    "            detail_level: 'simple', 'detailed', 'ultra' ì¤‘ ì„ íƒ\n",
    "            language: 'en' (ì˜ì–´) ë˜ëŠ” 'ko' (í•œêµ­ì–´)\n",
    "\n",
    "        Returns:\n",
    "            ë”•ì…”ë„ˆë¦¬:\n",
    "                - 'prompt': FLUX.1ìš© ìƒì„± í”„ë¡¬í”„íŠ¸\n",
    "                - 'raw_response': ëª¨ë¸ì˜ ì›ë³¸ ì‘ë‹µ\n",
    "                - 'image_size': ì…ë ¥ ì´ë¯¸ì§€ì˜ (width, height)\n",
    "\n",
    "        Example:\n",
    "            >>> result = analyzer.generate_flux_prompt(image, detail_level=\"ultra\")\n",
    "            >>> flux_prompt = result['prompt']\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._load_model()\n",
    "\n",
    "            # ê²½ë¡œê°€ ì œê³µëœ ê²½ìš° ì´ë¯¸ì§€ ë¡œë“œ\n",
    "            if isinstance(image, str):\n",
    "                image = Image.open(image).convert(\"RGB\")\n",
    "\n",
    "            image_size = image.size\n",
    "\n",
    "            # detail_levelì— ë”°ë¥¸ ê¸°ë³¸ ì§ˆë¬¸ ì„¤ì •\n",
    "            if query is None:\n",
    "                detail_prompts = {\n",
    "                    \"simple\": (\n",
    "                        \"Describe this image briefly for text-to-image generation.\"\n",
    "                        if language == \"en\"\n",
    "                        else \"ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•´ ì´ ì´ë¯¸ì§€ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
    "                    ),\n",
    "                    \"detailed\": (\n",
    "                        \"Analyze this image in detail for FLUX.1 text-to-image generation. \"\n",
    "                        \"Include: main subject, composition, colors, lighting, mood, style, \"\n",
    "                        \"and any important visual elements. Format as a single descriptive prompt.\"\n",
    "                        if language == \"en\"\n",
    "                        else \"FLUX.1 ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•´ ì´ ì´ë¯¸ì§€ë¥¼ ìƒì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”. \"\n",
    "                        \"í¬í•¨ ì‚¬í•­: ì£¼ìš” í”¼ì‚¬ì²´, êµ¬ë„, ìƒ‰ê°, ì¡°ëª…, ë¶„ìœ„ê¸°, ìŠ¤íƒ€ì¼, ì¤‘ìš”í•œ ì‹œê°ì  ìš”ì†Œ. \"\n",
    "                        \"í•˜ë‚˜ì˜ ì„¤ëª… í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.\"\n",
    "                    ),\n",
    "                    \"ultra\": (\n",
    "                            # [English] - ì–´ì¢… íŒë³„ ë¡œì§(Forensic Taxonomy)ì„ ì¶”ê°€í•˜ì—¬ ì •í™•í•œ í˜•íƒœ(Shape)ë¥¼ ìœ ë„í•©ë‹ˆë‹¤.\n",
    "                            \"Act as a 'Forensic Seafood Taxonomist & Visual Tagger'. \"\n",
    "                            \"Your goal is to identify the EXACT SPECIES based on visual anatomy (tail shape, skin pattern) \"\n",
    "                            \"to ensure FLUX.1 draws the correct skeletal structure. \"\n",
    "                            \n",
    "                            \"Analyze the image and output a comma-separated tag list following this priority logic: \"\n",
    "                            \n",
    "                            \"1. [SPECIES IDENTIFICATION] (CRITICAL): \"\n",
    "                            \"   - Look at the Tail & Body: \"\n",
    "                            \"     * Forked/V-shape tail + Golden skin = Tag: 'dried young pollack (nogari), dried alaska pollock'. \"\n",
    "                            \"     * Round/Oval pressed shape = Tag: 'dried filefish fillet (jwipo)'. \"\n",
    "                            \"     * Tentacles/Triangular head = Tag: 'dried squid, cuttlefish'. \"\n",
    "                            \"     * Silver skin + Spiky fins = Tag: 'dried gizzard shad'. \"\n",
    "                            \"   - If unsure, use safe fallback: 'roasted dried fish jerky'. \"\n",
    "                            \n",
    "                            \"2. [PHYSICAL STATE] (Must modify species): \"\n",
    "                            \"   - Tag: 'headless torso', 'flattened body', 'shriveled skin', 'stiff texture'. \"\n",
    "                            \"   - Constraint: 'no fresh scales', 'no slime', 'no eyes'. \"\n",
    "                            \n",
    "                            \"3. [TEXTURE & COLOR]: \"\n",
    "                            \"   - Tag: 'matte parchment-like finish', 'fibrous meat', 'golden-brown toasted', 'char marks'. \"\n",
    "                            \n",
    "                            \"4. [PLATING & CONTEXT]: \"\n",
    "                            \"   - Tag: 'rectangular plate', 'mayonnaise dip with chili', 'roasted peanuts', 'pub lighting'. \"\n",
    "                            \n",
    "                            \"5. [STYLE]: \"\n",
    "                            \"   - Tag: 'macro food photography', 'realistic texture', '8k', 'top-down view'. \"\n",
    "                            \n",
    "                            \"Final Output: A single list of tags starting with the identified species.\"\n",
    "                            if language == \"en\"\n",
    "                            \n",
    "                            # [Korean] - í•œê¸€ ì§€ì¹¨\n",
    "                            else \"FLUX.1ì„ ìœ„í•œ 'í•´ì‚°ë¬¼ ë²•ì˜í•™ ë¶„ì„ê°€ ë° íƒœê±°'ë¡œ í–‰ë™í•˜ì„¸ìš”. \"\n",
    "                            \"ì‹œê°ì  í•´ë¶€í•™(ê¼¬ë¦¬ ëª¨ì–‘, ê»ì§ˆ íŒ¨í„´)ì„ ë¶„ì„í•˜ì—¬ 'ì •í™•í•œ ì–´ì¢…(Species)'ì„ ì‹ë³„í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤. \"\n",
    "                            \"ê·¸ë˜ì•¼ AIê°€ ë¼ˆëŒ€ì™€ í˜•íƒœë¥¼ ì •í™•íˆ ê·¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \"\n",
    "                            \n",
    "                            \"ë‹¤ìŒ ìš°ì„ ìˆœìœ„ ë¡œì§ì— ë”°ë¼ ë¶„ì„í•˜ê³  ì‰¼í‘œë¡œ êµ¬ë¶„ëœ íƒœê·¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•˜ì„¸ìš”: \"\n",
    "                            \n",
    "                            \"1. [ì–´ì¢… ì •ë°€ íŒë³„] (í•µì‹¬): \"\n",
    "                            \"   - ê¼¬ë¦¬ì™€ ëª¸í†µì„ ê´€ì°°í•˜ì„¸ìš”: \"\n",
    "                            \"     * ê°ˆë¼ì§„ Vì ê¼¬ë¦¬ + í™©ê¸ˆë¹› ê»ì§ˆ = íƒœê·¸: 'dried young pollack (nogari), dried alaska pollock' (ë…¸ê°€ë¦¬/ëª…íƒœ). \"\n",
    "                            \"     * ë‘¥ê¸€ê³  ë‚©ì‘í•œ ì••ì¶• í˜•íƒœ = íƒœê·¸: 'dried filefish fillet (jwipo)' (ì¥í¬). \"\n",
    "                            \"     * ë‹¤ë¦¬/ì‚¼ê°í˜• ë¨¸ë¦¬ = íƒœê·¸: 'dried squid' (ì˜¤ì§•ì–´). \"\n",
    "                            \"     * ì€ìƒ‰ ê»ì§ˆ + ë¾°ì¡±í•œ ì§€ëŠëŸ¬ë¯¸ = íƒœê·¸: 'dried gizzard shad' (ì „ì–´). \"\n",
    "                            \"   - í™•ì‹ ì´ ì—†ìœ¼ë©´ ì•ˆì „í•œ íƒœê·¸ ì‚¬ìš©: 'roasted dried fish jerky' (êµ¬ìš´ ê±´ì–´ë¬¼). \"\n",
    "                            \n",
    "                            \"2. [ë¬¼ë¦¬ì  ìƒíƒœ] (ì–´ì¢… ìˆ˜ì‹): \"\n",
    "                            \"   - íƒœê·¸: 'ë¨¸ë¦¬ ì—†ëŠ” ëª¸í†µ(headless)', 'ë‚©ì‘í•´ì§„ ëª¸ì²´', 'ìª¼ê·¸ë¼ë“ ', 'ë»£ë»£í•œ ì§ˆê°'. \"\n",
    "                            \"   - ì œì•½: 'ì‹±ì‹±í•œ ë¹„ëŠ˜ ì—†ìŒ', 'ì ì•¡ ì—†ìŒ', 'ëˆˆì•Œ ì—†ìŒ'. \"\n",
    "                            \n",
    "                            \"3. [ì§ˆê° ë° ìƒ‰ìƒ]: \"\n",
    "                            \"   - íƒœê·¸: 'ë§¤íŠ¸í•œ ì–‘í”¼ì§€ ê°™ì€ ë§ˆê°', 'ì„¬ìœ ì§ˆ ì‚´ê²°', 'í™©ê¸ˆë¹› ê°ˆìƒ‰ êµ¬ì´', 'ê·¸ì„ë¦° ìêµ­'. \"\n",
    "                            \n",
    "                            \"4. [í”Œë ˆì´íŒ… ë° ë§¥ë½]: \"\n",
    "                            \"   - íƒœê·¸: 'ì§ì‚¬ê°í˜• ì ‘ì‹œ', 'ê³ ì¶” ë§ˆìš”ë„¤ì¦ˆ ì†ŒìŠ¤', 'êµ¬ìš´ ë•…ì½©', 'ìˆ ì§‘ ì¡°ëª…'. \"\n",
    "                            \n",
    "                            \"5. [ìŠ¤íƒ€ì¼]: \"\n",
    "                            \"   - íƒœê·¸: 'ë§¤í¬ë¡œ ìŒì‹ ì‚¬ì§„', 'ì‚¬ì‹¤ì  ì§ˆê°', '8k', 'íƒ‘ë‹¤ìš´ ë·°'. \"\n",
    "                            \n",
    "                            \"ìµœì¢… ì¶œë ¥: ì‹ë³„ëœ ì–´ì¢… íƒœê·¸ë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì¼ íƒœê·¸ ë¦¬ìŠ¤íŠ¸.\"\n",
    "                        )\n",
    "                }\n",
    "                query = detail_prompts.get(detail_level, detail_prompts[\"detailed\"])\n",
    "\n",
    "            print(f\"  ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘ (ë ˆë²¨: {detail_level})...\")\n",
    "\n",
    "            # ëª¨ë¸ì„ ìœ„í•œ ë©”ì‹œì§€ ì¤€ë¹„\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": image},\n",
    "                        {\"type\": \"text\", \"text\": query},\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            # ì…ë ¥ ì „ì²˜ë¦¬\n",
    "            text = self.processor.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "            inputs = self.processor(\n",
    "                text=[text],\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            inputs = inputs.to(self.device)\n",
    "\n",
    "            # ì‘ë‹µ ìƒì„± (detail_levelì— ë”°ë¼ í† í° ìˆ˜ ì¡°ì •)\n",
    "            max_tokens = {\"simple\": 128, \"detailed\": 256, \"ultra\": 512}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_tokens.get(detail_level, 256),\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9\n",
    "                )\n",
    "\n",
    "            # ì‘ë‹µ ë””ì½”ë”©\n",
    "            generated_ids_trimmed = [\n",
    "                out_ids[len(in_ids):]\n",
    "                for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "\n",
    "            output_text = self.processor.batch_decode(\n",
    "                generated_ids_trimmed,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=False,\n",
    "            )[0]\n",
    "\n",
    "            # í”„ë¡¬í”„íŠ¸ ì •ì œ (ë¶ˆí•„ìš”í•œ ë¬¸ì¥ ì œê±°)\n",
    "            prompt = self._refine_prompt(output_text)\n",
    "\n",
    "            result = {\n",
    "                \"prompt\": prompt,\n",
    "                \"raw_response\": output_text,\n",
    "                \"image_size\": image_size\n",
    "            }\n",
    "\n",
    "            print(f\"  âœ“ í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(prompt)} chars)\")\n",
    "            print(f\"  í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {prompt[:100]}...\")\n",
    "\n",
    "            return result\n",
    "\n",
    "        finally:\n",
    "            if auto_unload:\n",
    "                self._unload_model()\n",
    "\n",
    "    def generate_flux_prompt_from_keywords(\n",
    "        self,\n",
    "        keywords: Union[str, List[str]],\n",
    "        style: str = \"photorealistic\",\n",
    "        detail_level: str = \"ultra\",\n",
    "        language: str = \"en\",\n",
    "        auto_unload: bool = True,\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        í‚¤ì›Œë“œë§Œìœ¼ë¡œ FLUX.1ìš© ìƒì„¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        Args:\n",
    "            keywords: ë‹¨ì¼ ë¬¸ìì—´ ë˜ëŠ” í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: \"beer, pub\" ë˜ëŠ” [\"beer\", \"pub\", \"night\"])\n",
    "            style: 'photorealistic', 'artistic', 'cinematic', 'minimalist' ì¤‘ ì„ íƒ\n",
    "            detail_level: 'simple', 'detailed', 'ultra' ì¤‘ ì„ íƒ\n",
    "            language: 'en' (ì˜ì–´) ë˜ëŠ” 'ko' (í•œêµ­ì–´)\n",
    "            \n",
    "        Returns:\n",
    "            ë”•ì…”ë„ˆë¦¬:\n",
    "                - 'prompt': FLUX.1ìš© ìƒì„± í”„ë¡¬í”„íŠ¸\n",
    "                - 'raw_response': ëª¨ë¸ì˜ ì›ë³¸ ì‘ë‹µ\n",
    "                - 'keywords': ì…ë ¥ëœ í‚¤ì›Œë“œ\n",
    "                \n",
    "        Example:\n",
    "            >>> result = analyzer.generate_flux_prompt_from_keywords(\n",
    "            ...     \"beer, wooden table, warm lighting\",\n",
    "            ...     style=\"cinematic\",\n",
    "            ...     detail_level=\"ultra\"\n",
    "            ... )\n",
    "            >>> flux_prompt = result['prompt']\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._load_model()\n",
    "            \n",
    "            # í‚¤ì›Œë“œ ì •ê·œí™”\n",
    "            if isinstance(keywords, str):\n",
    "                keywords_list = [k.strip() for k in keywords.split(',')]\n",
    "            else:\n",
    "                keywords_list = keywords\n",
    "            \n",
    "            keywords_str = \", \".join(keywords_list)\n",
    "            \n",
    "            # ìŠ¤íƒ€ì¼ë³„ ê¸°ë³¸ ì„¤ì •\n",
    "            style_guides = {\n",
    "                \"photorealistic\": {\n",
    "                    \"en\": \"professional photography, sharp focus, 8k resolution, highly detailed\",\n",
    "                    \"ko\": \"ì „ë¬¸ ì‚¬ì§„, ì„ ëª…í•œ ì´ˆì , 8k í•´ìƒë„, ë§¤ìš° ìƒì„¸í•¨\"\n",
    "                },\n",
    "                \"artistic\": {\n",
    "                    \"en\": \"artistic composition, painterly quality, creative lighting, masterpiece\",\n",
    "                    \"ko\": \"ì˜ˆìˆ ì  êµ¬ë„, íšŒí™”ì  í’ˆì§ˆ, ì°½ì˜ì  ì¡°ëª…, ê±¸ì‘\"\n",
    "                },\n",
    "                \"cinematic\": {\n",
    "                    \"en\": \"cinematic lighting, dramatic atmosphere, film grain, anamorphic lens\",\n",
    "                    \"ko\": \"ì˜í™”ì  ì¡°ëª…, ê·¹ì ì¸ ë¶„ìœ„ê¸°, í•„ë¦„ ê·¸ë ˆì¸, ì•„ë‚˜ëª¨í”½ ë Œì¦ˆ\"\n",
    "                },\n",
    "                \"minimalist\": {\n",
    "                    \"en\": \"minimalist composition, clean lines, soft shadows, elegant simplicity\",\n",
    "                    \"ko\": \"ë¯¸ë‹ˆë©€ êµ¬ë„, ê¹”ë”í•œ ì„ , ë¶€ë“œëŸ¬ìš´ ê·¸ë¦¼ì, ìš°ì•„í•œ ë‹¨ìˆœí•¨\"\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            style_suffix = style_guides.get(style, style_guides[\"photorealistic\"])[language]\n",
    "            \n",
    "            # detail_levelì— ë”°ë¥¸ ì§ˆë¬¸ êµ¬ì„±\n",
    "            if language == \"en\":\n",
    "                if detail_level == \"simple\":\n",
    "                    query = (\n",
    "                        f\"Create a brief FLUX.1 text-to-image prompt based on these keywords: {keywords_str}. \"\n",
    "                        f\"Style: {style}. Write a concise, single-paragraph prompt suitable for image generation.\"\n",
    "                    )\n",
    "                elif detail_level == \"detailed\":\n",
    "                    query = (\n",
    "                        f\"You are a professional FLUX.1 prompt engineer. Create a highly detailed text-to-image prompt \"\n",
    "                        f\"using these keywords: {keywords_str}. Target style: {style}.\\n\\n\"\n",
    "                        f\"Your prompt must be rich, descriptive, and include:\\n\"\n",
    "                        f\"- Main subject: Describe appearance, materials, textures, colors in detail\\n\"\n",
    "                        f\"- Spatial layout: Foreground, midground, background elements and their relationships\\n\"\n",
    "                        f\"- Lighting: Type (natural/artificial), direction, quality (soft/hard), color temperature\\n\"\n",
    "                        f\"- Color palette: Dominant colors, accent colors, color harmony\\n\"\n",
    "                        f\"- Atmosphere: Mood, feeling, time of day, weather conditions\\n\"\n",
    "                        f\"- Technical aspects: {style_suffix}\\n\\n\"\n",
    "                        f\"Write as a flowing, narrative paragraph. DO NOT use bullet points or lists. \"\n",
    "                        f\"Output ONLY the final prompt text.\"\n",
    "                    )\n",
    "                else:  # ultra\n",
    "                    query = (\n",
    "                        f\"You are a master FLUX.1 prompt engineer creating a production-grade, \"\n",
    "                        f\"ultra-detailed text-to-image prompt from these keywords: {keywords_str}. \"\n",
    "                        f\"Target style: {style}.\\n\\n\"\n",
    "                        f\"Create an EXTREMELY DETAILED prompt with vivid, specific descriptions:\\n\\n\"\n",
    "                        f\"1. SUBJECT DETAILS:\\n\"\n",
    "                        f\"   - Exact appearance: shape, size, proportions\\n\"\n",
    "                        f\"   - Surface qualities: texture (rough/smooth/glossy), material properties, patterns\\n\"\n",
    "                        f\"   - Colors: specific hues, saturation levels, color variations, highlights/shadows\\n\"\n",
    "                        f\"   - Fine details: scratches, reflections, transparency, wear marks\\n\\n\"\n",
    "                        f\"2. SPATIAL COMPOSITION:\\n\"\n",
    "                        f\"   - Camera angle and perspective (bird's eye, eye-level, low angle, Dutch tilt)\\n\"\n",
    "                        f\"   - Depth layers: foreground (closest), midground, background (farthest)\\n\"\n",
    "                        f\"   - Rule of thirds positioning, leading lines, symmetry/asymmetry\\n\"\n",
    "                        f\"   - Depth of field: what's in focus vs. bokeh blur\\n\\n\"\n",
    "                        f\"3. LIGHTING DESIGN:\\n\"\n",
    "                        f\"   - Light source type: sunlight, studio lights, ambient, practical lights\\n\"\n",
    "                        f\"   - Direction: front-lit, back-lit, side-lit, rim lighting\\n\"\n",
    "                        f\"   - Quality: hard shadows vs. soft diffused light\\n\"\n",
    "                        f\"   - Color temperature: warm (golden hour) vs. cool (blue hour)\\n\"\n",
    "                        f\"   - Special effects: god rays, lens flare, volumetric fog, caustics\\n\\n\"\n",
    "                        f\"4. COLOR & ATMOSPHERE:\\n\"\n",
    "                        f\"   - Primary color scheme and palette\\n\"\n",
    "                        f\"   - Accent colors for visual interest\\n\"\n",
    "                        f\"   - Color grading: vibrant, desaturated, high contrast, muted\\n\"\n",
    "                        f\"   - Atmospheric conditions: clear, hazy, misty, dusty, smoky\\n\"\n",
    "                        f\"   - Emotional mood: peaceful, dramatic, mysterious, energetic, melancholic\\n\\n\"\n",
    "                        f\"5. TECHNICAL PHOTOGRAPHY:\\n\"\n",
    "                        f\"   - {style_suffix}\\n\"\n",
    "                        f\"   - Lens characteristics: focal length, aperture effect, lens distortion\\n\"\n",
    "                        f\"   - Image quality: resolution, sharpness, grain/noise level\\n\"\n",
    "                        f\"   - Post-processing style: color grading, contrast adjustments\\n\\n\"\n",
    "                        f\"Write as ONE comprehensive, flowing narrative paragraph that paints a complete visual picture. \"\n",
    "                        f\"Be specific, vivid, and detailed. Use descriptive adjectives and sensory language. \"\n",
    "                        f\"DO NOT write as a list or analysis. Output ONLY the raw prompt string ready for FLUX.1.\"\n",
    "                    )\n",
    "            else:  # Korean\n",
    "                if detail_level == \"simple\":\n",
    "                    query = (\n",
    "                        f\"ë‹¤ìŒ í‚¤ì›Œë“œë¡œ ê°„ë‹¨í•œ FLUX.1 ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”: {keywords_str}. \"\n",
    "                        f\"ìŠ¤íƒ€ì¼: {style}. ê°„ê²°í•œ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\"\n",
    "                    )\n",
    "                elif detail_level == \"detailed\":\n",
    "                    query = (\n",
    "                        f\"ë‹¹ì‹ ì€ ì „ë¬¸ FLUX.1 í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤. ë‹¤ìŒ í‚¤ì›Œë“œë¡œ ë§¤ìš° ìƒì„¸í•œ ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ë¥¼ \"\n",
    "                        f\"ì‘ì„±í•˜ì„¸ìš”: {keywords_str}. ëª©í‘œ ìŠ¤íƒ€ì¼: {style}.\\n\\n\"\n",
    "                        f\"í”„ë¡¬í”„íŠ¸ëŠ” í’ë¶€í•˜ê³  êµ¬ì²´ì ì´ì–´ì•¼ í•˜ë©° ë‹¤ìŒì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:\\n\"\n",
    "                        f\"- ì£¼ìš” í”¼ì‚¬ì²´: ì™¸í˜•, ì¬ì§ˆ, ì§ˆê°, ìƒ‰ìƒì„ ìƒì„¸íˆ ë¬˜ì‚¬\\n\"\n",
    "                        f\"- ê³µê°„ ë°°ì¹˜: ì „ê²½, ì¤‘ê²½, ë°°ê²½ ìš”ì†Œë“¤ê³¼ ê·¸ ê´€ê³„\\n\"\n",
    "                        f\"- ì¡°ëª…: ì¢…ë¥˜(ìì—°ê´‘/ì¸ê³µê´‘), ë°©í–¥, í’ˆì§ˆ(ê°•í•¨/ë¶€ë“œëŸ¬ì›€), ìƒ‰ì˜¨ë„\\n\"\n",
    "                        f\"- ìƒ‰ìƒ íŒ”ë ˆíŠ¸: ì£¼ì¡°ìƒ‰, ê°•ì¡°ìƒ‰, ìƒ‰ìƒ ì¡°í™”\\n\"\n",
    "                        f\"- ë¶„ìœ„ê¸°: ë¬´ë“œ, ëŠë‚Œ, ì‹œê°„ëŒ€, ë‚ ì”¨ ì¡°ê±´\\n\"\n",
    "                        f\"- ê¸°ìˆ ì  ìš”ì†Œ: {style_suffix}\\n\\n\"\n",
    "                        f\"íë¥´ëŠ” ì„œìˆ í˜• ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë‚˜ ëª©ë¡ í˜•ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. \"\n",
    "                        f\"ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.\"\n",
    "                    )\n",
    "                else:  # ultra\n",
    "                    query = (\n",
    "                        f\"ë‹¹ì‹ ì€ ìµœê³  ìˆ˜ì¤€ì˜ FLUX.1 í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤. ë‹¤ìŒ í‚¤ì›Œë“œë¡œ í”„ë¡œë•ì…˜ ê¸‰ì˜ \"\n",
    "                        f\"ì´ˆìƒì„¸ ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”: {keywords_str}. \"\n",
    "                        f\"ëª©í‘œ ìŠ¤íƒ€ì¼: {style}.\\n\\n\"\n",
    "                        f\"ìƒë™ê° ìˆê³  êµ¬ì²´ì ì¸ ë¬˜ì‚¬ë¡œ ê·¹ë„ë¡œ ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”:\\n\\n\"\n",
    "                        f\"1. í”¼ì‚¬ì²´ ìƒì„¸ ë¬˜ì‚¬:\\n\"\n",
    "                        f\"   - ì •í™•í•œ ì™¸í˜•: í˜•íƒœ, í¬ê¸°, ë¹„ìœ¨\\n\"\n",
    "                        f\"   - í‘œë©´ íŠ¹ì„±: ì§ˆê°(ê±°ì¹ ê¸°/ë§¤ë„ëŸ¬ì›€/ê´‘íƒ), ì¬ì§ˆ ì†ì„±, íŒ¨í„´\\n\"\n",
    "                        f\"   - ìƒ‰ìƒ: êµ¬ì²´ì ì¸ ìƒ‰ì¡°, ì±„ë„ ìˆ˜ì¤€, ìƒ‰ìƒ ë³€í™”, í•˜ì´ë¼ì´íŠ¸/ê·¸ë¦¼ì\\n\"\n",
    "                        f\"   - ë¯¸ì„¸ ë””í…Œì¼: ê¸íŒ ìêµ­, ë°˜ì‚¬, íˆ¬ëª…ë„, ë§ˆëª¨ í”ì \\n\\n\"\n",
    "                        f\"2. ê³µê°„ êµ¬ì„±:\\n\"\n",
    "                        f\"   - ì¹´ë©”ë¼ ê°ë„ì™€ ì›ê·¼ë²•(ì¡°ê°ë„, ëˆˆë†’ì´, ë¡œìš°ì•µê¸€, ë”ì¹˜ì•µê¸€)\\n\"\n",
    "                        f\"   - ì‹¬ë„ ë ˆì´ì–´: ì „ê²½(ê°€ì¥ ê°€ê¹Œìš´), ì¤‘ê²½, ë°°ê²½(ê°€ì¥ ë¨¼)\\n\"\n",
    "                        f\"   - ì‚¼ë¶„í• ë²• ë°°ì¹˜, ìœ ë„ì„ , ëŒ€ì¹­/ë¹„ëŒ€ì¹­\\n\"\n",
    "                        f\"   - í”¼ì‚¬ê³„ ì‹¬ë„: ì´ˆì ì´ ë§ëŠ” ë¶€ë¶„ vs. ë³´ì¼€ ë¸”ëŸ¬\\n\\n\"\n",
    "                        f\"3. ì¡°ëª… ì„¤ê³„:\\n\"\n",
    "                        f\"   - ê´‘ì› ì¢…ë¥˜: í–‡ë¹›, ìŠ¤íŠœë””ì˜¤ ì¡°ëª…, ì£¼ë³€ê´‘, ì‹¤ìš© ì¡°ëª…\\n\"\n",
    "                        f\"   - ë°©í–¥: ì •ë©´ê´‘, ì—­ê´‘, ì¸¡ë©´ê´‘, ë¦¼ ë¼ì´íŒ…\\n\"\n",
    "                        f\"   - í’ˆì§ˆ: ê°•í•œ ê·¸ë¦¼ì vs. ë¶€ë“œëŸ½ê²Œ í™•ì‚°ëœ ë¹›\\n\"\n",
    "                        f\"   - ìƒ‰ì˜¨ë„: ë”°ëœ»í•œ ë¹›(ê³¨ë“ ì•„ì›Œ) vs. ì°¨ê°€ìš´ ë¹›(ë¸”ë£¨ì•„ì›Œ)\\n\"\n",
    "                        f\"   - íŠ¹ìˆ˜ íš¨ê³¼: ë¹›ì¤„ê¸°, ë Œì¦ˆ í”Œë ˆì–´, ë³¼ë¥˜ë©”íŠ¸ë¦­ ì•ˆê°œ, ì½”ìŠ¤í‹±ìŠ¤\\n\\n\"\n",
    "                        f\"4. ìƒ‰ìƒ ë° ë¶„ìœ„ê¸°:\\n\"\n",
    "                        f\"   - ì£¼ìš” ìƒ‰ìƒ êµ¬ì„±ê³¼ íŒ”ë ˆíŠ¸\\n\"\n",
    "                        f\"   - ì‹œê°ì  í¥ë¯¸ë¥¼ ìœ„í•œ ê°•ì¡°ìƒ‰\\n\"\n",
    "                        f\"   - ìƒ‰ ë³´ì •: ì„ ëª…í•œ, íƒˆìƒ‰ëœ, ê³ ëŒ€ë¹„, ìŒì†Œê±°ëœ\\n\"\n",
    "                        f\"   - ëŒ€ê¸° ìƒíƒœ: ë§‘ì€, íë¦¿í•œ, ì•ˆê°œ ë‚€, ë¨¼ì§€ ë‚€, ì—°ê¸° ììš±í•œ\\n\"\n",
    "                        f\"   - ê°ì •ì  ë¬´ë“œ: í‰í™”ë¡œìš´, ê·¹ì ì¸, ì‹ ë¹„ë¡œìš´, í™œê¸°ì°¬, ìš°ìš¸í•œ\\n\\n\"\n",
    "                        f\"5. ê¸°ìˆ ì  ì´¬ì˜ ìš”ì†Œ:\\n\"\n",
    "                        f\"   - {style_suffix}\\n\"\n",
    "                        f\"   - ë Œì¦ˆ íŠ¹ì„±: ì´ˆì ê±°ë¦¬, ì¡°ë¦¬ê°œ íš¨ê³¼, ë Œì¦ˆ ì™œê³¡\\n\"\n",
    "                        f\"   - ì´ë¯¸ì§€ í’ˆì§ˆ: í•´ìƒë„, ì„ ëª…ë„, ê·¸ë ˆì¸/ë…¸ì´ì¦ˆ ìˆ˜ì¤€\\n\"\n",
    "                        f\"   - í›„ì²˜ë¦¬ ìŠ¤íƒ€ì¼: ìƒ‰ ë³´ì •, ëŒ€ë¹„ ì¡°ì •\\n\\n\"\n",
    "                        f\"ì™„ì „í•œ ì‹œê°ì  ê·¸ë¦¼ì„ ê·¸ë¦¬ëŠ” í•˜ë‚˜ì˜ í¬ê´„ì ì´ê³  íë¥´ëŠ” ì„œìˆ í˜• ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. \"\n",
    "                        f\"êµ¬ì²´ì ì´ê³ , ìƒë™ê° ìˆê³ , ìƒì„¸í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. ë¬˜ì‚¬ì ì¸ í˜•ìš©ì‚¬ì™€ ê°ê°ì  ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. \"\n",
    "                        f\"ëª©ë¡ì´ë‚˜ ë¶„ì„ë¬¸ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì§€ ë§ˆì„¸ìš”. FLUX.1ì— ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ë§Œ ì¶œë ¥í•˜ì„¸ìš”.\"\n",
    "                    )\n",
    "            \n",
    "            print(f\"  í‚¤ì›Œë“œ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...\")\n",
    "            print(f\"  í‚¤ì›Œë“œ: {keywords_str}\")\n",
    "            print(f\"  ìŠ¤íƒ€ì¼: {style}, ë ˆë²¨: {detail_level}\")\n",
    "            \n",
    "            # í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë©”ì‹œì§€ ì¤€ë¹„ (ì´ë¯¸ì§€ ì—†ìŒ)\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": query}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # ì…ë ¥ ì „ì²˜ë¦¬\n",
    "            text = self.processor.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            inputs = self.processor(\n",
    "                text=[text],\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            inputs = inputs.to(self.device)\n",
    "            \n",
    "            # ì‘ë‹µ ìƒì„±\n",
    "            max_tokens = {\"simple\": 128, \"detailed\": 256, \"ultra\": 512}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_tokens.get(detail_level, 256),\n",
    "                    temperature=0.8,  # ì°½ì˜ì„±ì„ ìœ„í•´ ì•½ê°„ ë†’ì„\n",
    "                    top_p=0.9\n",
    "                )\n",
    "            \n",
    "            # ì‘ë‹µ ë””ì½”ë”©\n",
    "            generated_ids_trimmed = [\n",
    "                out_ids[len(in_ids):]\n",
    "                for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "            \n",
    "            output_text = self.processor.batch_decode(\n",
    "                generated_ids_trimmed,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=False\n",
    "            )[0]\n",
    "            \n",
    "            # í”„ë¡¬í”„íŠ¸ ì •ì œ\n",
    "            prompt = self._refine_prompt(output_text)\n",
    "            \n",
    "            result = {\n",
    "                \"prompt\": prompt,\n",
    "                \"raw_response\": output_text,\n",
    "                \"keywords\": keywords_str,\n",
    "                \"style\": style\n",
    "            }\n",
    "            \n",
    "            print(f\"  âœ“ í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(prompt)} chars)\")\n",
    "            print(f\"  í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {prompt[:100]}...\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        finally:\n",
    "            if auto_unload:\n",
    "                self._unload_model()\n",
    "\n",
    "    def _refine_prompt(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        ëª¨ë¸ ì‘ë‹µì„ FLUX.1ìš© í”„ë¡¬í”„íŠ¸ë¡œ ì •ì œí•©ë‹ˆë‹¤.\n",
    "        \n",
    "        Args:\n",
    "            text: ëª¨ë¸ì˜ ì›ë³¸ ì‘ë‹µ\n",
    "            \n",
    "        Returns:\n",
    "            ì •ì œëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´\n",
    "        \"\"\"\n",
    "        # ë¶ˆí•„ìš”í•œ ì ‘ë‘ì‚¬ ì œê±°\n",
    "        prefixes_to_remove = [\n",
    "            \"This image shows \",\n",
    "            \"The image depicts \",\n",
    "            \"In this image, \",\n",
    "            \"ì´ ì´ë¯¸ì§€ëŠ” \",\n",
    "            \"ì´ë¯¸ì§€ì—ëŠ” \"\n",
    "        ]\n",
    "        \n",
    "        refined = text.strip()\n",
    "        for prefix in prefixes_to_remove:\n",
    "            if refined.startswith(prefix):\n",
    "                refined = refined[len(prefix):]\n",
    "        \n",
    "        # ì²« ê¸€ì ëŒ€ë¬¸ìë¡œ\n",
    "        if refined:\n",
    "            refined = refined[0].upper() + refined[1:]\n",
    "        \n",
    "        return refined\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"ê°ì²´ ì†Œë©¸ ì‹œ ì •ë¦¬ ì‘ì—…\"\"\"\n",
    "        self._unload_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77ed0194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-17 07:47:28 I [flux_generator:37] - FluxGenerator initialized (pipelines will load on demand)\n",
      "ğŸ”§ SpatialAnalyzer ì´ˆê¸°í™”: Qwen/Qwen2-VL-7B-Instruct\n",
      "2026-01-17 07:47:28 D [ipykernel_launcher:6] - ë¡œë”© ì„±ê³µ\n",
      "ğŸ”§ SpatialAnalyzer ì´ˆê¸°í™”: Qwen/Qwen2-VL-7B-Instruct\n",
      "2026-01-17 07:47:28 D [ipykernel_launcher:6] - ë¡œë”© ì„±ê³µ\n"
     ]
    }
   ],
   "source": [
    "from models.flux_generator import FluxGenerator\n",
    "\n",
    "fluxgenerator = FluxGenerator()\n",
    "segmenter = SegmentationModel()\n",
    "analyzer = SpatialAnalyzer()\n",
    "logger.debug(\"ë¡œë”© ì„±ê³µ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a730ce0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-17 07:47:30 D [ipykernel_launcher:29] - [Engine] Loading BiRefNet... (BiRefNet ëª¨ë¸ ë¡œë”© ì¤‘)\n"
     ]
    }
   ],
   "source": [
    "product_fg, product_mask = segmenter.run(Image.open(\"image01.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e4cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "resutl = analyzer.generate_flux_prompt(\"image01.png\", detail_level=\"ultra\")\n",
    "logger.info(resutl['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29996603",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"A detailed, photorealistic close-up shot on a worn wooden pub table. Two glasses of Korean draft beer are central. On the left, a tall, slender pilsner glass for a woman, filled with golden beer and a thick foam head. On the right, a robust, handled glass mug for a man, also filled. Both glasses are heavily covered in cold condensation and realistic water droplets streaking down the sides, pooling slightly on the table. The labels on the glasses are partially obscured by moisture but suggest popular Korean brands like 'Cass' or 'Terra'. In the background, out of focus, a lively, warm-lit Korean pub (pocha) interior with blurred patrons, metal chopsticks, and small snack dishes. Natural bar lighting, highly textured, cinematic quality, 8k resolution.\"\"\"\n",
    "image = fluxgenerator.generate_background(f\"{resutl['prompt']}\")\n",
    "analyzer._unload_model()\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c46cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-17 07:47:43 I [flux_generator:37] - FluxGenerator initialized (pipelines will load on demand)\n",
      "2026-01-17 07:47:43 I [flux_generator:139] - [FluxGenerator] Generating background with Text-to-Image...\n",
      "2026-01-17 07:47:43 I [flux_generator:60] - [FluxGenerator] Loading FLUX Text-to-Image pipeline...\n",
      "2026-01-17 07:47:43 I [flux_generator:44] - [FluxGenerator] Loading 8bit quantized Transformer...\n",
      "2026-01-17 07:47:43 I [flux_generator:139] - [FluxGenerator] Generating background with Text-to-Image...\n",
      "2026-01-17 07:47:43 I [flux_generator:60] - [FluxGenerator] Loading FLUX Text-to-Image pipeline...\n",
      "2026-01-17 07:47:43 I [flux_generator:44] - [FluxGenerator] Loading 8bit quantized Transformer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a439f9c215446ab1b8f2e090829b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586baa19bf594f91b379dbb3b3de3b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-17 07:50:02 I [flux_generator:52] - [FluxGenerator] Transformer loaded with 8bit quantization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318737ede9874e7fb8df2990ca3845ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672d6a7e2b2a40048348fa6a2e909a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module 'FluxTransformer2DModel' has been loaded in `bitsandbytes` 8bit and moving it to cpu via `.to()` is not supported. Module is still on cuda:0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-17 07:50:08 I [flux_generator:73] - [FluxGenerator] Text-to-Image pipeline ready\n",
      "2026-01-17 07:50:08 I [flux_generator:159] -  [FluxGenerator] prompt='\n",
      "A blurred, dreamy illustration of a cozy Korean beer pub (pocha) interior. \n",
      "Cartoon style, anime aesthetic, soft focus background with warm ambient lighting. \n",
      "Blurred hanging lanterns, wooden tables, and traditional decorations. \n",
      "Empty foreground space for product placement. \n",
      "Bokeh effect, shallow depth of field, cinematic atmosphere, 8k illustration.\n",
      "' \n",
      "2026-01-17 07:50:08 I [flux_generator:159] -  [FluxGenerator] prompt='\n",
      "A blurred, dreamy illustration of a cozy Korean beer pub (pocha) interior. \n",
      "Cartoon style, anime aesthetic, soft focus background with warm ambient lighting. \n",
      "Blurred hanging lanterns, wooden tables, and traditional decorations. \n",
      "Empty foreground space for product placement. \n",
      "Bokeh effect, shallow depth of field, cinematic atmosphere, 8k illustration.\n",
      "' \n",
      "2026-01-17 07:50:08 I [flux_generator:160] -  [FluxGenerator] negative_prompt='None' \n",
      "2026-01-17 07:50:08 I [flux_generator:160] -  [FluxGenerator] negative_prompt='None' \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c6af6320b3443b8c5d1720b5d78619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spai0433/miniconda3/envs/py311_adv2/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "The module 'FluxTransformer2DModel' has been loaded in `bitsandbytes` 8bit and moving it to cpu via `.to()` is not supported. Module is still on cuda:0.\n",
      "The module 'FluxTransformer2DModel' has been loaded in `bitsandbytes` 8bit and moving it to cpu via `.to()` is not supported. Module is still on cuda:0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-17 07:52:27 I [flux_generator:173] - [FluxGenerator] Background generation completed\n",
      "2026-01-17 07:52:27 I [flux_generator:322] - [FluxGenerator] Injecting features via Inpainting...\n",
      "2026-01-17 07:52:27 I [flux_generator:322] - [FluxGenerator] Injecting features via Inpainting...\n",
      "2026-01-17 07:52:27 I [flux_generator:102] - [FluxGenerator] Loading FLUX Inpaint pipeline...\n",
      "2026-01-17 07:52:27 I [flux_generator:102] - [FluxGenerator] Loading FLUX Inpaint pipeline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b8e6a02196493f81ec22e5baf4a4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb88c7d0fc14135a4fa078cb6b94cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module 'FluxTransformer2DModel' has been loaded in `bitsandbytes` 8bit and moving it to cpu via `.to()` is not supported. Module is still on cuda:0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-17 07:52:31 I [flux_generator:115] - [FluxGenerator] Inpaint pipeline ready\n",
      "2026-01-17 07:52:31 I [flux_generator:353] - [FluxGenerator] Injecting features: prompt='\n",
      "Crystal clear, ultra-sharp dried yellow corvina f...', strength=0.3\n",
      "2026-01-17 07:52:31 I [flux_generator:357] -  [FluxGenerator] prompt='\n",
      "Crystal clear, ultra-sharp dried yellow corvina fish (nogari) on a white plate. \n",
      "Photorealistic detail with visible texture, char marks, and natural sheen. \n",
      "Sharp focus on product, contrasting with blurred cartoon background. \n",
      "Professional food photography lighting, 8k, masterpiece quality.\n",
      "' \n",
      "2026-01-17 07:52:31 I [flux_generator:358] -  [FluxGenerator] negative_prompt='blurry product, soft focus product, cartoon fish, unrealistic' \n",
      "2026-01-17 07:52:31 I [flux_generator:353] - [FluxGenerator] Injecting features: prompt='\n",
      "Crystal clear, ultra-sharp dried yellow corvina f...', strength=0.3\n",
      "2026-01-17 07:52:31 I [flux_generator:357] -  [FluxGenerator] prompt='\n",
      "Crystal clear, ultra-sharp dried yellow corvina fish (nogari) on a white plate. \n",
      "Photorealistic detail with visible texture, char marks, and natural sheen. \n",
      "Sharp focus on product, contrasting with blurred cartoon background. \n",
      "Professional food photography lighting, 8k, masterpiece quality.\n",
      "' \n",
      "2026-01-17 07:52:31 I [flux_generator:358] -  [FluxGenerator] negative_prompt='blurry product, soft focus product, cartoon fish, unrealistic' \n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from models.flux_generator import FluxGenerator\n",
    "\n",
    "# 1. FluxGenerator ì´ˆê¸°í™”\n",
    "generator = FluxGenerator()\n",
    "\n",
    "# 2. íë¦° í•œêµ­ ë§¥ì£¼ì§‘ ë°°ê²½ ìƒì„± (ë§Œí™”í’)\n",
    "background_prompt = \"\"\"\n",
    "A blurred, dreamy illustration of a cozy Korean beer pub (pocha) interior. \n",
    "Cartoon style, anime aesthetic, soft focus background with warm ambient lighting. \n",
    "Blurred hanging lanterns, wooden tables, and traditional decorations. \n",
    "Empty foreground space for product placement. \n",
    "Bokeh effect, shallow depth of field, cinematic atmosphere, 8k illustration.\n",
    "\"\"\"\n",
    "\n",
    "bg_image = generator.generate_background(\n",
    "    prompt=background_prompt,\n",
    "    guidance_scale=3.5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# 3. ë…¸ê°€ë¦¬ ì´ë¯¸ì§€ ë¡œë“œ (ë°°ê²½ ì œê±°ëœ RGBA)\n",
    "nogari_image = Image.open(\"image01.png\").convert(\"RGBA\")\n",
    "nogari_mask = nogari_image.split()[3]  # ì•ŒíŒŒ ì±„ë„ì„ ë§ˆìŠ¤í¬ë¡œ ì‚¬ìš©\n",
    "\n",
    "# 4. ë…¸ê°€ë¦¬ë¥¼ ì„ ëª…í•˜ê²Œ ìœ ì§€í•˜ë©´ì„œ ë°°ê²½ì— í•©ì„±\n",
    "product_prompt = \"\"\"\n",
    "Crystal clear, ultra-sharp dried yellow corvina fish (nogari) on a white plate. \n",
    "Photorealistic detail with visible texture, char marks, and natural sheen. \n",
    "Sharp focus on product, contrasting with blurred cartoon background. \n",
    "Professional food photography lighting, 8k, masterpiece quality.\n",
    "\"\"\"\n",
    "\n",
    "final_image = generator.inject_features_via_inpaint(\n",
    "    background=bg_image,\n",
    "    product_foreground=nogari_image,\n",
    "    product_mask=nogari_mask,\n",
    "    position=(200, 300),  # ë°°ì¹˜ ìœ„ì¹˜ ì¡°ì •\n",
    "    prompt=product_prompt,\n",
    "    negative_prompt=\"blurry product, soft focus product, cartoon fish, unrealistic\",\n",
    "    strength=0.3,  # ë‚®ì€ ê°’ = ë…¸ê°€ë¦¬ ì›ë³¸ ìœ ì§€\n",
    "    guidance_scale=4.0,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "#final_image.save(\"nogari_ad.png\")\n",
    "final_image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_adv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
