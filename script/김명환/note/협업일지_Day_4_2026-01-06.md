# 협업일지 Day 4 (2026-01-06)

## 작성자
김명환

## 수행 내용

### 1. 아키텍처 설계 내용 공유

#### 파이프라인 구조 설계
- **전체 워크플로우**: 배경 분석 → 객체 추출 → 마스크 생성 → 합성
- **모듈화 설계**: 각 단계를 독립적인 클래스로 분리
  - `BackgroundAnalyzer`: 배경 이미지 분석 및 설명 생성
  - `ObjectExtractor`: 참조 이미지에서 객체 추출
  - `MaskGenerator`: 배경에서 합성 위치 마스크 생성
  - `ObjectSynthesizer`: 최종 합성 수행

#### 메모리 최적화 전략
- **8bit 양자화**: Transformer 모델을 8bit로 로드하여 VRAM 절약 (~12GB)
- **CPU Offload**: 미사용 컴포넌트를 CPU로 이동
- **Attention Slicing**: VRAM 추가 절약 (1-2GB)
- **GPU 메모리 모니터링**: 각 로딩 단계별 메모리 사용량 출력

#### 주요 기술 스택
- **FLUX.1-Fill-dev**: 인페인팅 모델 (배경과 자연스럽게 합성)
- **Qwen2-VL-7B**: 이미지 설명 생성 모델
- **SAM2**: 객체 세그멘테이션
- **RMBG-2.0**: 배경 제거

### 2. Qwen 엔진 테스트 내용 공유

#### 테스트 개요
- **모델**: Qwen2-VL-7B-Instruct (4bit 양자화)
- **목적**: 배경 이미지 분석 및 자연어 설명 생성

#### 테스트 결과
- ✅ **이미지 분석 성능**: 배경의 조명, 색감, 분위기를 정확히 파악
- ✅ **메모리 효율성**: 4bit 양자화로 L4 GPU에서 안정적 실행 (~4-5GB VRAM)
- ✅ **추론 속도**: 적절한 응답 시간 (몇 초 내)
- ✅ **한글 지원**: 한글 프롬프트 및 응답 정상 작동

#### 주요 발견 사항
- **프롬프트 엔지니어링 중요성**: 구체적인 질문이 더 나은 결과 생성
- **멀티모달 처리**: 이미지와 텍스트를 동시에 처리하여 맥락 이해
- **재현성**: `seed` 파라미터로 일관된 결과 생성 가능

### 3. 문제 해결 내용

#### IP-Adapter 통합 이슈
- **문제**: `InstantX/FLUX.1-dev-IP-Adapter`가 diffusers에 미통합
- **해결**: 현재 버전에서는 IP-Adapter 비활성화, 프롬프트로 대체
- **향후 계획**: 커스텀 코드 통합 또는 대체 솔루션 검토

#### 메모리 최적화 구현
- **문제**: 트랜스포머 양자화 시 `BitsAndBytesConfig`를 파이프라인에 직접 전달 불가
- **해결**: 트랜스포머를 먼저 양자화한 후 파이프라인에 전달
- **추가 최적화**: Attention Slicing 활성화로 1-2GB 추가 절약

## 다음 단계 계획

### 1. 파이프라인 통합 테스트
- [ ] 전체 워크플로우 end-to-end 테스트
- [ ] 각 모듈 간 데이터 전달 검증
- [ ] 메모리 사용량 프로파일링

### 2. 품질 개선
- [ ] 프롬프트 최적화 (더 나은 합성 결과)
- [ ] 마스크 정밀도 향상
- [ ] 조명/그림자 일관성 개선

### 3. 성능 최적화
- [ ] 추론 시간 측정 및 벤치마크
- [ ] 필요시 Sequential CPU Offload 적용 (옵션 3)
- [ ] 배치 처리 지원 검토

## 기술적 메모

### 파일 구조
```
note/
├── src/
│   ├── analyzer.py      # BackgroundAnalyzer 클래스
│   ├── extractor.py     # ObjectExtractor 클래스
│   ├── generator.py     # MaskGenerator 클래스
│   ├── synthesizer.py   # ObjectSynthesizer 클래스
│   └── utils.py         # 공통 유틸리티
├── notebooks/
│   └── pipeline_validation.ipynb  # 통합 테스트 노트북
└── outputs/             # 생성 결과 저장
```

### 메모리 사용량 추정 (L4 GPU 기준)
| 컴포넌트 | bfloat16 | 8bit 양자화 | 4bit 양자화 |
|---------|----------|------------|------------|
| FLUX Transformer | ~24GB | ~12GB | ~6GB |
| VAE | ~1.6GB | ~0.8GB | ~0.4GB |
| Text Encoder | ~2.4GB | ~1.2GB | ~0.6GB |
| Qwen2-VL-7B | ~14GB | ~7GB | ~4GB |
| **총합** | ~42GB | ~21GB | ~11GB |

*Attention Slicing 및 CPU Offload 적용 시 피크 VRAM 사용량 약 50% 감소*

## 참고 자료
- [FLUX.1-Fill-dev 공식 문서](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev)
- [Qwen2-VL 모델 카드](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct)
- [Diffusers 메모리 최적화 가이드](https://huggingface.co/docs/diffusers/optimization/memory)

## 이슈 및 질문
1. IP-Adapter 없이 프롬프트만으로 충분한 품질을 얻을 수 있는가?
2. Sequential CPU Offload 적용 시 속도 저하가 실용적인 수준인가?
3. 배치 처리 지원이 필요한가?

---
*작성일: 2026-01-06*
*다음 업데이트: Day 5*
