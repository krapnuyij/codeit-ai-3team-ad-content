# ğŸš€ 4bit Quantization for L4 GPU Support

## TL;DR
4bit ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ë©´ **L4 GPU(22GB)ì—ì„œë„ IP-Adapterë¥¼ ì‚¬ìš©í•œ 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê°€ëŠ¥**!

```python
# ì´ì œ L4ì—ì„œë„ ì´ë ‡ê²Œ ì‚¬ìš© ê°€ëŠ¥! ğŸ‰
synthesizer = ObjectSynthesizer(enable_ip_adapter=True)
result = synthesizer.fill_in_object(
    background=bg_image,
    mask=mask_image,
    reference=clean_product,  # âœ… ì°¸ì¡° ì´ë¯¸ì§€ ë°˜ì˜!
    prompt="...",
    use_two_stage=True,  # IP-Adapter í™œì„±í™”
    use_4bit=True,       # 4bit ì–‘ìí™” (ë©”ëª¨ë¦¬ ~12-14GB)
    seed=42
)
```

## ë¬¸ì œ ì¸ì‹

### Before (ë¬¸ì œì )
- **FluxFillPipelineì€ IP-Adapterë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ**
- 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ í•„ìš”: FluxPipeline(IP-Adapter) â†’ FluxFillPipeline
- 8bit ì–‘ìí™”ë¡œë„ **~18-20GB ë©”ëª¨ë¦¬ í•„ìš”**
- **L4 GPU(22GB)ì—ì„œ ì‹¤í–‰ ë¶ˆê°€ëŠ¥** (OOM ë°œìƒ)

### After (í•´ê²°ì±…)
- âœ… **4bit NF4 ì–‘ìí™” ë„ì…**
- âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ **~12-14GBë¡œ ê°ì†Œ**
- âœ… **L4 GPUì—ì„œ ì‹¤í–‰ ê°€ëŠ¥!**
- âœ… í’ˆì§ˆ ì €í•˜ ë¯¸ë¯¸

## ì£¼ìš” ë³€ê²½ì‚¬í•­

### 1. `use_4bit` íŒŒë¼ë¯¸í„° ì¶”ê°€
ëª¨ë“  ê´€ë ¨ ë©”ì„œë“œì— `use_4bit` íŒŒë¼ë¯¸í„° ì¶”ê°€ (ê¸°ë³¸ê°’: `True`)

```python
def fill_in_object(
    self,
    ...,
    use_4bit: bool = True,  # ğŸ†• 4bit ì–‘ìí™”
) -> Image.Image:
```

### 2. 4bit ì–‘ìí™” ì„¤ì •

**NF4 (NormalFloat4) ì–‘ìí™” ì‚¬ìš©:**
```python
BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type="nf4",  # ì •ê·œë¶„í¬ ìµœì í™”
    bnb_4bit_use_double_quant=True,  # ì¶”ê°€ ë©”ëª¨ë¦¬ ì ˆì•½
)
```

### 3. ë‘ íŒŒì´í”„ë¼ì¸ ëª¨ë‘ ì§€ì›
- **FluxFillPipeline** (`_load_model`): 4bit/8bit ì„ íƒ ê°€ëŠ¥
- **FluxPipeline** (`_load_flux_pipeline`): 4bit/8bit ì„ íƒ ê°€ëŠ¥

## ì‚¬ìš© ë°©ë²•

### ê¶Œì¥ ì„¤ì • (L4 GPU)

```python
# ë…¸íŠ¸ë¶ì—ì„œ
USE_TWO_STAGE = True  # IP-Adapter ì‚¬ìš©
USE_4BIT = True       # 4bit ì–‘ìí™” (ê¶Œì¥!)

final_image = synthesizer.fill_in_object(
    background=bg_image,
    mask=mask_image,
    reference=clean_ref_image,
    prompt=PROMPT_SCENARIO,
    use_two_stage=USE_TWO_STAGE,
    use_4bit=USE_4BIT,
    seed=42
)
```

### ì„¤ì • ì˜µì…˜ ë¹„êµ

| ì„¤ì • | ë©”ëª¨ë¦¬ | IP-Adapter | L4 ì§€ì› | ì¶”ì²œ |
|-----|--------|-----------|---------|------|
| `use_two_stage=True, use_4bit=True` | ~12-14GB | âœ… | âœ… | â­â­â­ |
| `use_two_stage=True, use_4bit=False` | ~18-20GB | âœ… | âš ï¸ | â­ |
| `use_two_stage=False, use_4bit=True` | ~7-8GB | âŒ | âœ… | â­â­ |
| `use_two_stage=False, use_4bit=False` | ~11GB | âŒ | âœ… | â­ |

## ë©”ëª¨ë¦¬ ì ˆê° íš¨ê³¼

### 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸
```
ì–‘ìí™” ì—†ìŒ:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ ~22GB+
8bit:         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       ~18-20GB (-18%)
4bit:         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             ~12-14GB (-45%)  â­ ê¶Œì¥!
```

### ë‹¨ì¼ íŒŒì´í”„ë¼ì¸
```
ì–‘ìí™” ì—†ìŒ:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          ~15GB
8bit:         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              ~11GB (-27%)
4bit:         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 ~7-8GB (-47%)
```

## ì„±ëŠ¥ & í’ˆì§ˆ

### ì†ë„
- 4bit: ê¸°ì¤€ ëŒ€ë¹„ ì•½ 80% ì†ë„
- ì–‘ìí™”ë¡œ ì¸í•œ ì•½ê°„ì˜ ì¶”ë¡  ì†ë„ ê°ì†Œ
- í•˜ì§€ë§Œ ë©”ëª¨ë¦¬ ì ˆì•½ìœ¼ë¡œ ì–»ëŠ” ì´ì ì´ í›¨ì”¬ í¼

### í’ˆì§ˆ
- ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì‹œê°ì  ì°¨ì´ ë¯¸ë¯¸
- ì°¸ì¡° ì´ë¯¸ì§€ ë°˜ì˜ ëŠ¥ë ¥ ìœ ì§€
- L4ì—ì„œ IP-Adapter ì‚¬ìš© ê°€ëŠ¥í•œ ê²ƒì´ ë” í° ì¥ì !

## í…ŒìŠ¤íŠ¸

```bash
# 4bit ì–‘ìí™” ì§€ì› í…ŒìŠ¤íŠ¸
python test_4bit_quantization.py
```

**ì˜ˆìƒ ì¶œë ¥:**
```
âœ… ObjectSynthesizer supports 4bit quantization
âœ… use_4bit parameter added to fill_in_object()
âœ… Default: use_4bit=True (4bit enabled)
âœ… BitsAndBytesConfig supports NF4 quantization

Recommended usage for L4 GPU (22GB):
  synthesizer.fill_in_object(..., use_two_stage=True, use_4bit=True)
  â†’ Memory usage: ~12-14GB (fits in L4!)
```

## ë…¸íŠ¸ë¶ ì‹¤í–‰

```bash
# Jupyter ë…¸íŠ¸ë¶ì—ì„œ
cd notebooks
jupyter notebook pipeline_validation.ipynb
```

**Cell 14ì—ì„œ ì„¤ì •:**
```python
USE_TWO_STAGE = True   # IP-Adapter ì‚¬ìš©
USE_4BIT = True        # 4bit ì–‘ìí™” (L4 ìµœì í™”!)
```

## íŒŒì¼ êµ¬ì¡°

```
script/ê¹€ëª…í™˜/note/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ synthesizer.py           # ğŸ”§ 4bit ì–‘ìí™” ì§€ì› ì¶”ê°€
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ pipeline_validation.ipynb  # ğŸ”§ USE_4BIT ì˜µì…˜ ì¶”ê°€
â”œâ”€â”€ test_4bit_quantization.py    # ğŸ†• 4bit ì–‘ìí™” í…ŒìŠ¤íŠ¸
â”œâ”€â”€ 4BIT_QUANTIZATION_UPDATE.md  # ğŸ†• ìƒì„¸ ì—…ë°ì´íŠ¸ ë¬¸ì„œ
â””â”€â”€ README_4BIT_QUANTIZATION.md  # ğŸ†• ì´ íŒŒì¼
```

## FAQ

### Q: 4bit ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ë©´ í’ˆì§ˆì´ ë–¨ì–´ì§€ë‚˜ìš”?
**A:** ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì‹œê°ì  ì°¨ì´ê°€ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤. NF4 ì–‘ìí™”ëŠ” ì‹ ê²½ë§ ê°€ì¤‘ì¹˜ì˜ ì •ê·œë¶„í¬ë¥¼ í™œìš©í•˜ì—¬ í’ˆì§ˆ ì €í•˜ë¥¼ ìµœì†Œí™”í•©ë‹ˆë‹¤.

### Q: L4 GPUê°€ ì•„ë‹Œ ë‹¤ë¥¸ GPUì—ì„œë„ 4bitë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜ìš”?
**A:** A100, H100 ë“± VRAMì´ ì¶©ë¶„í•œ GPUì—ì„œëŠ” 8bitë‚˜ ì–‘ìí™” ì—†ì´ ì‚¬ìš©í•´ë„ ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ 4bitë¥¼ ì‚¬ìš©í•˜ë©´ ë©”ëª¨ë¦¬ê°€ ë” ì ˆì•½ë˜ë¯€ë¡œ ë” í° ë°°ì¹˜ í¬ê¸°ë‚˜ ë™ì‹œ ì‹¤í–‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

### Q: ë‹¨ì¼ íŒŒì´í”„ë¼ì¸ê³¼ 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì¤‘ ë­˜ ì¨ì•¼ í•˜ë‚˜ìš”?
**A:**
- **ì°¸ì¡° ì´ë¯¸ì§€ ë°˜ì˜ì´ ì¤‘ìš”í•˜ë‹¤ë©´**: `use_two_stage=True` (ê¶Œì¥)
- **ë©”ëª¨ë¦¬ê°€ ê·¹ë„ë¡œ ì œí•œì ì´ê±°ë‚˜ ì†ë„ê°€ ì¤‘ìš”í•˜ë‹¤ë©´**: `use_two_stage=False`

### Q: 4bitì™€ 8bit ì¤‘ ë­˜ ì¨ì•¼ í•˜ë‚˜ìš”?
**A:**
- **L4 GPU (22GB)**: `use_4bit=True` (í•„ìˆ˜!)
- **A100 (40GB)**: `use_4bit=False` (ì„ íƒ)
- **A100 (80GB)**: ì–‘ìí™” ì—†ì´ ì‚¬ìš© ê°€ëŠ¥

## ê¸°ìˆ  ì„¸ë¶€ì‚¬í•­

### NF4 (NormalFloat4) Quantization
- ì‹ ê²½ë§ ê°€ì¤‘ì¹˜ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ëŠ” ê°€ì • í™œìš©
- 4bitë¡œ ì••ì¶•í•˜ë©´ì„œë„ í’ˆì§ˆ ìœ ì§€
- `bnb_4bit_use_double_quant=True`: ì–‘ìí™” ìƒìˆ˜ë„ ì¶”ê°€ ì••ì¶•

### BitsAndBytes Library
- Tim Dettmersì˜ ì–‘ìí™” ë¼ì´ë¸ŒëŸ¬ë¦¬
- CUDA ì»¤ë„ ìµœì í™”ë¡œ ë¹ ë¥¸ ì¶”ë¡ 
- Hugging Face Transformersì™€ í†µí•©

## ì°¸ê³  ìë£Œ

- [BitsAndBytes GitHub](https://github.com/TimDettmers/bitsandbytes)
- [QLoRA Paper (NF4)](https://arxiv.org/abs/2305.14314)
- [Hugging Face Quantization Guide](https://huggingface.co/docs/transformers/main/en/quantization)
- [FLUX.1-Fill Documentation](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev)
- [IP-Adapter for FLUX](https://huggingface.co/XLabs-AI/flux-ip-adapter-v2)

## ë¼ì´ì„¼ìŠ¤
This project follows the same license as the parent project.

---

**Made with â¤ï¸ for L4 GPU users**
