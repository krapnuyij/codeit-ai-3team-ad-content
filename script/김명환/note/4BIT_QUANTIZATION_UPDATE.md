# 4bit Quantization Update for L4 GPU Support

## ğŸ¯ Goal
L4 GPU(22GB VRAM)ì—ì„œ IP-Adapterë¥¼ ì‚¬ìš©í•œ 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ 4bit ì–‘ìí™” ì§€ì› ì¶”ê°€

## ğŸš€ What's New

### í•µì‹¬ ê°œì„ ì‚¬í•­
- **4bit ì–‘ìí™” ì§€ì›**: NF4 (NormalFloat4) ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëŒ€í­ ê°ì†Œ
- **L4 GPU ì§€ì›**: 22GB VRAMì—ì„œ IP-Adapter + 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê°€ëŠ¥
- **ìœ ì—°í•œ ì„¤ì •**: `use_4bit` íŒŒë¼ë¯¸í„°ë¡œ 4bit/8bit ì„ íƒ ê°€ëŠ¥

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ

#### 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ (IP-Adapter ì‚¬ìš©)
| ì–‘ìí™” ë°©ì‹ | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | L4 GPU ì§€ì› |
|------------|-------------|-----------|
| ì—†ìŒ | ~22GB+ | âŒ (ë¶ˆê°€ëŠ¥) |
| 8bit | ~18-20GB | âš ï¸ (ë¹ ë“¯) |
| **4bit** | **~12-14GB** | **(ê¶Œì¥!)** |

#### ë‹¨ì¼ íŒŒì´í”„ë¼ì¸ (í…ìŠ¤íŠ¸ë§Œ)
| ì–‘ìí™” ë°©ì‹ | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ |
|------------|-------------|
| ì—†ìŒ | ~15GB |
| 8bit | ~11GB |
| **4bit** | **~7-8GB** |

## ğŸ“ Changes

### 1. `src/synthesizer.py`

#### ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„° ì¶”ê°€
```python
def fill_in_object(
    self,
    ...,
    use_4bit: bool = True,  # ğŸ†• 4bit ì–‘ìí™” (ê¸°ë³¸ê°’: True)
) -> Image.Image:
```

#### `_load_model()` ë©”ì„œë“œ ì—…ë°ì´íŠ¸
**Before (8bit only):**
```python
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.bfloat16,
)
```

**After (4bit/8bit ì„ íƒ ê°€ëŠ¥):**
```python
if use_4bit:
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_quant_type="nf4",  # NormalFloat4
        bnb_4bit_use_double_quant=True,  # ì¶”ê°€ ë©”ëª¨ë¦¬ ì ˆì•½
    )
else:
    quantization_config = BitsAndBytesConfig(
        load_in_8bit=True,
        bnb_8bit_compute_dtype=torch.bfloat16,
    )
```

#### `_load_flux_pipeline()` ë©”ì„œë“œ ì—…ë°ì´íŠ¸
- 4bit ì–‘ìí™” ì§€ì› ì¶”ê°€
- FluxPipelineì˜ íŠ¸ëœìŠ¤í¬ë¨¸ì—ë„ ë™ì¼í•œ ì–‘ìí™” ì ìš©

#### í•˜ìœ„ ë©”ì„œë“œ ì—…ë°ì´íŠ¸
- `_stage1_ip_adapter_generation()`: `use_4bit` íŒŒë¼ë¯¸í„° ì¶”ê°€
- `_stage2_fill_refinement()`: `use_4bit` íŒŒë¼ë¯¸í„° ì¶”ê°€

### 2. `notebooks/pipeline_validation.ipynb`

#### Cell 14 ì—…ë°ì´íŠ¸
**New configuration options:**
```python
USE_TWO_STAGE = True   # IP-Adapter ì‚¬ìš© ì—¬ë¶€
USE_4BIT = True        # 4bit ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€ (ê¶Œì¥!)

final_image = synthesizer.fill_in_object(
    ...,
    use_two_stage=USE_TWO_STAGE,
    use_4bit=USE_4BIT  # ğŸš€ NEW!
)
```

## ğŸ’¡ Usage Examples

### Example 1: 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ + 4bit (L4 ìµœì í™”, ê¶Œì¥!)
```python
synthesizer = ObjectSynthesizer(enable_ip_adapter=True)

result = synthesizer.fill_in_object(
    background=bg_image,
    mask=mask_image,
    reference=clean_product,
    prompt="ê°ˆìƒ‰ ìœ ë¦¬ë³‘ì˜ ë§¥ì£¼, ë”°ëœ»í•œ ë°” ì¡°ëª…ì˜ ë‚˜ë¬´ í…Œì´ë¸” ìœ„",
    use_two_stage=True,   # IP-Adapter ì‚¬ìš©
    use_4bit=True,        # 4bit ì–‘ìí™” (ë©”ëª¨ë¦¬ ~12-14GB)
    seed=42
)
```

**ì¥ì :**
- ì°¸ì¡° ì´ë¯¸ì§€ì˜ ì‹œê°ì  íŠ¹ì§• ë°˜ì˜
- L4 GPU(22GB)ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (~12-14GB)

**ë‹¨ì :**
- âš ï¸ 2ê°œ ëª¨ë¸ ìˆœì°¨ ë¡œë“œë¡œ ì‹œê°„ ì†Œìš”

### Example 2: ë‹¨ì¼ íŒŒì´í”„ë¼ì¸ + 4bit (ë©”ëª¨ë¦¬ ìµœì†Œ)
```python
synthesizer = ObjectSynthesizer()

result = synthesizer.fill_in_object(
    background=bg_image,
    mask=mask_image,
    reference=clean_product,  # ë¬´ì‹œë¨
    prompt="ê°ˆìƒ‰ ìœ ë¦¬ë³‘ì˜ ë§¥ì£¼, ë”°ëœ»í•œ ë°” ì¡°ëª…ì˜ ë‚˜ë¬´ í…Œì´ë¸” ìœ„",
    use_two_stage=False,  # í…ìŠ¤íŠ¸ë§Œ
    use_4bit=True,        # 4bit ì–‘ìí™” (ë©”ëª¨ë¦¬ ~7-8GB)
    seed=42
)
```

**ì¥ì :**
- ë©”ëª¨ë¦¬ ìµœì†Œ (~7-8GB)
- ë¹ ë¥¸ ì‹¤í–‰ (1ê°œ ëª¨ë¸ë§Œ)

**ë‹¨ì :**
- âŒ ì°¸ì¡° ì´ë¯¸ì§€ ë¬´ì‹œ (í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë§Œ ì‚¬ìš©)

### Example 3: 8bit ì–‘ìí™” (ê¸°ì¡´ ë°©ì‹)
```python
result = synthesizer.fill_in_object(
    ...,
    use_two_stage=True,
    use_4bit=False,  # 8bit ì–‘ìí™” (ë©”ëª¨ë¦¬ ~18-20GB)
    seed=42
)
```

## ğŸ”§ Technical Details

### NF4 Quantization
- **bnb_4bit_quant_type="nf4"**: NormalFloat4 ì–‘ìí™”
  - ì •ê·œë¶„í¬ë¥¼ ê°€ì •í•œ 4bit ì–‘ìí™”
  - ì‹ ê²½ë§ ê°€ì¤‘ì¹˜ì— ìµœì í™”

- **bnb_4bit_use_double_quant=True**: ì´ì¤‘ ì–‘ìí™”
  - ì–‘ìí™” ìƒìˆ˜ë„ ì¶”ê°€ë¡œ ì–‘ìí™”
  - ë©”ëª¨ë¦¬ ì¶”ê°€ ì ˆì•½

### Memory Savings
- **4bit vs FP16**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ~75% ê°ì†Œ
- **4bit vs 8bit**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ~50% ê°ì†Œ

### Quality Trade-off
- 4bit ì–‘ìí™”ëŠ” ì•½ê°„ì˜ í’ˆì§ˆ ì €í•˜ê°€ ìˆì„ ìˆ˜ ìˆìŒ
- í•˜ì§€ë§Œ ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì‹œê°ì  ì°¨ì´ ë¯¸ë¯¸
- L4 GPUì—ì„œ IP-Adapter ì‚¬ìš© ê°€ëŠ¥í•œ ê²ƒì´ ë” í° ì¥ì !

## Verification

ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸:
```python
from src import ObjectSynthesizer

# 4bit ì–‘ìí™”ë¡œ ì´ˆê¸°í™”
synthesizer = ObjectSynthesizer(enable_ip_adapter=True)

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
import torch
print(f"GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
```

## ğŸ“Š Performance Comparison

| Configuration | Memory | Speed | Quality | L4 Support |
|--------------|--------|-------|---------|-----------|
| 2-stage + No Quant | ~22GB+ | Baseline | Best | âŒ |
| 2-stage + 8bit | ~18-20GB | 0.9x | Very Good | âš ï¸ |
| **2-stage + 4bit** | **~12-14GB** | **0.8x** | **Good** | **âœ…** |
| 1-stage + 8bit | ~11GB | 0.5x | Good | |
| 1-stage + 4bit | ~7-8GB | 0.4x | Good | |

## ğŸ‰ Conclusion

4bit ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ë©´:
1. L4 GPUì—ì„œ IP-Adapter ì‚¬ìš© ê°€ëŠ¥!
2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 50% ì´ìƒ ê°ì†Œ
3. í’ˆì§ˆ ì €í•˜ ë¯¸ë¯¸
4. ìœ ì—°í•œ ì„¤ì • (4bit/8bit ì„ íƒ ê°€ëŠ¥)

**ê¶Œì¥ ì„¤ì •:**
```python
use_two_stage=True  # IP-Adapterë¡œ ì°¸ì¡° ì´ë¯¸ì§€ ë°˜ì˜
use_4bit=True       # 4bit ì–‘ìí™”ë¡œ L4 GPU ì§€ì›
```

## ğŸ“ Modified Files

1. `src/synthesizer.py`
   - `_load_model()`: 4bit ì–‘ìí™” ì§€ì›
   - `_load_flux_pipeline()`: 4bit ì–‘ìí™” ì§€ì›
   - `_stage1_ip_adapter_generation()`: `use_4bit` íŒŒë¼ë¯¸í„° ì¶”ê°€
   - `_stage2_fill_refinement()`: `use_4bit` íŒŒë¼ë¯¸í„° ì¶”ê°€
   - `fill_in_object()`: `use_4bit` íŒŒë¼ë¯¸í„° ì¶”ê°€ ë° docstring ì—…ë°ì´íŠ¸

2. `notebooks/pipeline_validation.ipynb`
   - Cell 14: `USE_4BIT` ì˜µì…˜ ì¶”ê°€ ë° ì„¤ëª… ì—…ë°ì´íŠ¸

## ğŸ”— References

- [BitsAndBytes Documentation](https://github.com/TimDettmers/bitsandbytes)
- [NF4 Quantization Paper](https://arxiv.org/abs/2305.14314)
- [Hugging Face Quantization Guide](https://huggingface.co/docs/transformers/main/en/quantization)
